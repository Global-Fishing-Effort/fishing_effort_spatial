---
title: "Fit prediction model for fishing effort: stage 1"
output: html_document
date: "2024-12-11"
editor_options: 
  chunk_output_type: console
---

# Summary 

We use the data compiled in the previous scripts to fit a logistic regression model (or a random forest) to predict the presence (1) or absence (0) of fishing effort in every cell. We will do this for each flag country individually. We will start by testing some individual countries. 

In the second step, they model the intensity of fishing on only cells which actually have fishing effort. In our case, this will be the proportion of global production. Then they multiply the prediction of where fishing occurs (from step 1, 0 or 1) by the intensity of fishing (in our case 0-1). 

## Environment

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

rm(list = ls())

# install.packages("betareg")
# install.packages("foreach")
# install.packages("doParallel")
# install.packages("tictoc")
# install.packages("terra")
# install.packages("arrow")
# install.packages("qs")
# install.packages("strex")

library(tidyverse)
library("qs")
library(betareg)
library(foreach)
library(doParallel)
library(here)
library(mgcv)
library(tictoc)
library(progress)
library(terra)
library(glue)
library(arrow)
library(strex)
library(broom)

# install.packages("randomForest")
library(randomForest)

source(here("R/dir.R"))


pixel_size <- 1 # change this if you want to run 0.5?

```

## Functions to prepare enviro and effort data  

```{r}

elnino <- read.csv(here("data/model_features/enso_index.csv"))

pdo <- read.csv(here("data/model_features/pdo_index.csv"))

world_bank <- read.csv(here("data/model_features/world_bank_regions.csv")) %>%
  filter(ISO_SOV1 != "GIB") # filter out gibralter bc it is duplicating the UK for some reason

gfi_df <- read.csv(here("data/model_features/global_fishing_index_governance.csv"))


if(pixel_size == 1){
  
# Load prepared data
model_data <- readRDS(here("data/model_features/prepared_data_1deg.rds")) %>%
  mutate(year = as.numeric(year)) %>%
  mutate(presence = 1) # i think we need to adjust the prepared data to include cells which have no fishing effort in them. We can do this in the loop below? 

## read in environmental variables

  global_grid <- read.csv(here("data/model_features/deg_1_x_1/global_grid.csv"))
  
  ocean_data <- read.csv(here("data/model_features/deg_1_x_1/errdap_chl.csv")) %>%
  left_join(read.csv(here("data/model_features/deg_1_x_1/errdap_sst.csv")), by = c("pixel_id", "year")) %>% 
  left_join(read.csv(here("data/model_features/deg_1_x_1/remss_wind.csv")), by = c("pixel_id", "year")) %>%
  filter(year %in% c(2015:2017)) 


  spatial_data <- global_grid %>%
    left_join(read.csv(here("data/model_features/deg_1_x_1/gfw_static_spatial_measures.csv")) %>% dplyr::select(-lat, -lon), by = "pixel_id") %>%
   left_join(read.csv(here("data/model_features/deg_1_x_1/mesopelagiczones/mesopelagiczones_fixed.csv")), by = "pixel_id") %>%
   left_join(read.csv(here("data/model_features/deg_1_x_1/eez/eez.csv")), by = "pixel_id") %>%
   left_join(read.csv(here("data/model_features/deg_1_x_1/fao/fao_fixed.csv")), by = "pixel_id") %>%
   left_join(read.csv(here("data/model_features/deg_1_x_1/oceans_fixed.csv")), by = "pixel_id") %>%
    left_join(read.csv(here("data/model_features/deg_1_x_1/seamounts.csv")), by = "pixel_id") %>% 
   crossing(., year = c(2015:2017)) 
  
  
  env_data <- spatial_data %>%
    left_join(ocean_data) %>% # cool.
    dplyr::select(-geometry_wkt) %>%
    left_join(elnino) %>%
    left_join(pdo) %>%
    left_join(world_bank, by = c("eez_id" = "MRGID_SOV1")) %>%
    mutate(eez_region_world_bank_7 = ifelse(ISO_SOV1 %in% c("High seas", "Land"), "High seas", eez_region_world_bank_7)) %>% 
    left_join(gfi_df, by = c("ISO_SOV1" = "flag_fin")) %>% # add in global fishing index data here
    mutate(gov_score = ifelse(eez_id >= 99999 & is.na(gov_score), "high_seas", gov_score)) %>%
        mutate(gov_score = ifelse(eez_id < 99999 & is.na(gov_score), "no_data", gov_score)) %>%
    dplyr::select(-ISO_SOV1, -nearest_seamount_id) %>% 
    distinct() 

}else{
  # Load prepared data
model_data <- readRDS(here("data/model_features/prepared_data.rds")) %>%
  mutate(year = as.numeric(year)) %>%
  mutate(presence = 1) # i think we need to adjust the prepared data to include cells which have no fishing effort in them. We can do this in the loop below? 

## read in environmental variables

# Function to prepare prediction data for a given year
prepare_pred_data <- function(yr) {
  # yr = 2015
  message(sprintf("Loading environmental data for year %d...", yr))
  
  # Load data in chunks to manage memory
  chunks <- list()
  
  # Load and process chlorophyll data
  chunks$chl <- read.csv(here("data/model_features/erdMH1chlamday/errdap_2013_2017.csv")) %>% # since we're only training on 2015-2017
    filter(year == yr) %>%
    dplyr::select(pixel_id, chl_mg_per_m3_mean, chl_mg_per_m3_sd)
  
  # Load SST data
  if(yr == 2015){
  chunks$sst <- read.csv(here("data/model_features/ncdcOisst21Agg_LonPM180/errdap_sst_2011_2015.csv")) %>%
    filter(year == yr) %>%
    dplyr::select(pixel_id, sst_c_mean, sst_c_sd)
  
  }else{
      chunks$sst <- read.csv(here("data/model_features/ncdcOisst21Agg_LonPM180/errdap_sst_2016_2020.csv")) %>%
    filter(year == yr) %>%
    dplyr::select(pixel_id, sst_c_mean, sst_c_sd)
  }
  
  # Load and process wind data
  chunks$wind <- read.csv(here("data/model_features/remss_wind/wind_2013_2017.csv")) %>%
    filter(year == yr) %>%
    dplyr::select(pixel_id, wind_speed_mean = wind_speed_ms_mean, wind_speed_sd = wind_speed_ms_sd)
  
  # Load static spatial data
  chunks$spatial <- read.csv(here("data/model_features/gfw_static_spatial_measures.csv")) %>%
  dplyr::select(pixel_id, elevation_m, distance_from_port_m, distance_from_shore_m)
  
  # Load categorical spatial data
  chunks$eez <- read.csv(here("data/model_features/eez/eez.csv")) %>%
    dplyr::select(pixel_id, eez_id)
  chunks$fao <- read.csv(here("data/model_features/fao/fao.csv")) %>%
    dplyr::select(pixel_id, fao_id)
  
  # Load global grid
  global_grid <- read.csv(here("data/model_features/global_grid.csv"))
  
  # Combine all predictor data efficiently
  message("Merging data chunks...")
  pred_data <- global_grid %>%
    left_join(chunks$spatial, by = "pixel_id") %>%
    left_join(chunks$chl, by = "pixel_id") %>%
    left_join(chunks$sst, by = "pixel_id") %>%
    left_join(chunks$wind, by = "pixel_id") %>%
    left_join(chunks$eez, by = "pixel_id") %>%
    left_join(chunks$fao, by = "pixel_id") %>%
    mutate(eez_id = as.factor(eez_id),
           fao_id = as.factor(fao_id)) %>%
    mutate(year = yr)
  
  # # Ensure categorical variables are factors with correct levels
  # pred_data$eez_id <- factor(pred_data$eez_id,
  #                        levels = unique(model$model$eez_id))
  # pred_data$fao_id <- factor(pred_data$fao_id,
  #                               levels = unique(model$model$fao_id))
  
  # Clean up chunks to free memory
  rm(chunks)
  gc()
  
  return(pred_data)
}
  
}


# Get total fishing hours for a specific year from IMAS data
get_historical_total_fishing_hours <- function(yr) {
  # This function would load and process the IMAS data for the given year
  # Note: Implement based on your IMAS data structure
  imas_data <- qs::qread(here("data/int/rousseau_gear_fix.qs")) %>%
    
    ## CN remove artisanal
    filter(sector == "I") %>%
    
    group_by(year, flag_fin = country, gear = gear_new, length_category) %>%
    summarize(
      total_fishing_hours = sum(eff_active_hours, na.rm = TRUE),
      .groups = 'drop'
    ) %>%
    filter(year == yr) %>%
    dplyr::select(flag_fin, year, gear, length_category, total_fishing_hours)
  
  
  return(imas_data)
}


```

## Specify model formula and load data 

```{r}

# Prepare model formula
model_formula <- formula(
  presence ~ 
    # Categorical/factor predictors
   # flag_fin + # since we're creating a model for each flag country? s
    gear  + 
    length_category +
    factor(meso_id) +  
    factor(eez_id) + 
    factor(fao_id) + 
    factor(ocean) +  # Spatial categorical variables
    # Total effort predictor (log-transformed)
    log_total_fishing_hours + # honestly not sure if this is necessary. It was a suggestion from Julia to include. Basically a sort of fishing capacity variable. 
    # Continuous predictors
    lon + lat + 
    elevation_m + # depth
    distance_from_port_m + 
    distance_from_shore_m +
    chl_mg_per_m3_mean + 
    chl_mg_per_m3_sd +
    sst_c_mean + 
    sst_c_sd +
    wind_speed_ms_mean + 
    wind_speed_ms_sd +
    enso_index_mean + # el nino data 
    enso_index_sd + # pacific decadal oscillation
    pdo_index_mean +  
    pdo_index_sd + 
    nearest_seamount_distance_m + 
    eez_region_world_bank_7 + # world bank regions
    factor(gov_score) + # global fishing index; make sure this is factor and not a numeric variable
    # Year effect
  # s(year, k=2) # smooth term for year; maybe not necessary? Fishing effort isn't necessarily linear so keeping for now. Does this slow down modeling? 
    year
)

if(pixel_size == 1){
  
  env_data <- env_data
  
}else{

  env_data <- prepare_pred_data(2015) %>%
    rbind(., prepare_pred_data(2016)) %>%
    rbind(., prepare_pred_data(2017)) %>%
    left_join(elnino) 
  
}

hist_fish_data <- get_historical_total_fishing_hours(2015) %>%
    rbind(., get_historical_total_fishing_hours(2016)) %>%
    rbind(., get_historical_total_fishing_hours(2017)) %>%
    mutate(log_total_fishing_hours = log1p(total_fishing_hours)) %>%
    filter(total_fishing_hours > 0)
  
flags <- unique(hist_fish_data$flag_fin) # get the flags we need to run models for # 167 of them

unique_combinations <- hist_fish_data %>%
  distinct(year, flag_fin, gear, length_category) %>% # we only want to make models for these combinations since these are what is in the IMAS/FAO data 
  mutate(row_n = row_number())

# test_aus <- unique_combinations %>% filter(flag_fin == "AUS") # ok, AUS for example has 128 different combinations that 1 model will be able to predict. 128*46229 = ~6 million rows being fed into the model. Will this work?? Let's try! 
#   
# 
# setdiff(unique(hist_fish_data$flag_fin), unique(model_data$flag_fin))

env_grid <- env_data %>% 
  dplyr::select(lon, lat) %>% distinct()

```

## Run models in parallel 

```{r}
flags <- unique(hist_fish_data$flag_fin) # get the flags we need to run models for # 167 of them

# flags <- setdiff(flags,  str_before_last(str_after_last(list.files(file.path(rdsi_dir, "prep/logistic_regression_models/")), "_"), "\\."))

missing_flags <- setdiff(unique(hist_fish_data$flag_fin), unique(model_data$flag_fin))
length(missing_flags) # 28

flags <- setdiff(flags, missing_flags)  ## CN are these countries that have not run from previous iterations? GC - yes, these would not have been run before. 

# Set up parallel backend
# num_cores <- detectCores() - 1  # Use available cores minus 1 to avoid overloading
num_cores = 10 # to be safe. I tried with 31 and it overloaded. Lets assume ~8gb per model (that was what australia was, so ~16 is the max we could use)
cl <- makeCluster(num_cores)
registerDoParallel(cl)

# Run in parallel 
foreach(flag = flags, .packages = c("dplyr", "glue", "stats", "tictoc", "tidyr")) %dopar% {
  
  # flag = "VNM" 
  
  model_path <- glue(file.path(rdsi_dir, "prep/logistic_regression_models/fitted_model_{flag}.rds"))
  
  # if (file.exists(model_path)) {
  #   cat("Model for", flag, "exists... skipping\n")
  #   return(NULL)
  # }
  
  if(flag %in% missing_flags){
    cat(flag, "not in data... skippings")
    return(NULL)
  }
  
  message("Fitting logistic regression model for ", flag, "...")
  
  model_data_flag <- model_data %>%
    dplyr::select(lon, lat, flag_fin, gear, length_category, year, presence) %>%
    filter(flag_fin == flag)
  
  distinct_cats <- model_data_flag %>%
    distinct(year, flag_fin, gear, length_category)
  
  full_grid <- tidyr::crossing(env_grid, distinct_cats)
  
  full_data <- full_grid %>%
    left_join(env_data, by = c("lon", "lat", "year")) %>%
    left_join(model_data_flag, by = c("lon", "lat", "year", "flag_fin", "gear", "length_category")) %>%
    mutate(presence = ifelse(is.na(presence), 0, presence)) %>%
    dplyr::select(-pixel_id) %>%
    left_join(hist_fish_data) %>%
    mutate(total_fishing_hours = ifelse(is.na(total_fishing_hours), 0, total_fishing_hours),
           log_total_fishing_hours = ifelse(is.na(log_total_fishing_hours), 0, log_total_fishing_hours))
  
  if(length(unique(full_data$gear)) == 1 | length(unique(full_data$length_category)) == 1 ){ # skipping any that only have one categorical variable since model won't run these?  
  return(NULL)
  
}
  
  if(nrow(full_data) == 0){ # if its an empty dataframe we have nothing to model... These would be the countries missing from AIS i guess? 
        cat(flag, "not in data... skipping")
    return(NULL)
  }
  
  tic()
  model <- stats::glm(model_formula, data = full_data, family = binomial(link = "logit")) ## March 12 - not sure why, but these models are taking a lot longer to run now... is it because i fixed some of the input data? for example, the saved models are much larger now. Canada has a saved model of ~4GB now... 
  toc() # ok viet nam took ~ 10 mins now...
  
  # CN commented not to overwrite
  saveRDS(model, model_path)
}

# Stop cluster
stopCluster(cl)

############ March 11, 2025 - GC -
#### ok getting some weird results. For example, for ivory coast some of the variables are all NA and prompt this response "not defined because of singularities". I think this means that some of the variables are perfectly collinear... So maybe we shouldn't include every variable that mcdonald et al includes? Anyway, I am rerunning all of these models with all of the new variables I've prepped (e.g., PDO index, ocean, nearest seamount, global fishing index, world bank regions)


test <- readRDS(file.path(rdsi_dir, "prep/logistic_regression_models/fitted_model_VNM.rds"))
coef_df <- tidy(test)
summary(test)

# why are year, pdo, world bank regions, gfi governance score all NA??? The NAs are "not defined because of singularities". Apparently this means 2 or more variables are highly collinear

test_mod <- readRDS(file.path(rdsi_dir, "prep/logistic_regression_models/fitted_model_CAN.rds"))
coef_df <- tidy(test_mod)
summary(test_mod)

```

## GC - do some predictions 

```{r}
flag = "CAN" 
  
model_data_flag <- model_data %>%
  dplyr::select(lon, lat, flag_fin, gear, length_category, year, presence) %>%
  filter(flag_fin == flag)
  
distinct_cats <- model_data_flag %>%
    distinct(year, flag_fin, gear, length_category)
  
full_grid <- tidyr::crossing(env_grid, distinct_cats)
  
full_data <- full_grid %>%
  left_join(env_data, by = c("lon", "lat", "year")) %>%
  left_join(model_data_flag, by = c("lon", "lat", "year", "flag_fin", "gear", "length_category")) %>%
  mutate(presence = ifelse(is.na(presence), 0, presence)) %>%
  dplyr::select(-pixel_id) %>%
  left_join(hist_fish_data) %>%
  mutate(total_fishing_hours = ifelse(is.na(total_fishing_hours), 0, total_fishing_hours),
         log_total_fishing_hours = ifelse(is.na(log_total_fishing_hours), 0, log_total_fishing_hours))

test_mod <- readRDS(file.path(rdsi_dir, "prep/logistic_regression_models/fitted_model_CAN.rds"))

pred_test <- full_data %>%
  mutate(eez_region_world_bank_7 = ifelse(eez_region_world_bank_7 == "High seas", "high_seas", eez_region_world_bank_7)) %>%
    mutate(pred_prop_pres = stats::predict(test_mod, newdata = ., type = "response")) # ok...

testing <- pred_test %>% filter(pred_prop_pres >= 0.5, year == 2015) # filter for probs > 0.5 (i.e., more likely than not to having fishing in them). Results in only 464 cells for 2015... that doesn't seem right. 
testing <- full_data %>% filter(year == 2015, presence == 1) # 1549 cells reported in AIS data. That seems problematic that we are getting less cells predicted with fishing effort in them.

## what about if we do any probabilities greater than the minimum prob? 
min_prob <- min(pred_test$pred_prop_pres, na.rm = TRUE)
testing <- pred_test %>% filter(pred_prop_pres > min_prob, year == 2015) # seems like too many cells now.
quantile(testing$pred_prop_pres) # not sure what the best thing to do if we want to use logistic regression. Maybe we should just use random forest, but I fear we'll run into the same problem of not enough cells being predicted with fishing effort in them. Surely we should get at least the same number of cells with fishing predicted in them as there is actually AIS cells?

```


## CN check countries not run or gear combinations not run 

```{r}

## first check data ----

# load full effort data 
trial<- qs::qread(here("data/int/rousseau_gear_fix.qs")) %>% 
  group_by(year, sector) %>% 
  summarise(tot_effort = sum(nom_active)) %>% 
  ungroup()

ggplot(trial, aes(x = year, y = tot_effort, group = sector, color = sector))+
  geom_line()

### artisanal powered is much more than industrial in terms of h fished, not in terms of nom_active - i.e. accounting for power. 

# check ais data

trial<-model_data %>% 
  group_by(year, flag_fin, gear,length_category) %>% 
  summarise(prop = sum(prop_fishing_hours_cell, na.rm = T), 
            eff = sum(total_fishing_hours, na.rm = T))

filter(trial, prop>1)

## why prop > 1?? Also when effort = 0 ... 

### missing combo ----

## how much of the total effort do these missing flags account for? 
length(unique(hist_fish_data$flag_fin)) # 147 when artisanal is not considered. 
missing_flags <- setdiff(unique(hist_fish_data$flag_fin), unique(model_data$flag_fin))
length(missing_flags)

trial<-hist_fish_data %>% 
  mutate(missing_flag = ifelse(flag_fin %in% missing_flags, flag_fin, "not_missing")) %>% 
  group_by(missing_flag) %>% 
  reframe(tot_effort = sum(total_fishing_hours)) %>% 
  mutate(perc_effort = (tot_effort/sum(tot_effort))*100) %>% 
  arrange(-perc_effort)

## with artisanal: missing flags account for 21% of effort in Yannick's data - United Arab Emirates alone accounts for 15%. 
# without artisanal - see function in line ~160: missing flags account for 5% of effort.
sort(unique(hist_fish_data$gear)) # length cat does not mean "artisanal"
sort(unique(model_data$gear)) # same cat here. 

## missing gear combinations
combo_rousseau<-hist_fish_data %>% 
  select(flag_fin, gear, length_category, total_fishing_hours) %>% 
  group_by(flag_fin, gear, length_category) %>%  # remove year from table? 
  reframe(total_fishing_hours = sum(total_fishing_hours)) %>% 
  distinct() %>% 
  mutate(datasetRousseau = "Y") %>% 
  mutate(perc_effort = (total_fishing_hours/sum(total_fishing_hours))*100) %>% 
  arrange(-perc_effort) 

combo_ais<-model_data %>% 
  select(flag_fin, gear, length_category) %>% 
  distinct() %>% 
  mutate(datasetAIS = "Y")

missing_combo <- combo_rousseau %>% full_join(combo_ais) %>% 
  filter(is.na(datasetAIS)) %>% 
  mutate(perc_effort = (total_fishing_hours/sum(hist_fish_data$total_fishing_hours))*100) %>% 
  arrange(-perc_effort) %>% 
  select(-c(datasetRousseau, datasetAIS))

sum(missing_combo$perc_effort)

## missing combo account for 82% of effort in Yannick's data - is this possible?! - no one combo being particularly influential. 
# without artisanal: missing combo account for 36% of effort - no one combo being particularly influential. 

filter(model_data, flag_fin == "CIV", gear == "Trawl_Midwater_or_Unsp", length_category == "24-50m")
filter(model_data, flag_fin == "CIV", gear == "Trawl_Midwater_or_Unsp")
filter(model_data, flag_fin == "CIV")

##### OPTIONS TO RUN BEFORE MODEL FITTING ABOVE ---- 
# if missing year from AIS data (currently removed from analysis above) - assume constant value?
# aggregate up rousseau dataset?


### Also the opposite is problematic - i.e. combo missing in Rousseau. It is not worth modelling these as we don't have country data to spread spatially. BUT see comment in section below

missing_combo2 <- combo_rousseau %>% full_join(combo_ais) %>% 
  filter(is.na(datasetRousseau)) %>% 
  distinct() %>% 
  select(-c(datasetRousseau, datasetAIS)) %>% 
  mutate(id = paste(flag_fin, gear,length_category))

toRemove<-unique(missing_combo2$id)
# 495 combo in AIS that are missing from R dataset. create ID and remove? 

#model_data<-model_data %>% 
 # mutate(id = paste(flag_fin,gear,length_category)) # %>% 
  # filter(!id %in% toRemove) 
  
filter(model_data, id == "DJI Trawl_Midwater_or_Unsp Over 50m")

```

## CN apply model to one country only for ease of investigation 

```{r}

### prepare data as per function above ---- 
# unique(model_data$flag_fin)

flag = "NZL" # "CIV" no match between r and ais data  
  
model_data_flag <- model_data %>%
  dplyr::select(lon, lat, flag_fin, gear, length_category, year, presence) %>%
  filter(flag_fin == flag)
  
distinct_cats <- model_data_flag %>%
    distinct(year, flag_fin, gear, length_category)
  
full_grid <- tidyr::crossing(env_grid, distinct_cats)
  
full_data <- full_grid %>%
  left_join(env_data, by = c("lon", "lat", "year")) %>%
  left_join(model_data_flag, by = c("lon", "lat", "year", "flag_fin", "gear", "length_category")) %>%
  mutate(presence = ifelse(is.na(presence), 0, presence)) %>%
  dplyr::select(-pixel_id) %>%
  left_join(hist_fish_data) %>%
  mutate(total_fishing_hours = ifelse(is.na(total_fishing_hours), 0, total_fishing_hours),
         log_total_fishing_hours = ifelse(is.na(log_total_fishing_hours), 0, log_total_fishing_hours))
  
# checkes 
length(unique(full_data$gear)) == 1 | length(unique(full_data$length_category)) == 1 
nrow(full_data) == 0

# compare AIS with R combo - NO MATCH for flag = CIV! no need to model this as no R effort data is available... see comment above on removing these combo
# though for countries where some combo are matching - combos in ais not present in R might still add info to the model - we could leave them here but do not consider them in final dataset when info from ais-driven model/s is matched to R data. 

full_data %>% 
  select(gear,length_category) %>% 
  distinct()

hist_fish_data %>% 
  filter(flag_fin == flag) %>% 
  select(gear,length_category) %>% 
  distinct()

### simpler formula ----
# simplify model formula to
model_formula_simpler <- formula(
  presence ~ 
    # Categorical/factor predictors
   # flag_fin + # since we're creating a model for each flag country? s
    gear  + 
    length_category +
    factor(meso_id) +  
    factor(eez_id) + 
    factor(fao_id) + 
    factor(ocean) +  # Spatial categorical variables
    factor(eez_region_world_bank_7) + # world bank regions
    factor(gov_score) + # global fishing index; make sure this is factor and not a numeric variable
    # Total effort predictor (log-transformed)
    log_total_fishing_hours + # honestly not sure if this is necessary. It was a suggestion from Julia to include. Basically a sort of fishing capacity variable. 
    # Continuous predictors
    lon + lat + 
    elevation_m + # depth
    distance_from_port_m + 
    distance_from_shore_m +
    chl_mg_per_m3_mean + 
    chl_mg_per_m3_sd +
    sst_c_mean + 
    sst_c_sd +
    wind_speed_ms_mean + 
    wind_speed_ms_sd +
    enso_index_mean + # el nino data 
    enso_index_sd + # pacific decadal oscillation
    pdo_index_mean +  
    pdo_index_sd + 
    nearest_seamount_distance_m + 
    # Year effect
    year
)

### GLM ---- 
data_glm <- full_data %>% 
  # done for random forest below 
  # filter(!is.na(sst_c_mean)) %>% # cannot handle NAs - Why? and why are there NAs? is this land?
  # na.omit() %>% # these removes all NAs but... 
  mutate(presence = as.factor(presence)) %>% # transform in ctegorical
  distinct()

tic()
model <- stats::glm(model_formula_simpler, data = data_glm, family = binomial(link = "logit"))
toc() # full model 191 sec elapsed
# model
par(mfrow = c(2,2))
plot(model) 

### remove outliers and refit the model 




## Error in `contrasts<-`(`*tmp*`, value = contr.funs[1 + isOF[nn]]) : 
# contrasts can be applied only to factors with 2 or more levels
# unique(full_data$gear)
# unique(full_data$length_category)
# when a factor has 1 cat only - possible with gear, length_cat and year as for checks above 

#### random forest ----

# https://www.geeksforgeeks.org/ml-classification-vs-regression/

# classification: aims to find decision boundaries that separate classes (e.g. predict weather conditions like “sunny,” “rainy,” or “cloudy.”)
  # 1) A linear decision boundary might separate two classes in a 2D space with a straight line (e.g., logistic regression).
    # Binomial: In binomial Logistic regression, there can be only two possible types of the dependent variables, such as 0 or 1, Pass or Fail, etc. See assumtions 
  # 2) A more complex model, may create non-linear boundaries to better fit intricate datasets.

# regression: focuses on finding the best-fitting line to predict numerical outcomes

# # https://stats.stackexchange.com/questions/327081/random-forest-for-regression-binary-response
# mtcars$vs <- as.factor(mtcars$vs) # set to factor otherwise does regression. 
# classifier <- randomForest( formula = vs~hp+drat, data=mtcars)
# predict(classifier, type="prob")

data_random_forest <- full_data %>% 
  # filter(!is.na(sst_c_mean)) %>% # cannot handle NAs - Why? and why are there NAs? is this land?
  na.omit() %>% # these removes all NAs but removes a decent chunk of data. GC - I looked into why there are NAs in the sst, chl, and wind variables, and couldn't find a great answer. I've prepped the data the same exact way as McDonald et al, and I even looked at their input files and they have the exact same number of cells with data in them. 
  mutate(presence = as.factor(presence)) %>% # transform in categorical
  distinct()

# problems with categorical predictors: rf does not like them in model formula as done for glm; also when I convert them in the dataset: Error in randomForest.default(m, y, ...) : 
  # Can not handle categorical predictors with more than 53 categories.
# Some transformation needed https://www.geeksforgeeks.org/how-to-fit-categorical-data-types-for-random-forest-classification/
# though not doing anything seems to produce results too but not sure if right
#### GC - It looks like to me that RF just doesn't like factors, but if we include them as categorical, it works for me? 
# data_random_forest<-data_random_forest %>% 
#   mutate(gear = as.factor(gear),  
#          length_category = as.factor(length_category),
#          meso_id = as.factor(meso_id),
#          eez_id = as.factor(eez_id),
#          fao_id = as.factor(fao_id),
#          ocean = as.factor(ocean))

# plot the data 
# map 

# barplot - we don't have enough data on 1s to run a random forest I guess... e.g. the train data might only have 0s?
# ggplot(data_random_forest,aes(presence)) + geom_histogram(aes(fill=gear),bins=50, stat="count")

# Split the data into training and testing. 80% for training, 20% for testing.
set.seed(123)
samp <- sample(nrow(data_random_forest), 0.5 * nrow(data_random_forest)) 
# if the training dataset is too big: Error in randomForest.default(m, y, ...) : 
  # long vectors (argument 28) are not supported in .C
# Can check solutions here: https://stackoverflow.com/questions/24195805/issue-with-randomforest-long-vectors
train <- data_random_forest[samp, ]
# ggplot(train,aes(presence)) + geom_histogram(aes(fill=gear),bins=50, stat="count")
# ggplot(filter(train, presence == 1),aes(presence)) + geom_histogram(aes(fill=gear),bins=50, stat="count")
test <- data_random_forest[-samp, ]

# apply model on train 
model_formula_rf <- formula(
  presence ~ 
    # Categorical/factor predictors
   # flag_fin + # since we're creating a model for each flag country? s
    gear  + 
    length_category +
    meso_id +
    eez_id +
    fao_id +
    ocean +  # Spatial categorical variables
    eez_region_world_bank_7 + # world bank regions
  #  gov_score + # global fishing index; make sure this is categorical and not a numeric variable
    # Total effort predictor (log-transformed)
    log_total_fishing_hours + # honestly not sure if this is necessary. It was a suggestion from Julia to include. Basically a sort of fishing capacity variable. 
    # Continuous predictors
    lon + lat + 
    elevation_m + # depth
    distance_from_port_m + 
    distance_from_shore_m +
    chl_mg_per_m3_mean + 
    chl_mg_per_m3_sd +
    sst_c_mean + 
    sst_c_sd +
    wind_speed_ms_mean + 
    wind_speed_ms_sd +
    enso_index_mean + # el nino data 
    enso_index_sd + # pacific decadal oscillation
    pdo_index_mean +  
    pdo_index_sd + 
    nearest_seamount_distance_m + 
    # Year effect
    year
)

tic()
model <- randomForest(model_formula_rf, data = train, type = "classification", proximity = FALSE, ntree = 100) # , mtry = 5) # proximity = FALSE to speed up the process as no needed for classification (I think), reduce ntree to speed up process but careful not to set too low
toc() # 19 sec ## GC - March 12 - I fixed some of the input layers and now this is taking MUCH longer to run, now ~4 mins for NZL 
model

qs::qsave(model, here("data/output/train_rf_model_nzl.qs"))

```

random forest predictions 

```{r}

## RANDOM FOREST DIAGONOSTICS: https://cran.r-project.org/web/packages/ggRandomForests/vignettes/ggrfRegression.html
varImpPlot(model)

importance <- data.frame(importance(model)) %>%
  arrange(-MeanDecreaseGini)

## Partial dependence plots
# opar <- par(mfrow=c(3,2))
# partialPlot(model, train, lon)
# partialPlot(model, train, elevation_m)
# partialPlot(model, train, sst_c_mean)
# partialPlot(model, train, lat)
# partialPlot(model, train, distance_from_port_m)
# partialPlot(model, train, chl_mg_per_m3_mean)
# par(opar)

# validate our model using the test data

## GC - look at some predictions
prediction_prob <- data.frame(pred_presence = predict(model, newdata = test, type="prob")[, "1"]) # so these are the probability of presence. 
nrow(prediction_prob %>% filter(pred_presence>=0.5)) # 554 with prob of presence >= 0.5

prediction <- data.frame(pred_presence = predict(model, newdata = test, type="response")) # so these are the presence (0 or 1) predictions 
nrow(prediction %>% filter(pred_presence == 1)) # 543 cells with predictions? 

prediction_vote <- data.frame(pred_presence = predict(model, newdata = test, type="vote")) # not sure how this is different than the probability one?  

nrow(test %>% filter(presence ==1)) # 1134. So we are predicting less cells than are shown in the AIS data... that doesn't seem right. 
nrow(prediction_prob %>% filter(pred_presence>0)) # 554 with prob of presence >= 0.5

test_preds <- test %>%
  mutate(pred_presence = predict(model, newdata = .,, type="prob")[, "1"]) %>%
  filter(gear == "Trawl_Midwater_or_Unsp",
         length_category == "12-24m", 
         year == 2015) %>% 
  dplyr::select(lon, lat, pred_presence) %>%
   filter(pred_presence > 0) %>%
  #mutate(pred_presence = ifelse(pred_presence > 0.5, 1, pred_presence)) %>%
  distinct() %>%
  rast(., type = "xyz")

plot(test_preds)


test_actual <- test %>%
  mutate(pred_presence = predict(model, newdata = ., type="prob")[, "1"]) %>%
  filter(gear == "Trawl_Midwater_or_Unsp",
         length_category == "12-24m", 
         year == 2015) %>% 
  dplyr::select(lon, lat, presence) %>%
    filter(presence == 1) %>%
  distinct() %>%
  rast(., type = "xyz")
plot(test_actual)

# view results 
# calculate the accuracy of the model



```





# CN model diagnostic plots - print  

```{r}

```

# CN predictions 

```{r}

```


## GC testing a random forest presence/absence model with the full dataset

```{r}

model_data_all <- model_data %>%
  dplyr::select(lon, lat, gear, length_category, year, presence) %>%
  distinct()
  
distinct_cats <- model_data_all %>%
    distinct(year, gear, length_category)
  
full_grid <- tidyr::crossing(env_grid, distinct_cats)
  
hist_fish_data_all <- hist_fish_data %>%
  group_by(gear, length_category, year) %>%
  summarise(total_fishing_hours = sum(total_fishing_hours)) %>%
  ungroup() %>%
  mutate(log_total_fishing_hours = log1p(total_fishing_hours))
  
full_data <- full_grid %>%
  left_join(env_data, by = c("lon", "lat", "year")) %>%
  left_join(model_data_all, by = c("lon", "lat", "year", "gear", "length_category")) %>%
  mutate(presence = ifelse(is.na(presence), 0, presence)) %>%
  dplyr::select(-pixel_id) %>%
  left_join(hist_fish_data_all) %>%
  mutate(total_fishing_hours = ifelse(is.na(total_fishing_hours), 0, total_fishing_hours),
         log_total_fishing_hours = ifelse(is.na(log_total_fishing_hours), 0, log_total_fishing_hours))
  
# checkes 
length(unique(full_data$gear)) == 1 | length(unique(full_data$length_category)) == 1 
nrow(full_data) == 0

#### random forest ----

data_random_forest <- full_data %>% 
  na.omit() %>% 
  mutate(presence = as.factor(presence)) %>% # transform in categorical
  distinct()


# Split the data into training and testing. 80% for training, 20% for testing.
set.seed(123)
samp <- sample(nrow(data_random_forest), 0.5 * nrow(data_random_forest)) 

train <- data_random_forest[samp, ]
# ggplot(train,aes(presence)) + geom_histogram(aes(fill=gear),bins=50, stat="count")
# ggplot(filter(train, presence == 1),aes(presence)) + geom_histogram(aes(fill=gear),bins=50, stat="count")
test <- data_random_forest[-samp, ]

# apply model on train 
model_formula_rf <- formula(
  presence ~ 
    # Categorical/factor predictors
   # flag_fin + # since we're creating a model for each flag country? s
    gear  + 
    length_category +
    meso_id +
    eez_id +
    fao_id +
    ocean +  # Spatial categorical variables
    eez_region_world_bank_7 + # world bank regions
    gov_score + # global fishing index; make sure this is factor and not a numeric variable
    # Total effort predictor (log-transformed)
    log_total_fishing_hours + # honestly not sure if this is necessary. It was a suggestion from Julia to include. Basically a sort of fishing capacity variable. 
    # Continuous predictors
    lon + lat + 
    elevation_m + # depth
    distance_from_port_m + 
    distance_from_shore_m +
    chl_mg_per_m3_mean + 
    chl_mg_per_m3_sd +
    sst_c_mean + 
    sst_c_sd +
    wind_speed_ms_mean + 
    wind_speed_ms_sd +
    enso_index_mean + # el nino data 
    enso_index_sd + # pacific decadal oscillation
    pdo_index_mean +  
    pdo_index_sd + 
    nearest_seamount_distance_m + 
    # Year effect
    year
)

tic()
model <- randomForest(model_formula_rf, data = train, type = "classification", proximity = FALSE, ntree = 100) # , mtry = 5) # proximity = FALSE to speed up the process as no needed for classification (I think), reduce ntree to speed up process but careful not to set too low
toc() # 28 mins
model

qs::qsave(model, here("data/output/FULL_rf_model_train.qs"))

```



## Test to see if predictions look OK

```{r}

# all_predictions <- list()
#     
# # Function to make predictions for a specific combination
# 
#   flags <- unique(hist_fish_data$flag_fin)
#   # gears <- unique(hist_fish_data$gear)
#   # years <- 2015:2017
#   # lengths <- unique(hist_fish_data$length_category)
#   
#   for(yr in years){
#     # for(gear_type in gear){
#     #   for(length in lengths){
#     #     for(flag in flags){
#           
#           
# 
# # flag = "AUS"
# # yr = 2015
# # gear_type = "Lines_Longlines"
# # length = "Over 50m"
#     
#     pred_data <- env_data 
#     imas_data <- hist_fish_data
#   
#       
#       #flag = "USA"
#     flag_data <- imas_data %>% 
#      filter(
#            flag_fin == flag)
#   
#     
#     model <- readRDS(glue(file.path(rdsi_dir, "prep/logistic_regression_models/fitted_model_{flag}.rds")))
#     
# #     eez_levels_in_model <- gsub("^eez_id", "", grep("eez_id", names(model$coefficients), value = TRUE))
# # eez_levels_in_model <- str_replace_all(eez_levels_in_model, "factor\\(eez_id\\)", "") # we only want to make predictions in EEZs which are represented in the gfw data? 
# # 
# #    fao_levels_in_model <- gsub("^fao_id", "", grep("fao_id", names(model$coefficients), value = TRUE))
# # fao_levels_in_model <- str_replace_all(fao_levels_in_model, "factor\\(fao_id\\)", "") # we only want to make predictions in faos which are represented in the gfw data? 
#     
# gear_levels_in_model <- gsub("^gear", "", grep("gear", names(model$coefficients), value = TRUE))
# length_levels_in_model <- gsub("^length_category", "", grep("length_category", names(model$coefficients), value = TRUE)) # adding these in for now so that the predictions will actually run...
# 
#     
#   # Create prediction dataset
#   pred_df <- pred_data %>%
#     left_join(flag_data) %>%
#     filter(gear %in% c(gear_levels_in_model),
#            length_category %in% c(length_levels_in_model))
# 
# 
#   # Make predictions in chunks to manage memory
#   chunk_size <- 10000
#   n_chunks <- ceiling(nrow(pred_df) / chunk_size)
#   predictions <- numeric(nrow(pred_df))
#   
#   pb <- progress_bar$new(
#     format = "Predicting [:bar] :percent eta: :eta",
#     total = n_chunks
#   )
# 
#   for(j in 1:n_chunks) { # not sure if we need to chunk but might as well...
#     # j = 1
#     chunk_start <- (j-1) * chunk_size + 1
#     chunk_end <- min(j * chunk_size, nrow(pred_df))
#     chunk_indices <- chunk_start:chunk_end
#     
#   predicted_props <- mgcv::predict.bam(model, newdata = pred_df[chunk_indices,], type = "response") # should the predicted proportions add to one for each category? 
#   
#   # Ensure predictions are valid proportions
#  # predicted_props <- pmax(0, pmin(1, predicted_props))
#   
#   predictions[chunk_indices] <- predicted_props  
#   
#     pb$tick()
#   }
#   
#   # Add predictions to dataframe
#   
#   predictions[is.na(predictions)] <- 0 # make any NAs into 0
#   
#   pred_df$predicted_proportion <- predictions
#   
#       # probably wanna save these one at a time?
#   
#   write_parquet(pred_df, glue(here("data/output/logistic_regression_predictions/preds_{flag}.parquet"))) # might wanna save on server instead if some get too big for github.
#   
#     #     }
#     #   }
#     # }
#   }
#       
#     
# # lets take a look at our predictions
# all_predictions_df <- read_parquet(glue(here("data/output/logistic_regression_predictions/preds_{flag}.parquet"))) %>%
#   left_join(global_grid)
# 
# test <- all_predictions_df %>%
#   filter(predicted_proportion > 0) %>%
# #  mutate(predicted_proportion = 1) %>%
#   filter(gear == "Lines_Longlines", year == 2015, length_category == "24-50m") %>%
#   dplyr::select(lon, lat, predicted_proportion) %>%
#   rast(., type = "xyz")
#     
# plot(test, col = "red")
# 
#     
#     ## lets look at raw GFW data 
# test_gfw <- qs::qread(file.path(rdsi_dir, "prep/gfw_props/deg_one/all_effort_gear_length_props_2017.qs")) %>%
#       filter(flag_fin == "AUS") %>% 
#       group_by(x, y) %>%
#       summarise(fishing_hours = sum(fishing_hours, na.rm = TRUE)) %>%
#       dplyr::select(x, y, fishing_hours) %>%
#       rast(., type = "xyz")
# 
# plot(test_gfw)

```


**How the model from McDonald et al works:**

**Overview of the Model**
The researchers developed a two-stage hurdle random forest model to predict fishing effort:

Stage 1 (Classification/Extensive Margin): Predicts whether any fishing occurs in a pixel (binary outcome: 0 or 1)
Stage 2 (Regression/Intensive Margin): Predicts the intensity of fishing effort if fishing occurred (continuous outcome: hours/m²)

**Data Used in Both Stages**
Both stages use the same set of input features, which include:

1. MPA implementation features (11 features): Distance to nearest MPA, years since MPA designation, fraction of MPA coverage, etc.
2. Environmental features (12 features): Sea surface temperature, chlorophyll-A, wind speed, etc.
3. Geographic features (7 features): Latitude, longitude, distance to shore, distance to seamount, bathymetry depth, etc.
4. Governance features (6 features): EEZ presence, governance capacity scores, etc.
5. Economic features (3 features): Distance to port, fuel prices, etc.
6. Technological features (2 features): AIS reception quality metrics
7. Residual effects features: Lagged fishing effort from previous years, year variable

**Relationship Between Stage 1 and Stage 2**
Training Process: The stages are trained independently but with different subsets of data:

 - Stage 1 uses all observations from the training dataset
 - Stage 2 only uses observations where fishing effort is non-zero (conditional on fishing occurring)
 
Prediction Process: When making predictions, the outputs from both stages are combined:

 - The paper explicitly states: "We combine each Stage 1 and Stage 2 out-of-sample predictions into full hurdle model predictions. For each observation, we simply multiply the stage 1 classification prediction (0 or 1) by the stage 2 prediction (h/m² of fishing effort)."
  
Data Flow: The Stage 1 model does not directly feed data into Stage 2 during training. Instead:

 - Stage 1 determines IF fishing occurs (probability of fishing)
 - Stage 2 determines HOW MUCH fishing occurs (intensity of fishing)
 - The final prediction multiplies these two outputs together
 
This is a classic hurdle model approach where the first stage models the "hurdle" of whether an event occurs at all, and the second stage models the intensity given that the hurdle is crossed.

**Performance Metrics**
 - Stage 1 (Classification): Evaluated using ROC area-under-curve (~0.97), F1 score (~0.91), precision, and recall
 - Stage 2 (Regression): Evaluated using R² (~0.8), RMSE, and normalized RMSE
 
**Summary**
The two stages work together but are trained separately. Stage 1 doesn't directly feed data into Stage 2 during training, but their outputs are multiplied together during prediction. This approach allows the model to separately handle the binary question of "does fishing occur?" and the continuous question of "how much fishing occurs?" which is particularly useful when dealing with data that has many zeros (areas with no fishing).

