---
title: "Fit prediction model for fishing effort"
output: html_document
date: "2024-12-11"
editor_options: 
  chunk_output_type: console
---

# Summary 

We use the data compiled in the previous scripts to fit a logistic regression model (or a random forest) to predict the prescence (1) or absence (0) of fishing effort in every cell. We will do this for each flag country individually. We will start by testing some individual countries. 

## Environement

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

rm(list = ls())

library(tidyverse)
# install.packages("betareg")
# install.packages("foreach")
# install.packages("doParallel")
# install.packages("tictoc")
# install.packages("terra")
# install.packages("arrow")
library(betareg)
library(foreach)
library(doParallel)
library(here)
library(mgcv)
library(tictoc)
library(progress)
library(terra)
library(glue)
library(arrow)

source(here("R/dir.R"))


pixel_size <- 1 # change this if you want to run 0.5?

```

## Functions to prepare enviro and effort data  

```{r}

if(pixel_size == 1){

# Load prepared data
model_data <- readRDS(here("data/model_features/prepared_data_1deg.rds")) %>%
  mutate(year = as.numeric(year)) %>%
  mutate(presence = 1) # i think we need to adjust the prepared data to include cells which have no fishing effort in them. We can do this in the loop below? 

## read in environmental variables

  global_grid <- read.csv(here("data/model_features/deg_1_x_1/global_grid.csv"))
  
  ocean_data <- read.csv(here("data/model_features/deg_1_x_1/errdap_chl.csv")) %>%
  left_join(read.csv(here("data/model_features/deg_1_x_1/errdap_sst.csv")), by = c("pixel_id", "year")) %>% 
  left_join(read.csv(here("data/model_features/deg_1_x_1/remss_wind.csv")), by = c("pixel_id", "year")) %>%
  filter(year %in% c(2015:2017)) 


  spatial_data <- global_grid %>%
    left_join(read.csv(here("data/model_features/deg_1_x_1/gfw_static_spatial_measures.csv")) %>% dplyr::select(-lat, -lon), by = "pixel_id") %>%
   left_join(read.csv(here("data/model_features/deg_1_x_1/mesopelagiczones/mesopelagiczones.csv")), by = "pixel_id") %>%
   left_join(read.csv(here("data/model_features/deg_1_x_1/eez/eez.csv")), by = "pixel_id") %>%
   left_join(read.csv(here("data/model_features/deg_1_x_1/fao/fao.csv")), by = "pixel_id") %>%
   left_join(read.csv(here("data/model_features/deg_1_x_1/oceans.csv")), by = "pixel_id") %>%
   crossing(., year = c(2015:2017)) 
  
  
  env_data <- spatial_data %>%
    left_join(ocean_data) %>% # cool.
    rename(meso_id = provid) %>%
    dplyr::select(-geometry_wkt)

}else{
  # Load prepared data
model_data <- readRDS(here("data/model_features/prepared_data.rds")) %>%
  mutate(year = as.numeric(year)) %>%
  mutate(presence = 1) # i think we need to adjust the prepared data to include cells which have no fishing effort in them. We can do this in the loop below? 

## read in environmental variables

# Function to prepare prediction data for a given year
prepare_pred_data <- function(yr) {
  # yr = 2015
  message(sprintf("Loading environmental data for year %d...", yr))
  
  # Load data in chunks to manage memory
  chunks <- list()
  
  # Load and process chlorophyll data
  chunks$chl <- read.csv(here("data/model_features/erdMH1chlamday/errdap_2013_2017.csv")) %>% # since we're only training on 2015-2017
    filter(year == yr) %>%
    dplyr::select(pixel_id, chl_mg_per_m3_mean, chl_mg_per_m3_sd)
  
  # Load SST data
  if(yr == 2015){
  chunks$sst <- read.csv(here("data/model_features/ncdcOisst21Agg_LonPM180/errdap_sst_2011_2015.csv")) %>%
    filter(year == yr) %>%
    dplyr::select(pixel_id, sst_c_mean, sst_c_sd)
  
  }else{
      chunks$sst <- read.csv(here("data/model_features/ncdcOisst21Agg_LonPM180/errdap_sst_2016_2020.csv")) %>%
    filter(year == yr) %>%
    dplyr::select(pixel_id, sst_c_mean, sst_c_sd)
  }
  
  # Load and process wind data
  chunks$wind <- read.csv(here("data/model_features/remss_wind/wind_2013_2017.csv")) %>%
    filter(year == yr) %>%
    dplyr::select(pixel_id, wind_speed_mean = wind_speed_ms_mean, wind_speed_sd = wind_speed_ms_sd)
  
  # Load static spatial data
  chunks$spatial <- read.csv(here("data/model_features/gfw_static_spatial_measures.csv")) %>%
  dplyr::select(pixel_id, elevation_m, distance_from_port_m, distance_from_shore_m)
  
  # Load categorical spatial data
  chunks$eez <- read.csv(here("data/model_features/eez/eez.csv")) %>%
    dplyr::select(pixel_id, eez_id)
  chunks$fao <- read.csv(here("data/model_features/fao/fao.csv")) %>%
    dplyr::select(pixel_id, fao_id)
  
  # Load global grid
  global_grid <- read.csv(here("data/model_features/global_grid.csv"))
  
  # Combine all predictor data efficiently
  message("Merging data chunks...")
  pred_data <- global_grid %>%
    left_join(chunks$spatial, by = "pixel_id") %>%
    left_join(chunks$chl, by = "pixel_id") %>%
    left_join(chunks$sst, by = "pixel_id") %>%
    left_join(chunks$wind, by = "pixel_id") %>%
    left_join(chunks$eez, by = "pixel_id") %>%
    left_join(chunks$fao, by = "pixel_id") %>%
    mutate(eez_id = as.factor(eez_id),
           fao_id = as.factor(fao_id)) %>%
    mutate(year = yr)
  
  # # Ensure categorical variables are factors with correct levels
  # pred_data$eez_id <- factor(pred_data$eez_id,
  #                        levels = unique(model$model$eez_id))
  # pred_data$fao_id <- factor(pred_data$fao_id,
  #                               levels = unique(model$model$fao_id))
  
  # Clean up chunks to free memory
  rm(chunks)
  gc()
  
  return(pred_data)
}
  
}


# Get total fishing hours for a specific year from IMAS data
get_historical_total_fishing_hours <- function(yr) {
  # This function would load and process the IMAS data for the given year
  # Note: Implement based on your IMAS data structure
  imas_data <- qs::qread(here("data/int/rousseau_gear_fix.qs")) %>%
    
    ## CN remove artisanal
    filter(sector == "I") %>%
    
    group_by(year, flag_fin = country, gear = gear_new, length_category) %>%
    summarize(
      total_fishing_hours = sum(eff_active_hours, na.rm = TRUE),
      .groups = 'drop'
    ) %>%
    filter(year == yr) %>%
    dplyr::select(flag_fin, year, gear, length_category, total_fishing_hours)
  
  
  return(imas_data)
}


```

## Specify model formula and load data 

```{r}

# Prepare model formula
model_formula <- formula(
  presence ~ 
    # Categorical/factor predictors
   # flag_fin + 
    gear  + 
    length_category +
    factor(meso_id) +  
    factor(eez_id) + 
    factor(fao_id) + 
    factor(ocean) +  # Spatial categorical variables
    # Total effort predictor (log-transformed)
    log_total_fishing_hours + # honestly not sure if this is necessary. It was a suggestion from Julia to include. Basically a sort of fishing capacity variable. 
    # Continuous predictors
    lon + lat + 
    elevation_m + # depth
    distance_from_port_m + 
    distance_from_shore_m +
    chl_mg_per_m3_mean + 
    chl_mg_per_m3_sd +
    sst_c_mean + 
    sst_c_sd +
    wind_speed_ms_mean + 
    wind_speed_ms_sd +
    enso_index_mean + # el nino data 
    enso_index_sd + 
    # Year effect
  # s(year, k=2) # smooth term for year; maybe not necessary? Fishing effort isn't necessarily linear so keeping for now. Does this slow down modeling? 
    year
)

elnino <- read.csv(here("data/model_features/enso_index.csv"))

if(pixel_size == 1){
  
  env_data <- env_data %>%
    left_join(elnino)
  
}else{

  env_data <- prepare_pred_data(2015) %>%
    rbind(., prepare_pred_data(2016)) %>%
    rbind(., prepare_pred_data(2017)) %>%
    left_join(elnino)
  
}

# install.packages("qs")
library("qs")

hist_fish_data <- get_historical_total_fishing_hours(2015) %>%
    rbind(., get_historical_total_fishing_hours(2016)) %>%
    rbind(., get_historical_total_fishing_hours(2017)) %>%
    mutate(log_total_fishing_hours = log1p(total_fishing_hours)) %>%
    filter(total_fishing_hours > 0)
  
flags <- unique(hist_fish_data$flag_fin) # get the flags we need to run models for # 167 of them

unique_combinations <- hist_fish_data %>%
  distinct(year, flag_fin, gear, length_category) %>% # we only want to make models for these combinations since these are what is in the IMAS/FAO data 
  mutate(row_n = row_number())

# test_aus <- unique_combinations %>% filter(flag_fin == "AUS") # ok, AUS for example has 128 different combinations that 1 model will be able to predict. 128*46229 = ~6 million rows being fed into the model. Will this work?? Let's try! 
#   
# 
# setdiff(unique(hist_fish_data$flag_fin), unique(model_data$flag_fin))

env_grid <- env_data %>% 
  dplyr::select(lon, lat) %>% distinct()

# for(flag in flags){
#        
#   # flag = "AUS"
#   
#   
#   if(file.exists(glue(file.path(rdsi_dir, "prep/logistic_regression_models/fitted_model_{flag}.rds")))){
#     cat("exists... next")
#     next()
#   }
#         
#         
# 
# # Fit beta regression model
# message("Fitting logistic regression model...")
# 
# 
#   model_data_flag <- model_data %>%
#     dplyr::select(lon, lat, flag_fin, gear, length_category, year, presence) %>%
#     filter(flag_fin == flag)
# 
#   # test <- model_data_flag %>%
#   #    distinct(year, flag_fin, gear, length_category) # oof... GFW Australia only has data for at least 44 out of 128 categories in rousseau...
# 
#   distinct_cats <- model_data_flag %>%
#     distinct(year, flag_fin, gear, length_category) 
#   
# full_grid <- crossing(env_grid, distinct_cats)
# 
# # Join with environmental data (already included in full_grid)
# full_data <- full_grid %>%
#   left_join(env_data, by = c("lon", "lat", "year")) %>%
#   left_join(model_data_flag) %>%  # Join with fishing presence data
#     mutate(presence = ifelse(is.na(presence), 0, presence)) %>%
#   dplyr::select(-pixel_id) %>%
#     left_join(hist_fish_data) %>%
#     mutate(total_fishing_hours = ifelse(is.na(total_fishing_hours), 0, total_fishing_hours),
#          log_total_fishing_hours = ifelse(is.na(log_total_fishing_hours), 0, log_total_fishing_hours)) # there are some combinations that are not in the IMAS data but in the GFW data, so we'll just put total fishing hours as 0 for these cases... This will just predict 0 for those (i think?), which is fine, since we technically don't have to predict anything for these anyways. 
# 
# # what about if they are in IMAS data but not in GFW? This could be problematic! 
# 
# #ctrl <- list(nthreads = 24) # specify cores; for some reason >=24 cores didn't work, maybe a memory issue
# 
# tic()
# # model <- bam( # trying bam because it is supposed to be good for "very large datasets"
# #   model_formula,
# #   data = full_data,
# #   na.action = na.exclude,
# #  control = ctrl, # I think this does parallelization? 
# #   family = binomial(link = "logit")
# # )
# 
# model <- stats::glm(model_formula, data = full_data, family = binomial(link = "logit"))
# toc() 
# ## with bam(); took ~4 mins for flag country Australia using 20 cores (i think using that many cores...?). 
# ## with stats::glm() took ~ 4 mins for flag country australia using 1 core
# 
# # Save model
# saveRDS(model, glue(file.path(rdsi_dir, "prep/logistic_regression_models/fitted_model_{flag}.rds")))
# 
# }
# 
# 
# # Print model summary
# summary(model)



# random forest instead of logistic regression???? Or maybe try a regular logistic regression using stats::glm without smoothing parameters. maybe those would be faster? Consider speedglm package

```

## Run models in parallel 

```{r}

library(foreach)
library(doParallel)
# install.packages("strex")
library(strex)

flags <- setdiff(flags,  str_before_last(str_after_last(list.files("/home/ubuntu/data_storage/prep/logistic_regression_models/"), "_"), "\\."))

missing_flags <- setdiff(unique(hist_fish_data$flag_fin), unique(model_data$flag_fin))
length(missing_flags)

flags <- setdiff(flags, missing_flags)  ## CN are these countries that have not run from previous iterations? 

# Set up parallel backend
# num_cores <- detectCores() - 1  # Use available cores minus 1 to avoid overloading
num_cores = 10 # to be safe. I tried with 31 and it overloaded. Lets assume ~8gb per model (that was what australia was, so ~16 is the max we could use)
cl <- makeCluster(num_cores)
registerDoParallel(cl)

# Run in parallel 
foreach(flag = flags, .packages = c("dplyr", "glue", "stats", "tictoc", "tidyr")) %dopar% {
  
  # flag = "CIV" 
  
  model_path <- glue(file.path(rdsi_dir, "prep/logistic_regression_models/fitted_model_{flag}.rds"))
  
  if (file.exists(model_path)) {
    cat("Model for", flag, "exists... skipping\n")
    return(NULL)
  }
  
  if(flag %in% missing_flags){
    cat(flag, "not in data... skippings")
    return(NULL)
  }
  
  message("Fitting logistic regression model for ", flag, "...")
  
  model_data_flag <- model_data %>%
    dplyr::select(lon, lat, flag_fin, gear, length_category, year, presence) %>%
    filter(flag_fin == flag)
  
  distinct_cats <- model_data_flag %>%
    distinct(year, flag_fin, gear, length_category)
  
  full_grid <- tidyr::crossing(env_grid, distinct_cats)
  
  full_data <- full_grid %>%
    left_join(env_data, by = c("lon", "lat", "year")) %>%
    left_join(model_data_flag, by = c("lon", "lat", "year", "flag_fin", "gear", "length_category")) %>%
    mutate(presence = ifelse(is.na(presence), 0, presence)) %>%
    dplyr::select(-pixel_id) %>%
    left_join(hist_fish_data) %>%
    mutate(total_fishing_hours = ifelse(is.na(total_fishing_hours), 0, total_fishing_hours),
           log_total_fishing_hours = ifelse(is.na(log_total_fishing_hours), 0, log_total_fishing_hours))
  
  if(length(unique(full_data$gear)) == 1 | length(unique(full_data$length_category)) == 1 ){
  return(NULL)
  
}
  
  if(nrow(full_data) == 0){
        cat(flag, "not in data... skippings")
    return(NULL)
  }
  
  tic()
  model <- stats::glm(model_formula, data = full_data, family = binomial(link = "logit"))
  toc()
  
  # CN commented not to overwrite
  # saveRDS(model, model_path)
}

# Stop cluster
stopCluster(cl)

```

## CN check countries not run or gear combinations not run 

```{r}

## first check data ----

# load full effort data 
trial<- qs::qread(here("data/int/rousseau_gear_fix.qs")) %>% 
  group_by(year, sector) %>% 
  summarise(tot_effort = sum(nom_active)) %>% 
  ungroup()

ggplot(trial, aes(x = year, y = tot_effort, group = sector, color = sector))+
  geom_line()

### artisanal powered is much more than industrial in terms of h fished, not in terms of nom_active - i.e. accounting for power. 

# check ais data

trial<-model_data %>% 
  group_by(year, flag_fin, gear,length_category) %>% 
  summarise(prop = sum(prop_fishing_hours_cell, na.rm = T), 
            eff = sum(total_fishing_hours, na.rm = T))

filter(trial, prop>1)

## why prop > 1?? Also when effort = 0 ... 

### missing combo ----

## how much of the total effort do these missing flags account for? 
length(unique(hist_fish_data$flag_fin)) # 147 when artisanal is not considered. 
missing_flags <- setdiff(unique(hist_fish_data$flag_fin), unique(model_data$flag_fin))
length(missing_flags)

trial<-hist_fish_data %>% 
  mutate(missing_flag = ifelse(flag_fin %in% missing_flags, flag_fin, "not_missing")) %>% 
  group_by(missing_flag) %>% 
  reframe(tot_effort = sum(total_fishing_hours)) %>% 
  mutate(perc_effort = (tot_effort/sum(tot_effort))*100) %>% 
  arrange(-perc_effort)

## with artisanal: missing flags account for 21% of effort in Yannick's data - United Arab Emirates alone accounts for 15%. 
# without artisanal - see function in line ~160: missing flags account for 5% of effort.
sort(unique(hist_fish_data$gear)) # length cat does not mean "artisanal"
sort(unique(model_data$gear)) # same cat here. 

## missing gear combinations
combo_rousseau<-hist_fish_data %>% 
  select(flag_fin, gear, length_category, total_fishing_hours) %>% 
  group_by(flag_fin, gear, length_category) %>%  # remove year from table? 
  reframe(total_fishing_hours = sum(total_fishing_hours)) %>% 
  distinct() %>% 
  mutate(datasetRousseau = "Y") %>% 
  mutate(perc_effort = (total_fishing_hours/sum(total_fishing_hours))*100) %>% 
  arrange(-perc_effort) 

combo_ais<-model_data %>% 
  select(flag_fin, gear, length_category) %>% 
  distinct() %>% 
  mutate(datasetAIS = "Y")

missing_combo <- combo_rousseau %>% full_join(combo_ais) %>% 
  filter(is.na(datasetAIS)) %>% 
  mutate(perc_effort = (total_fishing_hours/sum(hist_fish_data$total_fishing_hours))*100) %>% 
  arrange(-perc_effort) %>% 
  select(-c(datasetRousseau, datasetAIS))

sum(missing_combo$perc_effort)

## missing combo account for 82% of effort in Yannick's data - is this possible?! - no one combo being particularly influential. 
# without artisanal: missing combo account for 36% of effort - no one combo being particularly influential. 

filter(model_data, flag_fin == "CIV", gear == "Trawl_Midwater_or_Unsp", length_category == "24-50m")
filter(model_data, flag_fin == "CIV", gear == "Trawl_Midwater_or_Unsp")
filter(model_data, flag_fin == "CIV")

##### OPTIONS TO RUN BEFORE MODEL FITTING ABOVE ---- 
# if missing year from AIS data (currently removed from analysis above) - assume constant value?
# aggregate up rousseau dataset?


### Also the opposite is problematic - i.e. combo missing in Rousseau. It is not worth modelling these as we don't have country data to spread spatially. BUT see comment in section below

missing_combo2 <- combo_rousseau %>% full_join(combo_ais) %>% 
  filter(is.na(datasetRousseau)) %>% 
  distinct() %>% 
  select(-c(datasetRousseau, datasetAIS)) %>% 
  mutate(id = paste(flag_fin, gear,length_category))

toRemove<-unique(missing_combo2$id)
# 495 combo in AIS that are missing from R dataset. create ID and remove? 

#model_data<-model_data %>% 
 # mutate(id = paste(flag_fin,gear,length_category)) # %>% 
  # filter(!id %in% toRemove) 
  
filter(model_data, id == "DJI Trawl_Midwater_or_Unsp Over 50m")

```

## CN apply model to one country only fo ease of investigation 

```{r}

### prepare data as per function above ---- 
# unique(model_data$flag_fin)

flag = "NZL" # "CIV" no match between r and ias data  
  
model_data_flag <- model_data %>%
  dplyr::select(lon, lat, flag_fin, gear, length_category, year, presence) %>%
  filter(flag_fin == flag)
  
distinct_cats <- model_data_flag %>%
    distinct(year, flag_fin, gear, length_category)
  
full_grid <- tidyr::crossing(env_grid, distinct_cats)
  
full_data <- full_grid %>%
  left_join(env_data, by = c("lon", "lat", "year")) %>%
  left_join(model_data_flag, by = c("lon", "lat", "year", "flag_fin", "gear", "length_category")) %>%
  mutate(presence = ifelse(is.na(presence), 0, presence)) %>%
  dplyr::select(-pixel_id) %>%
  left_join(hist_fish_data) %>%
  mutate(total_fishing_hours = ifelse(is.na(total_fishing_hours), 0, total_fishing_hours),
         log_total_fishing_hours = ifelse(is.na(log_total_fishing_hours), 0, log_total_fishing_hours))
  
# checkes 
length(unique(full_data$gear)) == 1 | length(unique(full_data$length_category)) == 1 
nrow(full_data) == 0

# compare AIS with R combo - NO MATCH for flag = CIV! no need to model this as no R effort data is available... see comment above on removing these combo
# though for countries where some combo are matching - combos in ais not present in R might still add info to the model - we could leave them here but do not consider them in final dataset when info from ais-driven model/s is matched to R data. 

full_data %>% 
  select(gear,length_category) %>% 
  distinct()

hist_fish_data %>% 
  filter(flag_fin == flag) %>% 
  select(gear,length_category) %>% 
  distinct()

### simpler formula ----
# simplify model formula to
model_formula_simpler <- formula(
  presence ~ 
    gear  + 
    length_category +
    # factor(meso_id) +  
    # factor(eez_id) + 
    # factor(fao_id) + 
    # factor(ocean) +  # Spatial categorical variables
    # log_total_fishing_hours + # honestly not sure if this is necessary. It was a suggestion from Julia to include. Basically a sort of fishing capacity variable. 
    # lon + 
    # lat + 
    # elevation_m + # depth
    # distance_from_port_m + 
    # distance_from_shore_m +
    # chl_mg_per_m3_mean + 
    # chl_mg_per_m3_sd +
    sst_c_mean + 
    # sst_c_sd +
    # wind_speed_ms_mean + 
    # wind_speed_ms_sd +
    # enso_index_mean + # el nino data 
    # enso_index_sd + 
    # s(year, k=2) # smooth term for year; maybe not necessary? Fishing effort isn't necessarily linear so keeping for now. Does this slow down modeling? 
    year
)

### GLM ---- 
data_glm<-full_data %>% 
  # done for random forest below 
  # filter(!is.na(sst_c_mean)) %>% # cannot handle NAs - Why? and why are there NAs? is this land?
  na.omit() %>% # these removes all NAs but... 
  mutate(presence = as.factor(presence)) %>% # transform in ctegorical
  distinct()

tic()
model <- stats::glm(model_formula, data = data_glm, family = binomial(link = "logit"))
toc() # full model 191 sec elapsed
# model
par(mfrow = c(2,2))
plot(model) 

### remove outliers and refit the model 




## Error in `contrasts<-`(`*tmp*`, value = contr.funs[1 + isOF[nn]]) : 
# contrasts can be applied only to factors with 2 or more levels
# unique(full_data$gear)
# unique(full_data$length_category)
# when a factor has 1 cat only - possible with gear, length_cat and year as for checks above 

#### random forest ----
# install.packages("randomForest")
library(randomForest)

# https://www.geeksforgeeks.org/ml-classification-vs-regression/

# classification: aims to find decision boundaries that separate classes (e.g. predict weather conditions like “sunny,” “rainy,” or “cloudy.”)
  # 1) A linear decision boundary might separate two classes in a 2D space with a straight line (e.g., logistic regression).
    # Binomial: In binomial Logistic regression, there can be only two possible types of the dependent variables, such as 0 or 1, Pass or Fail, etc. See assumtions 
  # 2) A more complex model, may create non-linear boundaries to better fit intricate datasets.

# regression: focuses on finding the best-fitting line to predict numerical outcomes

# # https://stats.stackexchange.com/questions/327081/random-forest-for-regression-binary-response
# mtcars$vs <- as.factor(mtcars$vs) # set to factor otherwise does regression. 
# classifier <- randomForest( formula = vs~hp+drat, data=mtcars)
# predict(classifier, type="prob")

data_random_forest<-full_data %>% 
  # filter(!is.na(sst_c_mean)) %>% # cannot handle NAs - Why? and why are there NAs? is this land?
  na.omit() %>% # these removes all NAs but... 
  mutate(presence = as.factor(presence)) %>% # transform in ctegorical
  distinct()

# problems with categorical predictors: rf does not like them in model formula as done for glm; also when I convert them in the dataset: Error in randomForest.default(m, y, ...) : 
  # Can not handle categorical predictors with more than 53 categories.
# Some transformation needed https://www.geeksforgeeks.org/how-to-fit-categorical-data-types-for-random-forest-classification/
# though not doing anything seems to produce results too but not sure if right

# data_random_forest<-data_random_forest %>% 
#   mutate(gear = as.factor(gear),  
#          length_category = as.factor(length_category),
#          meso_id = as.factor(meso_id),
#          eez_id = as.factor(eez_id),
#          fao_id = as.factor(fao_id),
#          ocean = as.factor(ocean))

# plot the data 
# map 

# barplot - we don't have enough data on 1s to run a random forest I guess... e.g. the train data might only have 0s?
# ggplot(data_random_forest,aes(presence)) + geom_histogram(aes(fill=gear),bins=50, stat="count")

# Split the data into training and testing. 80% for training, 20% for testing.
set.seed(123)
samp <- sample(nrow(data_random_forest), 0.5 * nrow(data_random_forest)) 
# if the training dataset is too big: Error in randomForest.default(m, y, ...) : 
  # long vectors (argument 28) are not supported in .C
# Can check solutions here: https://stackoverflow.com/questions/24195805/issue-with-randomforest-long-vectors
train <- data_random_forest[samp, ]
# ggplot(train,aes(presence)) + geom_histogram(aes(fill=gear),bins=50, stat="count")
# ggplot(filter(train, presence == 1),aes(presence)) + geom_histogram(aes(fill=gear),bins=50, stat="count")
test <- data_random_forest[-samp, ]

# apply model on train 
model_formula_rf <- formula(
  presence ~ 
    gear  + 
    length_category +
    meso_id +
    eez_id +
    fao_id +
    ocean +  
    # log_total_fishing_hours + 
    lon +
    lat +
    elevation_m + # depth
    distance_from_port_m +
    distance_from_shore_m +
    chl_mg_per_m3_mean +
    chl_mg_per_m3_sd +
    sst_c_mean + 
    sst_c_sd +
    wind_speed_ms_mean +
    wind_speed_ms_sd +
    enso_index_mean + 
    enso_index_sd +
    year
)

tic()
model <- randomForest(model_formula_rf, data = train, type = "classification", proximity = FALSE, ntree = 100) # , mtry = 5) # proximity = FALSE to speed up the process as no needed for classification (I think), reduce ntree to speed up process but careful not to set too low
toc() # 19 sec
model

# validate our model using the test data
prediction <- predict(model, newdata = test, type="prob")

# view results 
# calculate the accuracy of the model


## GC - lets try looking at the testing predictions? 

pred_test <- test %>%
  mutate(pred_prop_pres = predict(model, newdata = ., type="prob")[, "1"]) %>%
   mutate(pred_prop_abs = predict(model, newdata = ., type="prob")[, "1"]) %>%
  filter(pred_prop_pres > 0) %>% # just test filtering for any presence possible
  # mutate(pres = ifelse(pred_prop_pres > 0, 1, 0)) %>%
  mutate(pres = 1) %>%
  distinct(lat, lon, pres)

pred_rast <- pred_test %>%
  rast(., type = "xyz")

plot(pred_rast, col = "red") # ok... looks a little wonky... but I guess we are only predicting to the testing data. Maybe doing predictions on all data would be better? 


pred_test <- full_data %>%
  mutate(pred_prop_pres = predict(model, newdata = ., type="prob")[, "1"]) %>%
   mutate(pred_prop_abs = predict(model, newdata = ., type="prob")[, "1"]) %>%
  filter(pred_prop_pres > 0) %>% # just test filtering for any presence possible
  # mutate(pres = ifelse(pred_prop_pres > 0, 1, 0)) %>%
  mutate(pres = 1) %>%
  distinct(lat, lon, pres)

pred_rast <- pred_test %>%
  rast(., type = "xyz")

plot(pred_rast) # still looks really weird. hard to tell what is going on but its putting fishing predictions from 150 to -150 longitude I think?

```

# CN model diagnostic plots - print  

```{r}

```

# CN predictions 

```{r}

```

## Test to see if predictions look OK

```{r}

all_predictions <- list()
    
# Function to make predictions for a specific combination

  flags <- unique(hist_fish_data$flag_fin)
  # gears <- unique(hist_fish_data$gear)
  # years <- 2015:2017
  # lengths <- unique(hist_fish_data$length_category)
  
  for(yr in years){
    # for(gear_type in gear){
    #   for(length in lengths){
    #     for(flag in flags){
          
          

# flag = "AUS"
# yr = 2015
# gear_type = "Lines_Longlines"
# length = "Over 50m"
    
    pred_data <- env_data 
    imas_data <- hist_fish_data
  
      
      #flag = "USA"
    flag_data <- imas_data %>% 
     filter(
           flag_fin == flag)
  
    
    model <- readRDS(glue(file.path(rdsi_dir, "prep/logistic_regression_models/fitted_model_{flag}.rds")))
    
#     eez_levels_in_model <- gsub("^eez_id", "", grep("eez_id", names(model$coefficients), value = TRUE))
# eez_levels_in_model <- str_replace_all(eez_levels_in_model, "factor\\(eez_id\\)", "") # we only want to make predictions in EEZs which are represented in the gfw data? 
# 
#    fao_levels_in_model <- gsub("^fao_id", "", grep("fao_id", names(model$coefficients), value = TRUE))
# fao_levels_in_model <- str_replace_all(fao_levels_in_model, "factor\\(fao_id\\)", "") # we only want to make predictions in faos which are represented in the gfw data? 
    
gear_levels_in_model <- gsub("^gear", "", grep("gear", names(model$coefficients), value = TRUE))
length_levels_in_model <- gsub("^length_category", "", grep("length_category", names(model$coefficients), value = TRUE)) # adding these in for now so that the predictions will actually run...

    
  # Create prediction dataset
  pred_df <- pred_data %>%
    left_join(flag_data) %>%
    filter(gear %in% c(gear_levels_in_model),
           length_category %in% c(length_levels_in_model))


  # Make predictions in chunks to manage memory
  chunk_size <- 10000
  n_chunks <- ceiling(nrow(pred_df) / chunk_size)
  predictions <- numeric(nrow(pred_df))
  
  pb <- progress_bar$new(
    format = "Predicting [:bar] :percent eta: :eta",
    total = n_chunks
  )

  for(j in 1:n_chunks) { # not sure if we need to chunk but might as well...
    # j = 1
    chunk_start <- (j-1) * chunk_size + 1
    chunk_end <- min(j * chunk_size, nrow(pred_df))
    chunk_indices <- chunk_start:chunk_end
    
  predicted_props <- mgcv::predict.bam(model, newdata = pred_df[chunk_indices,], type = "response") # should the predicted proportions add to one for each category? 
  
  # Ensure predictions are valid proportions
 # predicted_props <- pmax(0, pmin(1, predicted_props))
  
  predictions[chunk_indices] <- predicted_props  
  
    pb$tick()
  }
  
  # Add predictions to dataframe
  
  predictions[is.na(predictions)] <- 0 # make any NAs into 0
  
  pred_df$predicted_proportion <- predictions
  
      # probably wanna save these one at a time?
  
  write_parquet(pred_df, glue(here("data/output/logistic_regression_predictions/preds_{flag}.parquet"))) # might wanna save on server instead if some get too big for github.
  
    #     }
    #   }
    # }
  }
      
    
# lets take a look at our predictions
all_predictions_df <- read_parquet(glue(here("data/output/logistic_regression_predictions/preds_{flag}.parquet"))) %>%
  left_join(global_grid)

test <- all_predictions_df %>%
  filter(predicted_proportion > 0) %>%
#  mutate(predicted_proportion = 1) %>%
  filter(gear == "Lines_Longlines", year == 2015, length_category == "24-50m") %>%
  dplyr::select(lon, lat, predicted_proportion) %>%
  rast(., type = "xyz")
    
plot(test, col = "red")

    
    ## lets look at raw GFW data 
test_gfw <- qs::qread(file.path(rdsi_dir, "prep/gfw_props/deg_one/all_effort_gear_length_props_2017.qs")) %>%
      filter(flag_fin == "AUS") %>% 
      group_by(x, y) %>%
      summarise(fishing_hours = sum(fishing_hours, na.rm = TRUE)) %>%
      dplyr::select(x, y, fishing_hours) %>%
      rast(., type = "xyz")

plot(test_gfw)

```


### ARCHIVE:
this runs by each gear, flag, vessel length and year category and saves a model for each

```{r}
  flags <- unique(hist_fish_data$flag_fin)
  gears <- unique(hist_fish_data$gear)
  years <- 2015:2017
  lengths <- unique(hist_fish_data$length_category)
  
  
unique_combinations <- hist_fish_data %>%
  distinct(year, flag_fin, gear, length_category) %>% # we only want to make models for these combinations since these are what is in the IMAS/FAO data 
  mutate(row_n = row_number())

test_aus <- unique_combinations %>% filter(flag_fin == "AUS")
  

# for(flag in flags){
#   for(gear_type in gears){
#     for(yr in years){
#       for(length in lengths){
        
for(row in 1:nrow(unique_combinations)){
  
#  row = 1 
  
  data <- unique_combinations %>%
    filter(row_n == row)
  
  flag = pull(data, flag_fin)
  gear_type = pull(data, gear)
  length = pull(data, length_category)
  yr = pull(data, year)
  
 # flag = "ISL"
 # gear_type = "Trawl_Midwater_or_Unsp"
 # length = "24-50m"
 # yr = 2017
  
  if(file.exists(glue(file.path(rdsi_dir, "prep/logistic_regression_models/fitted_model_{flag}_{gear_type}_{length}_{yr}.rds")))){
    cat("exists... next")
    next()
  }
        
        

# Fit beta regression model
message("Fitting logistic regression model...")
  

  model_data_flag <- model_data %>%
    dplyr::select(lon, lat, flag_fin, gear, length_category, year, presence) %>%
    filter(flag_fin == flag,
           gear == gear_type, 
           length_category == length, 
           year == yr) 
  
  
# OK we need to make sure that the data which we feel into the model has cells which have NO fishing effort in them. Hence the way we are expanding the grid below. This could cause problems computationally, given the sheer number of rows that will be produced by this, so I'm not sure this is the best way to go about this. I wonder if we should loop through each gear, length, and year combination instead? Trying to run the model with 0 fishing presence cells gives me this error: Error in qr.default(G$X) : too large a matrix for LINPACK


env_grid <- env_data %>% filter (year == yr) %>%
  dplyr::select(lon, lat, year) %>% distinct()

full_grid <- crossing(env_grid, flag_fin = flag, gear = gear_type, length_category = length)

# Join with environmental data (already included in full_grid)
full_data <- full_grid %>%
  left_join(env_data, by = c("lon", "lat", "year"))

# Join with fishing presence data
full_data <- full_data %>%
  left_join(model_data_flag, by = c("lon", "lat", "year", "flag_fin", "gear", "length_category"))

full_data <- full_data %>%
  mutate(presence = ifelse(is.na(presence), 0, presence))

full_data <- full_data %>%
  left_join(hist_fish_data) %>%
  mutate(total_fishing_hours = ifelse(is.na(total_fishing_hours), 0, total_fishing_hours),
         log_total_fishing_hours = ifelse(is.na(log_total_fishing_hours), 0, log_total_fishing_hours)) # there are some combinations that are not in the IMAS data but in the GFW data, so we'll just put total fishing hours as 0 for these cases... This will just predict 0 for those, which is fine, since we technically don't have to predict anything for these anyways. 

# what about if they are in IMAS data but not in GFW? 

ctrl <- list(nthreads = 20) # specify cores; for some reason >=24 cores didn't work, maybe a memory issue

tic()
model <- gam(
  model_formula,
  data = full_data,
  na.action = na.exclude,
  control = ctrl, # I think this does parallelization? 
  family = binomial(link = "logit")
)
toc() # took 2.5 minutes for USA, seine, over 50m, 2017; took 2.5 mins for ISL, midwater trawls, 24-50m, 2017

# Save model
saveRDS(model, glue(file.path(rdsi_dir, "prep/logistic_regression_models/fitted_model_{flag}_{gear_type}_{length}_{yr}.rds")))

}
      
  #       }
  #     }
  #   }
  # }

# Print model summary
summary(model)

```

### ARCHIVE 2 

```{r}

global_grid <- read.csv(here("data/model_features/global_grid.csv"))



all_predictions <- list()
    
# Function to make predictions for a specific combination

  flags <- unique(hist_fish_data$flag_fin)
  gears <- unique(hist_fish_data$gear)
  years <- 2015:2017
  lengths <- unique(hist_fish_data$length_category)
  
  for(yr in years){
    for(gear_type in gear){
      for(length in lengths){
        for(flag in flags){
          
          

# flag = "RUS"
# yr = 2015
# gear_type = "Lines_Longlines"
# length = "Over 50m"
    
    pred_data <- env_data %>%
      filter(year == yr)
    imas_data <- hist_fish_data %>%
      filter(year == yr)
  
      
      #flag = "USA"
    flag_data <- imas_data %>% 
    filter(year == yr, 
           flag_fin == flag) %>%
    filter(gear == gear_type,
           length_category == length) 
  
    
    model <- readRDS(glue(here("data/output/logistic_regression_outputs/fitted_model_{flag}_{gear_type}_{length}_{yr}.rds")))
    
#     eez_levels_in_model <- gsub("^eez_id", "", grep("eez_id", names(model$coefficients), value = TRUE))
# eez_levels_in_model <- str_replace_all(eez_levels_in_model, "factor\\(eez_id\\)", "") # we only want to make predictions in EEZs which are represented in the gfw data? 
# 
#    fao_levels_in_model <- gsub("^fao_id", "", grep("fao_id", names(model$coefficients), value = TRUE))
# fao_levels_in_model <- str_replace_all(fao_levels_in_model, "factor\\(fao_id\\)", "") # we only want to make predictions in faos which are represented in the gfw data? 

    
  # Create prediction dataset
  pred_df <- pred_data %>%
    left_join(flag_data)


  # Make predictions in chunks to manage memory
  chunk_size <- 10000
  n_chunks <- ceiling(nrow(pred_df) / chunk_size)
  predictions <- numeric(nrow(pred_df))
  
  pb <- progress_bar$new(
    format = "Predicting [:bar] :percent eta: :eta",
    total = n_chunks
  )

  for(j in 1:n_chunks) { # not sure if we need to chunk but might as well...
    # j = 1
    chunk_start <- (j-1) * chunk_size + 1
    chunk_end <- min(j * chunk_size, nrow(pred_df))
    chunk_indices <- chunk_start:chunk_end
    
  predicted_props <- mgcv::predict.gam(model, newdata = pred_df[chunk_indices,], type = "response") # should the predicted proportions add to one for each category? 
  
  # Ensure predictions are valid proportions
 # predicted_props <- pmax(0, pmin(1, predicted_props))
  
  predictions[chunk_indices] <- predicted_props  
  
    pb$tick()
  }
  
  # Add predictions to dataframe
  
  predictions[is.na(predictions)] <- 0 # make any NAs into 0
  
  pred_df$predicted_proportion <- predictions
  


      
      # probably wanna save these one at a time instead of in a list in the environment? 
  
  write_parquet(pred_df %>% filter(predicted_proportion > 0), glue(here("data/output/logistic_regression_predictions/preds_{flag}_{gear_type}_{length}_{yr}.parquet")))
  
        }
      }
    }
  }
      
    
# lets take a look at our predictions
all_predictions_df <- read_parquet(glue(here("data/output/logistic_regression_predictions/preds_{flag}_{gear_type}_{length}_{yr}.parquet"))) %>%
  left_join(global_grid)

test <- all_predictions_df %>%
  filter(predicted_proportion > 0) %>%
#  mutate(predicted_proportion = 1) %>%
  dplyr::select(lon, lat, predicted_proportion) %>%
  rast(., type = "xyz")
    
plot(test)

    
    ## lets look at raw GFW data 
test_gfw <- qs::qread(file.path(rdsi_dir, "prep/gfw_props/deg_half/all_effort_gear_length_props_2017.qs")) %>%
      filter(flag_fin == "ISL", gear == "Trawl_Midwater_or_Unsp", length_category == "24-50m") %>% 
      group_by(x, y) %>%
      summarise(fishing_hours = sum(fishing_hours, na.rm = TRUE)) %>%
      dplyr::select(x, y, fishing_hours) %>%
      rast(., type = "xyz")

plot(test_gfw)
```

