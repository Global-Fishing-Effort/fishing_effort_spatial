---
title: "Predict props to cells per flag, year, gear, length"
output: html_document
date: "2024-12-11"
---

# Summary

We use the GAM model fit in the previous script to predict the proportion of fishing effort occurring in each cell per our categorical variables (gear, length, flag country). To do this, we create a dummy dataset where any flag country can theoretically fish in any cell, and predict for that. In theory, the model should predict proxy-intensity values to each cell where fishing is most likely.. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(terra)
library(foreach)
library(doParallel)
library(progress)
library(pryr)  # for memory tracking
library(glue)
library(qs)
library(here)

source(here("R/dir.R"))
```

```{r}
# Load model and prediction function
model <- readRDS(here("data/output/fitted_model.rds"))
#predict_proportions <- readRDS("model/prediction_functions.rds")

  global_grid <- read.csv(here("data/model_features/global_grid.csv"))



# Function to prepare prediction data for a given year
prepare_pred_data <- function(yr) {
  # yr = 2015
  message(sprintf("Loading environmental data for year %d...", yr))
  
  # Load data in chunks to manage memory
  chunks <- list()
  
  # Load and process chlorophyll data
  chunks$chl <- read.csv(here("data/model_features/erdMH1chlamday/errdap_2013_2017.csv")) %>% # since we're only training on 2015-2017
    filter(year == yr) %>%
    dplyr::select(pixel_id, chl_mg_per_m3_mean, chl_mg_per_m3_sd)
  
  # Load SST data
  if(yr == 2015){
  chunks$sst <- read.csv(here("data/model_features/ncdcOisst21Agg_LonPM180/errdap_sst_2011_2015.csv")) %>%
    filter(year == yr) %>%
    dplyr::select(pixel_id, sst_c_mean, sst_c_sd)
  
  }else{
      chunks$sst <- read.csv(here("data/model_features/ncdcOisst21Agg_LonPM180/errdap_sst_2016_2020.csv")) %>%
    filter(year == yr) %>%
    dplyr::select(pixel_id, sst_c_mean, sst_c_sd)
  }
  
  # Load and process wind data
  chunks$wind <- read.csv(here("data/model_features/remss_wind/wind_2013_2017.csv")) %>%
    filter(year == yr) %>%
    dplyr::select(pixel_id, wind_speed_mean = wind_speed_ms_mean, wind_speed_sd = wind_speed_ms_sd)
  
  # Load static spatial data
  chunks$spatial <- read.csv(here("data/model_features/gfw_static_spatial_measures.csv")) %>%
  dplyr::select(pixel_id, elevation_m, distance_from_port_m, distance_from_shore_m)
  
  # Load categorical spatial data
  chunks$eez <- read.csv(here("data/model_features/eez/eez.csv")) %>%
    select(pixel_id, eez_id)
  chunks$fao <- read.csv(here("data/model_features/fao/fao.csv")) %>%
    select(pixel_id, fao_id)
  
  # Load global grid
  global_grid <- read.csv(here("data/model_features/global_grid.csv"))
  
  # Combine all predictor data efficiently
  message("Merging data chunks...")
  pred_data <- global_grid %>%
    left_join(chunks$spatial, by = "pixel_id") %>%
    left_join(chunks$chl, by = "pixel_id") %>%
    left_join(chunks$sst, by = "pixel_id") %>%
    left_join(chunks$wind, by = "pixel_id") %>%
    left_join(chunks$eez, by = "pixel_id") %>%
    left_join(chunks$fao, by = "pixel_id") %>%
    mutate(eez_id = as.factor(eez_id),
           fao_id = as.factor(fao_id))
  
  # # Ensure categorical variables are factors with correct levels
  # pred_data$eez_id <- factor(pred_data$eez_id,
  #                        levels = unique(model$model$eez_id))
  # pred_data$fao_id <- factor(pred_data$fao_id,
  #                               levels = unique(model$model$fao_id))
  
  # Clean up chunks to free memory
  rm(chunks)
  gc()
  
  return(pred_data)
}



# Get total fishing hours for a specific year from IMAS data
get_historical_total_fishing_hours <- function(yr) {
  # This function would load and process the IMAS data for the given year
  # Note: Implement based on your IMAS data structure
  imas_data <- qs::qread(here("data/int/rousseau_gear_fix.qs")) %>%
    group_by(year, flag_fin = country, gear = gear_new, length_category) %>%
    summarize(
      total_fishing_hours = sum(eff_active_hours, na.rm = TRUE),
      .groups = 'drop'
    ) %>%
    filter(year == yr) %>%
    select(flag_fin, year, gear, length_category, total_fishing_hours)
  
  
  return(imas_data)
}

eez_levels_in_model <- gsub("^eez_id", "", grep("eez_id", names(model$coefficients), value = TRUE))
eez_levels_in_model <- str_replace_all(eez_levels_in_model, "factor\\(eez_id\\)", "") # we only want to make predictions in EEZs which are represented in the gfw data? 

flag_levels_in_model <- gsub("^flag_fin", "", grep("flag_fin", names(model$coefficients), value = TRUE)) # ok, this might be a problem. How do we make predictions when a flag country is not represented in GFW. For example Armenia is not in GFW but is in IMAS data...

    all_predictions <- list()

# Function to make predictions for a specific combination
  for(yr in c(2015:2017)){

  # flag = "HKG"
  # yr = 2017
    pred_data <- prepare_pred_data(yr)
    imas_data <- get_historical_total_fishing_hours(yr)
  
 for(flag in unique(imas_data$flag_fin)){
   
   if(!(flag %in% flag_levels_in_model)){ # I think this might be problematic; there are flags in the IMAS data which are not in the GFW data. If we can't predict for those flags, what do we do for those? Use some sort of larger regional model instead? 
     next()
   }
    
  flag_data <- imas_data %>% 
    filter(year == yr, 
           flag_fin == flag) %>%
    mutate(row_n = row_number()) # now we'll loop through each of the combinations of gear, flag, vessel length and year.
  
  for(i in 1:nrow(flag_data)){
  
  # i = 1
    
    flag_data_i <- flag_data %>%
            filter(row_n == i) %>%
      dplyr::select(-row_n)
    
    length_cat_i = unique(flag_data_i$length_category)
    gear_i = unique(flag_data_i$gear)
    total_hours_i = flag_data_i %>% 
      pull(total_fishing_hours)
    
  # Create prediction dataset
  pred_df <- pred_data %>%
    mutate(
      flag_fin = flag,
      gear = gear_i,
      length_category = length_cat_i,
      total_fishing_hours = total_hours_i,
      year = yr  # Reference year
    ) %>%
    mutate(log_total_fishing_hours = log1p(total_fishing_hours)) %>%
          filter(eez_id %in% eez_levels_in_model)

  # Make predictions in chunks to manage memory
  chunk_size <- 10000
  n_chunks <- ceiling(nrow(pred_df) / chunk_size)
  predictions <- numeric(nrow(pred_df))
  
  pb <- progress_bar$new(
    format = "Predicting [:bar] :percent eta: :eta",
    total = n_chunks
  )

  for(j in 1:n_chunks) {
    # j = 1
    chunk_start <- (j-1) * chunk_size + 1
    chunk_end <- min(j * chunk_size, nrow(pred_df))
    chunk_indices <- chunk_start:chunk_end
    
  predicted_props <- mgcv::predict.gam(model, newdata = pred_df[chunk_indices,], type = "response") # should the predicted proportions add to one for each category? 
  
  # Ensure predictions are valid proportions
  predicted_props <- pmax(0, pmin(1, predicted_props))
  
  predictions[chunk_indices] <- predicted_props  
  
    pb$tick()
  }
  
  # Add predictions to dataframe
  
  pred_df$predicted_proportion <- predictions
  
  
        # Store results in a nested list
      if (!is.list(all_predictions[[as.character(yr)]])) {
        all_predictions[[as.character(yr)]] <- list()
      }
      if (!is.list(all_predictions[[as.character(yr)]][[flag]])) {
        all_predictions[[as.character(yr)]][[flag]] <- list()
      }

      all_predictions[[as.character(yr)]][[flag]][[i]] <- pred_df %>%
        filter(!is.na(predicted_proportion)) %>%
        dplyr::select(pixel_id, predicted_proportion, flag_fin, year, gear, length_category)

  
    }
  
   }
  }
    
    # ok so this is gonna be a huge dataset... we might need to save it in chunks instead 
all_predictions_df <- all_predictions %>%
  map(~ map(., ~ bind_rows(.))) %>% # Combine data frames at the [[i]] level for each flag
  map(bind_rows) %>%                # Combine data frames at the [[flag]] level for each year
  bind_rows()   
    
  imas_data_ind <-   qs::qread(here("data/int/rousseau_gear_fix.qs")) %>%
    filter(sector == "I") %>%
    group_by(year, flag_fin = country, gear = gear_new, length_category) %>%
    summarize(
      total_fishing_hours = sum(eff_active_hours, na.rm = TRUE),
      .groups = 'drop'
    ) %>%
   # filter(year == yr, flag_fin == flag) %>%
    select(flag_fin, gear, year, length_category, total_fishing_hours)
    
    # Normalize predictions within each pixel to sum to 1
    normalized_predictions <- all_predictions_df %>%
      group_by(flag_fin, gear, year, length_category) %>%
      mutate(
        total_props = sum(predicted_proportion, na.rm = TRUE)
      ) %>%
      ungroup() %>%
      mutate(normalized_prop = predicted_proportion/total_props) %>%
      left_join(imas_data_ind) %>% ## need to change this to just industrial
      mutate(fishing_hours = normalized_prop*total_fishing_hours) %>%
      filter(!is.na(fishing_hours))
    
    qs::qsave(normalized_predictions, file.path(rdsi_dir, glue("prep/gam_predictions/all_preds.qs")))
    
    unique(all_predictions_df$flag_fin) # uh ok why are there only 5 flags? 
    
    test <- all_predictions_df %>%
      filter(flag_fin == "HKG", year == 2017, gear == "Trawl_Midwater_or_Unsp", length_category == "Over 50m") %>%
      left_join(global_grid) %>%
      dplyr::select(lon, lat, predicted_proportion) %>%
      rast(., type = "xyz") # LOL ok so this obviously didn't work. It is predicting across the whole globe. And looking at other countries/gears/vessel lengths they all look really similar. So maybe the non-gfw data is influencing the predictions more than the actual fishing data? Should we run individual models for each flag country/gear/vessel length? 
    plot(test)

    test <- normalized_predictions %>%
      filter(flag_fin == "HKG", year == 2017, gear == "Trawl_Midwater_or_Unsp", length_category == "Over 50m") %>%
    dplyr::select(pixel_id, fishing_hours) %>%
      left_join(global_grid) %>%
      dplyr::select(lon, lat, fishing_hours) %>%
      rast(., type = "xyz")
    plot(test)
    
    ## lets look at raw GFW data 
    test_gfw <- qs::qread(file.path(rdsi_dir, "prep/gfw_props/deg_half/all_effort_gear_length_props_2017.qs")) %>%
      filter(flag_fin == "HKG", gear == "Trawl_Midwater_or_Unsp", length_category == "24-50m") %>% 
      group_by(x, y) %>%
      summarise(fishing_hours = sum(fishing_hours, na.rm = TRUE)) %>%
      dplyr::select(x, y, fishing_hours) %>%
      rast(., type = "xyz")
    

    # 
    # test_gfw_raw <- qs::qread(file.path(rdsi_raw_dir, "global_fishing_watch/apparent_fishing_hours_mmsi/v2_aggregated/all_effort_2015.qs")) %>%
    #   filter(flag_gfw == "USA") %>%
    #   group_by(vessel_class_gfw, length_m_registry) %>%
    #   summarise(count_n = n_distinct(cell_ll_lon))
  
  #qs::qsave(flag_year_predictions, here(glue("data/output/predictions/{flag}_{yr}_preds.qs")))
   # qs::qsave(flag_year_predictions, file.path(rdsi_dir, glue("prep/gam_predictions/{flag}_{yr}_preds.qs")))

  
  


```


```{r}

# # Function to save predictions as raster efficiently
# save_as_raster <- function(pred_df, filename) {
#   # Create raster from predictions
#   message("Creating raster...")
#   coordinates(pred_df) <- ~lon+lat
#   gridded(pred_df) <- TRUE
#   
#   # Convert to raster and save
#   raster <- raster(pred_df)
#   writeRaster(raster, filename, overwrite=TRUE)
#   
#   # Clean up to free memory
#   rm(pred_df, raster)
#   gc()
# }
# 
# # Function to make predictions for a specific year
# predict_year <- function(year) {
#   message(sprintf("\nMaking predictions for year %d", year))
#   report_memory("Start of year prediction")
#   
#   # Get prediction data for the year
#   pred_data <- prepare_pred_data(year)
#   report_memory("After preparing prediction data")
#   
#   # Get historical total fishing hours
#   total_efforts <- get_historical_total_fishing_hours(year)
#   
#   # Create output directory
#   year_dir <- sprintf("model/predictions/%d", year)
#   dir.create(year_dir, recursive = TRUE, showWarnings = FALSE)
#   
#   # Setup parallel processing
#   cores <- parallel::detectCores() - 1
#   cl <- makeCluster(cores)
#   registerDoParallel(cl)
#   
#   # Create progress bar for combinations
#   n_combinations <- nrow(total_efforts)
#   pb <- progress_bar$new(
#     format = "Processing combinations [:bar] :percent eta: :eta",
#     total = n_combinations
#   )
#   
#   # Make predictions for each combination
#   for(i in 1:nrow(total_efforts)) {
#     combo <- total_efforts[i,]
#     
#     # Make predictions
#     pred_df <- predict_combination(
#       pred_data,
#       flag = combo$flag_fin,
#       gear = combo$gear,
#       sector = combo$sector,
#       length_cat = combo$length_category,
#       total_hours = combo$total_fishing_hours,
#       year = year
#     )
#     
#     # Save predictions
#     filename <- sprintf("%s/pred_%s_%s_%s_%s.tif",
#                        year_dir,
#                        combo$flag_fin, combo$gear, combo$sector,
#                        combo$length_category)
#     
#     save_as_raster(pred_df, filename)
#     
#     # Clean up to free memory
#     rm(pred_df)
#     gc()
#     
#     pb$tick()
#     report_memory(sprintf("After combination %d/%d", i, n_combinations))
#   }
#   
#   stopCluster(cl)
#   
#   # Clean up year-specific data
#   rm(pred_data, total_efforts)
#   gc()
#   
#   message(sprintf("Completed predictions for year %d", year))
#   report_memory("End of year prediction")
# }
# 
# message("Prediction script ready. Use predict_year() function for specific years.")

```

