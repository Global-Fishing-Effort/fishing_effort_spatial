---
title: "Aggregate data layers to 1 by 1 degree where necessary"
author: "Gage Clawson (IMAS)"
date: '`r format(Sys.time(), "%m/%d/%Y")`'
output: 
  pdf_document: 
    number_sections: yes
    toc: true
    toc_depth: 4
editor_options: 
  chunk_output_type: console
---

# Summary 

Check 1x1 degree data downloaded from McDonald et al. 2024

```{r}
# Load all necessary packages
library(tidyverse)
library(sf)
library(stars)
library(rerddap)
library(rnaturalearth)
library(glue)
library(scico)
library(lubridate)
library(furrr)
library(here)
library(terra)
library(fasterize)
library(raster)
library(janitor)

source(here::here("prep/02_prep_spatial_layers_model/_functions_data_wrangling_erddap.R"))
source(here("R/dir.R"))

# Set the data directory. This specifies the folder in our drive where the data can be found. 

data_directory <- rdsi_raw_dir

theme_set(theme_minimal() +
            theme(axis.title.y = element_text(angle = 0,vjust=0.5),
                  strip.background = element_blank(),
                  strip.text.y = element_text(angle=0),
                  strip.text.y.right = element_text(angle=0),
                  strip.text.y.left = element_text(angle=0),
                  panel.grid = element_blank(),
                  panel.background = element_blank(),
                  panel.grid.minor = element_blank(),
                  panel.grid.major = element_blank()))

# Set mapping projection
map_projection <- "+proj=eqearth +datum=WGS84 +wktext"
# Create global land sf object for mapping
world_plotting <- ne_countries(scale = "small", returnclass = "sf")  %>%
  dplyr::select(geometry)

# Get high-res ocean data
ocean <- ne_download(scale = 50, type = 'ocean', category = 'physical',returnclass = "sf") %>%
  dplyr::select(geometry)

```

# Defining our global grid

```{r}
pixel_size <- 1

# Create polygon rectangle of globe
# This will serve as basis of grid
global_polygon_sf <-
  tibble(lon = c(-180,-180,180,180,-180),
       lat = c(-90,90,90,-90,-90))%>%
  as.matrix() %>%
  list(.) %>%
  st_polygon() %>%
  st_sfc(crs = "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs") %>%
  st_as_sf()

# Start with global ocean
starting_shape <- ocean

data_grid <- data.table::fread(here::here("data/model_features/deg_1_x_1/global_grid.csv")) %>%
  st_as_sf(wkt = "geometry_wkt",
           crs = "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs") %>%
              mutate(pixel_area_m2 = sf::st_area(geometry_wkt) %>%
                 units::drop_units())

```


```{r}

spatial_df <- read.csv(here("data/model_features/deg_1_x_1/gfw_static_spatial_measures.csv")) # static layers depth, distance to shore, distance to port. 

grid_df <- read.csv(here("data/model_features/deg_1_x_1/global_grid.csv")) # 1 by 1 grid

oceans_df <- read.csv(here("data/model_features/deg_1_x_1/oceans.csv")) 


```


Prep 2015 SST, CHL, data and append to data in model features folder 

```{r}
## start with prepping 1 by 1 sst data for 2015 only. Then we will appead it onto the existing data downloaded from the paper
# Spatially aggregate SST and SST anomaly data
spatially_aggregate_errdap_data_wrapper(dataset_name = "ncdcOisst21Agg_LonPM180",
                                        spatial_aggregation = data_grid,
                                        years = 2014, # choose years you want to run
                                        run_parallel = TRUE)


# temporally aggregate SST and SST anomaly data

temporally_aggregate_errdap_data_wrapper(dataset_name = "ncdcOisst21Agg_LonPM180",
                                         temporal_aggregation = "year",
                                         run_parallel = TRUE,
                                         years_per_chunk = 5)

old_sst <- read.csv(here("data/model_features/deg_1_x_1/errdap_sst.csv")) %>%
  filter(year != "2015")

test <- old_sst %>% filter(year == 2016) # ok cool

new_sst <- read.csv("data/model_features/deg_1_x_1/ncdcOisst21Agg_LonPM180/errdap_2014_2015.csv")

rewrite_sst <- rbind(new_sst, old_sst)

write.csv(rewrite_sst, here("data/model_features/deg_1_x_1/errdap_sst.csv"), row.names = FALSE)


# Spatially aggregate chl data for 2015
spatially_aggregate_errdap_data_wrapper(dataset_name = "erdMH1chlamday",
                                        spatial_aggregation = data_grid,
                                        years = 2014, # choose years you want to run
                                        run_parallel = TRUE)

temporally_aggregate_errdap_data_wrapper(dataset_name = "erdMH1chlamday",
                                         temporal_aggregation = "year",
                                         run_parallel = TRUE,
                                         years_per_chunk = 5)
 
old_chl <- read.csv(here("data/model_features/deg_1_x_1/errdap_chl.csv")) %>%
  filter(year != 2015)

new_chl <- read.csv("data/model_features/deg_1_x_1/erdMH1chlamday/errdap_2014_2015.csv")


rewrite_chl <- rbind(new_chl, old_chl)

write.csv(rewrite_chl, here("data/model_features/deg_1_x_1/errdap_chl.csv"), row.names = FALSE)


```

Prep 2014-2015 Wind data and append to data in model features folder 

```{r}
## now do wind for 2015

# Process data; spatially aggregate
# Do it in parallel?
run_parallel <- TRUE


tmp_data_directory <- glue::glue("{data_directory}/remss/clean/spatially_aggregated_{pixel_size}_degree")
 if(!dir.exists(tmp_data_directory)) dir.create(tmp_data_directory)
 if(run_parallel) plan(multisession) else plan(sequential)

years = 2014

 pattern <- paste(years, collapse = "|")
 
list.files(glue("{data_directory}/remss/wind-ccmp"), pattern = pattern) %>%
   future_map_dfr(function(tmp_file_name){
         tryCatch({
     
     # tmp_file_name <- "wind_1993-03-07.nc"
     date_tmp <- tmp_file_name %>%
       # Extract date
       stringr::str_replace(glue::glue("wind_"),"") %>% 
       stringr::str_remove(".nc") %>%
       lubridate::date()
     
     processed_file_name <- glue::glue("{tmp_data_directory}/{date_tmp}.csv")
     
          message(glue::glue("Processing file: {tmp_file_name}"))
     
     # If already processed, don't do it again
     if(file.exists(processed_file_name)) {
       message(glue::glue("File already processed: {processed_file_name}"))
       return()
     }
     
 glue::glue("{data_directory}/remss/wind-ccmp/{tmp_file_name}") %>%
       stars::read_ncdf() %>%
       # Calculate wind speed from u and v components, using pythagorean theorum
       dplyr::mutate(wind_speed_m_s = sqrt(uwnd^2 + vwnd^2)) %>%
       # Get rid of these
       dplyr::select(-uwnd,-vwnd,-nobs) %>% 
       # Data are for every 6 hours, so take mean for entire day
       aggregate(FUN = mean, by = "1 day") %>% 
       # Convert stars to raster, so we can use exactextractr::exact_extract
       as("Raster") %>%
       # Need to rotate from 0-360 to -180-180, since original NC is provided in 0-360
       raster::rotate() %>%
       # Let's spatially aggregate by taking the mean value for each of our pixels
       exactextractr::exact_extract(data_grid,
                                    # From the help: "mean - the mean cell value, weighted by the fraction of each cell that is covered by the polygon"
                                    "mean",
                                    # Include pixel_id column so we can match on it later
                                    append_cols = "pixel_id",
                                    progress = FALSE) %>% 
       # Don't save data that are just NAs
       dplyr::filter(!dplyr::if_all(-c(pixel_id),is.na)) %>%
       dplyr::rename(wind_speed_ms_mean = mean) %>%
       dplyr::mutate(date = date_tmp) %>%
       data.table::fwrite(processed_file_name)
     return(NULL)

    }, error = function(e) {
      # Log the error and skip the file
      message(glue::glue("Error processing file {tmp_file_name}: {e$message}"))
      return(NULL)
    })
   }, .options = furrr_options(globals=c("data_directory","data_grid","tmp_data_directory"),
                               seed = 101),.progress=TRUE)


# temporally aggregate data 

run_parallel <- TRUE

tmp_data_directory <- glue::glue("{data_directory}/remss/clean/spatially_aggregated_{pixel_size}_degree")

if(run_parallel) plan(multisession) else plan(sequential)

# Create sequence of start years for each 5-year chunk
start_years <- seq(2014, 2014, by = 5)

# Process each 5-year chunk
for(start_year in start_years) {
  # start_year = 1993
  end_year <- min(start_year + 4, 2024)  # Ensure we don't go beyond 2024
  years_to_process <- start_year:end_year
  
  # Get files for this 5-year chunk
  chunk_files <- list.files(tmp_data_directory, pattern = paste0(years_to_process, collapse = "|"))
  
chunk_data <- chunk_files %>% 
  future_map(function(file_temp) {
    # Construct the full path
    file_path <- glue::glue("{tmp_data_directory}/{file_temp}")
    
    # Read the file
    data <- data.table::fread(file_path)
    
    # Check if the file has rows
    if (nrow(data) == 0) {
      return(NULL)  # Return NULL for files with no rows
    }
    
    # Return the data if it has rows
    data
  }, .options = furrr_options(globals = c("tmp_data_directory"),
                               seed = 101), 
     .progress = TRUE) %>%
  
  # Remove NULL entries (from files with no rows)
  purrr::compact() %>%

  # Combine the data into one table
  data.table::rbindlist() %>%

  # Transform and process
  collapse::ftransform(year = lubridate::year(date)) %>%
  dplyr::select(-date) %>%
  dplyr::rename_with(~ gsub('_mean', '', .x)) %>%
  collapse::fgroup_by(pixel_id, year) %>% {
    collapse::add_vars(
      collapse::add_stub(collapse::fmean(., keep.group_vars = TRUE), "_mean", pre = FALSE, cols = -c(1, 2)),
      collapse::add_stub(collapse::fsd(., keep.group_vars = FALSE), "_sd", pre = FALSE)
    )
  }
  
  data.table::fwrite(chunk_data, here::here(glue::glue("data/model_features/deg_1_x_1/remss_wind/wind_{start_year}_{end_year}.csv")))

  
  # Clear chunk data to free memory
  rm(chunk_data)
  gc()
}


old_wind <- read.csv(here("data/model_features/deg_1_x_1/remss_wind.csv")) %>%
  filter(year != 2015)

new_wind <- read.csv("data/model_features/deg_1_x_1/remss_wind/wind_2014_2018.csv") # this is actually only 2015

rewrite_wind <- rbind(new_wind, old_wind)

write.csv(rewrite_wind, here("data/model_features/deg_1_x_1/remss_wind.csv"), row.names = FALSE)

```


Need to prep 1 by 1 EEZ, FAO, and mesopelagic data 

# Determine EEZ of each cell 

Use Marine Region's [Maritime Boundaries Geodatabase: Maritime Boundaries and Exclusive Economic Zones (200NM), version 12]

```{r}

analysis_projection <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"
eez <- st_read(file.path(rdsi_raw_dir, "marine_regions/World_EEZ_v12_20231025"), layer = 'eez_v12')

eez_lookup_ids <- eez %>% 
      filter(POL_TYPE !=  "Overlapping claim") %>% 
  st_drop_geometry() %>%
  dplyr::distinct(eez_sovereign = ISO_SOV1, eez_id = MRGID_SOV1) %>%
        filter(eez_sovereign != "ATA")  %>% 
  add_row(eez_sovereign = "High seas", eez_id = 99999)

write.csv(eez_lookup_ids, here("data/model_features/deg_1_x_1/eez/eez_lookup.csv"), row.names = FALSE)


  eez <- eez %>%
    filter(POL_TYPE !=  "Overlapping claim") %>%
    dplyr::select(eez_sovereign = ISO_SOV1) %>%
    # Don't include Antarctica - classify it as high seas instead
    filter(eez_sovereign != "ATA")  %>% 
    sf::st_wrap_dateline(options = c("WRAPDATELINE=YES", "DATELINEOFFSET=180"), quiet = TRUE) %>% 
    sf::st_transform(analysis_projection) %>%
    sf::st_make_valid() %>%
    mutate(eez_area_m2 = sf::st_area(geometry)%>%
             units::drop_units())
  


  # Now intersect our grids with EEZs, calculate fraction each grid overlaps with EEZ
  # And only pick one EEZ per grid
  data_grid_eez <- data_grid %>% 
    sf::st_intersection(eez) 
  
  save_data_grid_eez <- data_grid_eez %>%
    st_drop_geometry()
  
  qs::qsave(save_data_grid_eez, here("data/model_features/deg_1_x_1/eez/int/eez_intersection.qs")) # save as we go
  
  data_grid_eez_2 <- data_grid_eez %>% 
      mutate(geometry_wkt = sf::st_make_valid(geometry_wkt)) %>%
    mutate(area_eez_overlap_m2 = sf::st_area(geometry_wkt)%>%
             units::drop_units())
  
  data_grid_eez_3 <- data_grid_eez_2 %>%
    left_join(data_grid %>% st_drop_geometry()) %>%
    st_drop_geometry() %>%
    mutate(fraction_eez_overlap = (area_eez_overlap_m2 / pixel_area_m2) %>%
             pmax(0) %>%
             pmin(1))%>%
    group_by(pixel_id) %>%
    slice_max(area_eez_overlap_m2, n = 1, with_ties = FALSE)%>%
    # In cases when there are ties, take largest EEZ
    ungroup() %>%
    dplyr::select(pixel_id,eez_sovereign,fraction_eez_overlap)
  
  qs::qsave(data_grid_eez_3, here("data/model_features/deg_1_x_1/eez/int/eez_intersection_no_highseas.qs"))

  
  eez_lookup <- read.csv(here("data/model_features/deg_1_x_1/eez/eez_lookup.csv"))
  
  
  data_grid_high_seas <- data_grid_eez_3 %>%
    full_join(data_grid %>% st_drop_geometry()) %>%
    mutate(eez_sovereign = ifelse(is.na(eez_sovereign), "High seas", eez_sovereign)) %>%
        left_join(eez_lookup, by = c("eez_sovereign")) %>%
    dplyr::distinct(pixel_id, eez_id)
  
  # %>%
    # dplyr::select(lon, lat, eez_id = MRGID_SOV1) %>%
    # rast(., type = "xyz")
  
  write.csv(data_grid_high_seas, here("data/model_features/deg_1_x_1/eez/eez.csv"), row.names = FALSE)
  


```


# World bank development regions

```{r}

eez_lookup_ids <- read.csv(here("data/model_features/deg_1_x_1/eez/eez_lookup.csv"))


eez_region_features <- eez_lookup_ids %>%
    mutate(eez_region_world_bank_7 = countrycode::countrycode(ISO_SOV1,"iso3c","region"))%>%
    distinct(ISO_SOV1, eez_region_world_bank_7, MRGID_SOV1)  %>% 
  mutate(eez_region_world_bank_7 = ifelse(ISO_SOV1 == "ESH", "Middle East & North Africa", eez_region_world_bank_7))

write.csv(eez_region_features, here("data/model_features/world_bank_regions.csv"), row.names = FALSE)

```


# Determine FAO region of each cell 
https://data.apps.fao.org/map/catalog/srv/eng/catalog.search#/metadata/ac02a460-da52-11dc-9d70-0017f293bd28d

```{r}
## see if we can fill the missing fao_id pixels using different method 

fao <- st_read(file.path(rdsi_raw_dir, "fao/FAO_AREAS_CWP")) %>%
  filter(F_LEVEL == "MAJOR") %>%
  mutate(F_AREA = as.numeric(F_AREA))

# Print summary of FAO areas
cat("\nFAO areas summary:\n")
print(paste("Number of FAO areas:", nrow(fao)))
print(paste("CRS:", st_crs(fao)$proj4string))

# Check if CRS is different and transform if needed
if (st_crs(fao) != st_crs(data_grid)) {
  cat("\nTransforming FAO CRS to match data grid...\n")
  fao <- st_transform(fao, st_crs(data_grid))
}

# Save FAO major IDs
fao_major_ids <- fao %>% 
  st_drop_geometry() %>%
  distinct(fao_id = F_AREA, NAME_EN, OCEAN) %>%
  clean_names()

write.csv(fao_major_ids, here("data/model_features/deg_1_x_1/fao/fao_major_ids.csv"), row.names = FALSE)

# Convert grid to raster
grid <- data_grid %>%
  st_drop_geometry() %>%
  dplyr::select(lon, lat, pixel_id) %>% 
  rast(., type = "xyz") 

# Print grid raster info
cat("\nGrid raster info:\n")
print(grid)

# Rasterize FAO areas
cat("\nRasterizing FAO areas...\n")
fao_rast <- rast(fasterize::fasterize(fao, raster(grid), field = "F_AREA"))

# Print FAO raster info
cat("\nFAO raster info:\n")
print(fao_rast)

# Count NA cells in FAO raster
na_count <- global(is.na(fao_rast), "sum")
cat("\nNumber of NA cells in FAO raster:", na_count$sum, "\n") # 20265

176*360 - 20265 # 43095; ok, this value SHOULD BE 46229 (the number of ocean cells). However, it is lower than that. So we are missing ~3000 cells that should have FAO fishing area information. 

writeRaster(fao_rast, here("data/model_features/deg_1_x_1/fao/fao_id_rast.tif"), overwrite = TRUE)

# Create lookup table
fao_lookup <- fao_rast %>%
  as.data.frame(xy = TRUE, na.rm = FALSE)

cat("\nFAO lookup table dimensions:", dim(fao_lookup)[1], "rows,", dim(fao_lookup)[2], "columns\n")

# Join with data_grid to get pixel_id and fao_id pairs
fao_lookup_fin <- fao_lookup %>%
  dplyr::select(lon = x, lat = y, fao_id = layer) %>%
  full_join(data_grid %>% st_drop_geometry(), by = c("lon", "lat")) %>% 
  filter(!is.na(pixel_id)) %>%
  dplyr::select(pixel_id, fao_id)

write.csv(fao_lookup_fin, here("data/model_features/deg_1_x_1/fao/fao.csv"), row.names = FALSE)

# 
# test <- isl_full_data %>% 
#   filter(is.na(fao_id)) %>%
#   filter(gear == "Dredges",
#          length_category == "12-24m",
#          year == 2015) # why are there 3157 pixels without FAO ids? 
# 
# ## ok which EEZs are these? 
# missing_eez <- unique(test$eez_id)
# 
# eez_lookup_ids <- read.csv(here("data/model_features/deg_1_x_1/eez/eez_lookup.csv")) %>%
#   filter(MRGID_SOV1 %in% missing_eez)
# unique(eez_lookup_ids$ISO_SOV1) # [1] "USA"       "IRN"       "TKM"       "AZE"       "KAZ"       "RUS"       "Land"      "High seas"
# # ok so we don't care about Land. But the rest are worrisome.

# Analyze missing FAO IDs
missing_fao <- fao_lookup_fin %>%
  filter(is.na(fao_id))

cat("\nNumber of pixels with missing FAO ID:", nrow(missing_fao), "\n")

# Get the coordinates of missing pixels
missing_pixels <- data_grid %>%
  filter(pixel_id %in% missing_fao$pixel_id)

# Save missing pixels for visualization
st_write(missing_pixels, here("data/model_features/deg_1_x_1/fao/missing_fao_pixels.shp"), delete_layer = TRUE)

# Create a simple plot of missing pixels
missing_coords <- missing_pixels %>%
  st_centroid() %>%
  st_coordinates() %>%
  as.data.frame()

# Save coordinates for plotting
write.csv(missing_coords, here("data/model_features/deg_1_x_1/fao/missing_fao_coords.csv"), row.names = FALSE)

## we need to fill in any NA fao_id's with the nearest non-NA pixel_id's fao_id
library(zoo)
fao_lookup_fin <- fao_lookup_fin %>%
  arrange(pixel_id)

# Fill missing fao_id using nearest non-NA values. we will just use filling up or down for this, since we already know the pixel_ids are in order from where they are located. we assume that if the pixel id is close to it, then it will be in the same FAO id. For now. I will try to run the nearest neighbor approach overnight. 
fao_lookup_fin <- fao_lookup_fin %>%
  mutate(fao_id = na.locf(fao_id, na.rm = FALSE, fromLast = FALSE)) %>%  # Forward fill
  mutate(fao_id = na.locf(fao_id, na.rm = FALSE, fromLast = TRUE))       # Backward fill

# Check if there are still any NAs left
sum(is.na(fao_lookup_fin$fao_id)) 

write.csv(fao_lookup_fin, here("data/model_features/deg_1_x_1/fao/fao_fixed.csv"), row.names = FALSE)

# # Attempt to fix missing FAO IDs using nearest neighbor approach
# cat("\nAttempting to fix missing FAO IDs using nearest neighbor approach...\n")
# 
# # Create a spatial points object for missing pixels
# missing_points <- missing_pixels %>%
#   st_centroid()
# 
# # Find the nearest FAO area for each missing point
# if (nrow(missing_points) > 0) {
#   # Create a function to find nearest FAO area
#   find_nearest_fao <- function(point, fao_areas) {
#     distances <- st_distance(point, fao_areas)
#     nearest_idx <- which.min(distances)
#     return(fao_areas$F_AREA[nearest_idx])
#   }
#     # Apply the function to each missing point
#   cat("Finding nearest FAO area for each missing point...\n")
#   nearest_fao_ids <- sapply(1:nrow(missing_points), function(i) {
#     if (i %% 100 == 0) cat("Processing point", i, "of", nrow(missing_points), "\n")
#     find_nearest_fao(missing_points[i,], fao)
#   })
#   
#     # Create a fixed lookup table
#   fixed_lookup <- fao_lookup_fin %>%
#     mutate(fao_id = ifelse(is.na(fao_id) & pixel_id %in% missing_fao$pixel_id,
#                           nearest_fao_ids[match(pixel_id, missing_fao$pixel_id)],
#                           fao_id))
#   
#   # Save the fixed lookup table
#   write.csv(fixed_lookup, here("data/model_features/deg_1_x_1/fao/fao_fixed.csv"), row.names = FALSE)
#   
#   # Check if all pixels now have an FAO ID
#   still_missing <- fixed_lookup %>%
#     filter(is.na(fao_id))
#   
#   cat("\nAfter fixing, number of pixels still missing FAO ID:", nrow(still_missing), "\n")
# } else {
#   cat("No missing points to fix.\n")
# }

```


# Determine if cell is mesopelagic zone or not 

Use Marine Region's [Mesopelagic ecoregions of the world's oceans](https://www.sciencedirect.com/science/article/pii/S0967063717301437?via%3Dihub#ack0005)


```{r}

meso <- st_read(glue("{data_directory}/marine_regions/mesopelagiczones/")) %>%
  clean_names() %>%
  rename(meso_id = provid)

meso_ids <- meso %>%
  st_drop_geometry() %>%
  dplyr::select(meso_id, meso_region = provname)


 grid <- data_grid %>%
  st_drop_geometry() %>%
  dplyr::select(lon, lat, pixel_id) %>% 
  rast(., type = "xyz") 
 
meso_rast <- rast(fasterize::fasterize(meso, raster(grid), field = "meso_id"))


writeRaster(meso_rast, here("data/model_features/deg_1_x_1/mesopelagiczones/meso_id_rast.tif"), overwrite = TRUE)

# Count NA cells in FAO raster
na_count <- global(is.na(meso_rast), "sum")
cat("\nNumber of NA cells in FAO raster:", na_count$sum, "\n") # 24834; this is too many!! 

# test <- meso_rast %>%
#   mutate(layer = ifelse(is.na(layer), 9999999, layer))

meso_lookup <- meso_rast %>%
  as.data.frame(xy = TRUE)


meso_lookup_id <- meso_lookup %>%
  dplyr::select(lon = x, lat = y, meso_id = layer) %>%
  full_join(data_grid) %>% 
    filter(!is.na(pixel_id)) %>%
  dplyr::select(pixel_id, meso_id)

cat("\nFAO lookup table dimensions:", dim(meso_lookup_id)[1], "rows,", dim(meso_lookup_id)[2], "columns\n") # ok good but many of these are NA... we need to fill these in


test <- meso_lookup_id %>%
  group_by(pixel_id) %>%
  summarise(n_distinct(meso_id)) # ok cool, seems to have worked?! 
  

write.csv(meso_lookup_id, here("data/model_features/deg_1_x_1/mesopelagiczones/mesopelagiczones.csv"), row.names = FALSE)

## we need to fill in any NA meso_id's with the nearest non-NA pixel_id's fao_id

meso_lookup_id <- meso_lookup_id %>%
  arrange(pixel_id)

# Fill missing fao_id using nearest non-NA values. we will just use filling up or down for this, since we already know the pixel_ids are in order from where they are located. we assume that if the pixel id is close to it, then it will be in the same FAO id. For now. I will try to run the nearest neighbor approach overnight. 
library(zoo)
meso_lookup_id <- meso_lookup_id %>%
  mutate(meso_id = na.locf(meso_id, na.rm = FALSE, fromLast = FALSE)) %>%  # Forward fill
  mutate(meso_id = na.locf(meso_id, na.rm = FALSE, fromLast = TRUE))       # Backward fill

# Check if there are still any NAs left
sum(is.na(meso_lookup_id$meso_id)) 

write.csv(meso_lookup_id, here("data/model_features/deg_1_x_1/mesopelagiczones/mesopelagiczones_fixed.csv"), row.names = FALSE)

```

Look at ocean data from Gavin's paper. There are some cells with missing ocean values. We will fill these in like we have with meso regions and fao ids 

```{r}
ocean_df <- read.csv(here("data/model_features/deg_1_x_1/oceans.csv"))

# ocean_df_ids <- ocean_df %>%
#     mutate(ocean = ifelse(ocean == "", "MISSING", ocean)) %>%
#   mutate(ocean_id = as.numeric(as.factor(ocean))) %>%
#   distinct(ocean, ocean_id)
# 
# ocean_rast <- ocean_df %>%
#   mutate(ocean = ifelse(ocean == "", "MISSING", ocean)) %>%
#   mutate(ocean = as.numeric(as.factor(ocean))) %>%
#   left_join(data_grid) %>%
#   dplyr::select(lon, lat, ocean) %>%
#   rast(., type = "xyz")


ocean_df_fin <- ocean_df %>%
  arrange(pixel_id) %>%
  mutate(ocean = ifelse(ocean == "", NA, ocean)) %>%
  mutate(ocean = na.locf(ocean, na.rm = FALSE, fromLast = FALSE)) %>%  # Forward fill
  mutate(ocean = na.locf(ocean, na.rm = FALSE, fromLast = TRUE))       # Backward fill

# Check if there are still any NAs left
sum(is.na(meso_lookup_id$provid)) 

write.csv(ocean_df_fin, here("data/model_features/deg_1_x_1/oceans_fixed.csv"), row.names = FALSE)


# ocean_df_ids <- ocean_df_fin %>%
#     mutate(ocean = ifelse(ocean == "", "MISSING", ocean)) %>%
#   mutate(ocean_id = as.numeric(as.factor(ocean))) %>%
#   distinct(ocean, ocean_id)
# 
# ocean_rast <- ocean_df_fin %>%
#   mutate(ocean = ifelse(ocean == "", "MISSING", ocean)) %>%
#   mutate(ocean = as.numeric(as.factor(ocean))) %>%
#   left_join(data_grid) %>%
#   dplyr::select(lon, lat, ocean) %>%
#   rast(., type = "xyz")
# 
# plot(ocean_rast) # looks good! 

```


Let's download historical climate data from ISIMIP for surface wind, chl, and sst. We will need to make sure the units are the same and calculate standard deviations as above. 

Chlorophyll: https://data.isimip.org/datasets/49dc048b-29e5-4cde-a89a-2fe448d86476/

SST: https://data.isimip.org/datasets/9e8a4b3f-5a56-4f4c-9677-1737a9f952d7/

Surface wind: https://data.isimip.org/datasets/dbcf73ba-878d-41d4-be7d-e28ce13121bc/ 


SST first: 

 - Read in data
 - Resample to match our grid 
 - bias correct with mean for 2014 (matching year)
 - calculate yearly average and save

```{r}
## lets prep SST data. They provide monthly at 60 arc mins (1 degree). We want yearly average and sd. Will need to reproject to our projection likely. 

sst_hist_raw <- rast(file.path(rdsi_raw_dir, "ISIMIP/sst/gfdl-esm4_r1i1p1f1_historical_tos_60arcmin_global_monthly_1850_2014.nc"))
# ok 2219 = 1850, 2383 = 2014. 


## BIAS CORRECTION
sst_obs_2014 <- read.csv(here("data/model_features/deg_1_x_1/errdap_sst.csv")) %>%
  filter(year == 2014) %>%
  left_join(data_grid %>% st_drop_geometry()) %>%
  dplyr::select(lon, lat, sst_c_mean) %>%
  rast(., type = "xyz")

sst_hist_2014 <- sst_hist_raw[[1980-11:1980]] %>%
  resample(., sst_obs_2014, method = "bilinear")

mean_obs_sst <- mean(sst_obs_2014$sst_c_mean, na.rm = TRUE)
mean_isimip_sst <- mean(sst_hist_2014, na.rm = TRUE)

bias_correction <- mean_obs_sst - mean_isimip_sst

sst_hist_resampled <- resample(sst_hist_raw, sst_obs_2014, method = "bilinear") # this will take awhile probably? I wonder if i should do the bias correction AFTER I calculate the average sst annually? The chl and wind datasets are a lot bigger... and wind is daily

sst_hist_corrected <- sst_hist_resampled + bias_correction

# writeCDF(sst_hist_corrected, file.path(rdsi_raw_dir, "ISIMIP/sst/corrected_monthly_1850_2014.nc"))

# test <- rast(file.path(rdsi_raw_dir, "ISIMIP/sst/corrected_monthly_1850_2014.nc"))

# plot(mean(sst_hist_corrected[[1980-11:1980]], na.rm = TRUE))
# plot(sst_obs_2014) # PERFECT

# Define time range
start_year <- 1850
end_year <- 2014
desired_start <- 1950

# Compute indices
time_steps_per_year <- 12  # Monthly data
start_index <- (desired_start - start_year) * time_steps_per_year + 1
end_index <- (end_year - start_year + 1) * time_steps_per_year

 
# Subset raster
sst_1950_2014_rast <- sst_hist_corrected[[start_index:end_index]]

sst_1950_2014_monthly <- sst_1950_2014_rast %>%
  as.data.frame(., xy=TRUE) %>% 
    pivot_longer(cols = starts_with("tos_"), names_to = "time_index", values_to = "sst") %>%
    mutate(
    time_index = as.numeric(gsub("tos_", "", time_index)),  # Convert layer number to numeric
    year = 1850 + (time_index - 1) %/% 12,  # Compute year
    month = (time_index - 1) %% 12 + 1  # Compute month
  ) %>%
  dplyr::select(lon = x, lat = y, year, month, sst)



qs::qsave(sst_1950_2014_monthly, file.path(rdsi_raw_dir, "ISIMIP/sst/sst_corrected_1950_2014_monthly.qs"))

## Now we need to calcualte the yearly mean and sd per pixel 
sst_1950_2014_yearly <- sst_1950_2014_monthly %>%
  left_join(data_grid %>% st_drop_geometry()) %>%
  group_by(pixel_id, year) %>%
  summarise(sst_c_mean = mean(sst, na.rm = TRUE),
            sst_c_sd = sd(sst, na.rm = TRUE)) %>% 
  ungroup()

qs::qsave(sst_1950_2014_yearly, here("data/int/prediction_historical_data/isimip_sst/sst_yearly_1950_2014.qs")) # NICE! Only 32 MB


```

CHL now: 

 - Read in data
 - Resample to match our grid 
 - bias correct with mean for 2014 (matching year)
 - calculate yearly average and save
 
 - ERRDAP data is surface chlorophyll-a (mg/m3). The ISIMIP data is chlorophyll-a

```{r}
## lets prep chl data. They provide monthly at 60 arc mins (1 degree). We want yearly average and sd. Will need to reproject to our projection likely. 

## why is there so much more data than SST???? Should only be 1980 layers... Is this chlorophyll at different depths?

chl_obs_2014 <- read.csv(here("data/model_features/deg_1_x_1/errdap_chl.csv")) %>%
  filter(year == 2014) %>%
  left_join(data_grid %>% st_drop_geometry()) %>%
  dplyr::select(lon, lat, chl_mg_per_m3_mean) %>%
  rast(., type = "xyz")

chl_hist_raw <- rast(file.path(rdsi_raw_dir, "ISIMIP/chl/gfdl-esm4_r1i1p1f1_historical_chl_60arcmin_global_monthly_1850_2014.nc"))
# ok 2219 = 1850, 2383 = 2014. 
# what units? kg/m3. Converted to mg per m3 below
## what is the surface level? Looks like there are different depth levels. I see 2.5, 10, 20, etc; 35 of them? These represent different depths. We want the top depth? 

unique(names(chl_hist_raw))

# test_depths <- grep("_2$", names(chl_hist_raw), value = TRUE)  #I think this is the highest depth (i.e., surface depth?); need to check against the observation data i used
# 35*1980 # 35 depths x 1980 layers. What do the layers represent? Layers are each time step. 1 = 1850 jan, 2 = 1850 feb
# 
# test <- chl_hist_raw[[test_depths]]


surface_chl <- grep("chl_lev=2.5", names(chl_hist_raw), value = TRUE)  #I think this is the highest depth (i.e., surface depth?); need to check against the observation data i used
chl_hist_surface <- chl_hist_raw[[surface_chl]]*1000000
 
chl_hist_surface_resample <- resample(chl_hist_surface, chl_obs_2014, method = "bilinear")

test <- global(chl_hist_surface_resample, "max", na.rm = TRUE)
test <- global(chl_hist_surface_resample, "mean", na.rm = TRUE) # ok, the differences don't look that crazy actually.
mean(test$mean)
global(chl_obs_2014, "mean", na.rm = TRUE)

writeRaster(chl_hist_surface_resample, file.path(rdsi_raw_dir, "ISIMIP/chl/", "resample_surface_chl_1850_2014_mg_m3.tif"), overwrite=TRUE)

## BIAS CORRECTION
chl_hist_2014 <- chl_hist_surface_resample[[1969:1980]]  # grab only 2014 

mean_obs_chl <- mean(chl_obs_2014, na.rm = TRUE)
mean_isimip_chl <- mean(chl_hist_2014, na.rm = TRUE)

bias_correction <- mean_obs_chl - mean_isimip_chl

chl_hist_corrected <- chl_hist_surface_resample + bias_correction

# writeCDF(chl_hist_corrected, file.path(rdsi_raw_dir, "ISIMIP/chl/chl_surface_corrected_monthly_1850_2014.nc"))

# plot(mean(chl_hist_corrected[[1969:1980]], na.rm = TRUE))
# plot(chl_obs_2014) # PERFECT

# Define time range
start_year <- 1850
end_year <- 2014
desired_start <- 1950

# Compute indices
time_steps_per_year <- 12  # Monthly data
start_index <- (desired_start - start_year) * time_steps_per_year + 1
end_index <- (end_year - start_year + 1) * time_steps_per_year

 
# Subset raster
chl_1950_2014_rast <- chl_hist_corrected[[start_index:end_index]]

chl_1950_2014_monthly <- chl_1950_2014_rast %>%
  as.data.frame(., xy=TRUE) %>% 
    pivot_longer(cols = starts_with("chl_"), names_to = "time_index", values_to = "chl") %>%
    mutate(
    time_index = as.numeric(strex::str_after_last(time_index, "_")),  # Convert layer number to numeric
    year = 1850 + (time_index - 1) %/% 12,  # Compute year
    month = (time_index - 1) %% 12 + 1  # Compute month
  ) %>%
  left_join(data_grid %>% st_drop_geometry, by = c("x" = "lon", "y" = "lat")) %>%
  dplyr::select(pixel_id, lon = x, lat = y, year, month, chl) 

# test <- chl_1950_2014_monthly %>%
#   dplyr::select(lon, lat, chl) %>%
#   rast(., type = "xyz")
# plot(test)


qs::qsave(chl_1950_2014_monthly, file.path(rdsi_raw_dir, "ISIMIP/chl/chl_corrected_1950_2014_monthly.qs"))

## Now we need to calcualte the yearly mean and sd per pixel 
chl_1950_2014_yearly <- chl_1950_2014_monthly %>%
  group_by(pixel_id, year) %>%
  summarise(chl_mg_per_m3_mean = mean(chl, na.rm = TRUE),
            chl_mg_per_m3_sd = sd(chl, na.rm = TRUE)) %>% 
  ungroup()

qs::qsave(chl_1950_2014_yearly, here("data/int/prediction_historical_data/chl_yearly_1950_2014.qs")) # NICE! Only 32 MB


```

Now prep wind data 

```{r}
## lets prep chl data. They provide monthly at 60 arc mins (1 degree). We want yearly average and sd. Will need to reproject to our projection likely. 

## why is there so much more data than SST???? Should only be 1980 layers... Is this chlorophyll at different depths?

wind_obs_2014 <- read.csv(here("data/model_features/deg_1_x_1/remss_wind.csv")) %>%
  filter(year == 2014) %>%
  left_join(data_grid %>% st_drop_geometry()) %>%
  dplyr::select(lon, lat, wind_speed_ms_mean) %>%
  rast(., type = "xyz")

##  get old 0.5 deg data 
data_grid_0.5 <- read.csv(here("data/model_features/global_grid.csv"))
wind_obs_2014_0.5 <- read.csv(here("data/model_features/remss_wind/wind_2013_2017.csv")) %>%
  filter(year == 2014) %>%
  left_join(data_grid_0.5 %>% st_drop_geometry()) %>%
  dplyr::select(lon, lat, wind_speed_ms_mean) %>%
  rast(., type = "xyz")


wind_hist_raw <- rast(list.files(file.path(rdsi_raw_dir, "ISIMIP/wind/raw"), full.names = TRUE)) # shit.. its 0.5 by 0.5 degree... need to aggregate to 1x1 
## resample to 0.5 raster
# wind_hist_resample <- resample(wind_hist_raw, wind_obs_2014_0.5, method = "bilinear")
#   
# ## mask out any non ocean cells 
# wind_hist_ocean <- mask(wind_hist_resample, wind_obs_2014_0.5)
# 
# ## save 
# writeRaster(wind_hist_ocean, file.path(rdsi_raw_dir, "ISIMIP/wind/sfc_wind_ocean_resample_0.5.tif"))

## resample so rasters match
wind_obs_resample <- resample(wind_obs_2014_0.5, wind_hist_raw, method = "bilinear")

raw_files <- list.files(file.path(rdsi_raw_dir, "ISIMIP/wind/raw"), full.names = TRUE)

test <- rast("/home/ubuntu/data_storage/raw_data/ISIMIP/wind/gfdl-esm4_r1i1p1f1_w5e5_historical_sfcwind_global_daily_1941_1950.nc_ocean.tif")
plot(test[[1]])

for(file in raw_files[3]){

#  file <- raw_files[[1]]
  wind_hist_raw_i <- rast(file)
  
  file_name <- basename(file)
  
## mask out any non ocean cells
wind_hist_ocean <- mask(wind_hist_raw_i, wind_obs_resample)

## save
writeRaster(wind_hist_ocean, glue(file.path(rdsi_raw_dir, "ISIMIP/wind/ocean/{file_name}_ocean.tif")))

}

## now we need to compare and bias adjust; we can do this bias adjustment with the 0.5 data since we have 0.5 data from 1993-2014 that matches. 
 # read in 1993-2014 observational data; resample this to match model data; calculate annual means for 0.5 model data; 
wind_obs_1993_2014 <- read.csv(here("data/model_features/remss_wind/wind_2013_2017.csv")) %>%
  #filter(year < 2015) %>%
  rbind(., read.csv(here("data/model_features/remss_wind/wind_2008_2012.csv"))) %>%
    rbind(., read.csv(here("data/model_features/remss_wind/wind_2003_2007.csv"))) %>%
    rbind(., read.csv(here("data/model_features/remss_wind/wind_1998_2002.csv"))) %>%
      rbind(., read.csv(here("data/model_features/remss_wind/wind_1993_1997.csv"))) %>% 
  left_join(data_grid_0.5 %>% st_drop_geometry())

wind_obs_stack <- c()

for(yr in c(1993:2014)){
  
  # yr = 1993
  df_year <- wind_obs_1993_2014 %>%
    filter(year == yr) %>%
    dplyr::select(lon, lat, wind_speed_ms_mean) %>%
    rast(., type = "xyz")
  
  df_year_resample <- resample(df_year, wind_hist_raw, method = "bilinear")
  
  names(df_year_resample) <- glue("{yr}")
  
  wind_obs_stack <- c(wind_obs_stack, df_year_resample)
  
}

wind_obs_stack <- rast(wind_obs_stack)

writeRaster(wind_obs_stack, file.path(rdsi_raw_dir, "ISIMIP/wind/obs_wind_stack_1993_2014.tif"))

hist_files <- list.files(file.path(rdsi_raw_dir, "ISIMIP/wind/ocean"), pattern = "_ocean", full.names = TRUE)
for(file in hist_files){
 # file <- hist_files[1]
  ocean_hist_wind <- rast(file)
 
# Use regular expression to extract the start and end years
years <- sub(".*_([0-9]{4})_([0-9]{4}).*.tif", "\\1-\\2", file)

# Split the years into individual components
year_split <- as.integer(strsplit(years, "-")[[1]])

# Extract min and max years
min_year <- min(year_split)
max_year <- max(year_split)  
days_per_year <- 365
  
yearly_avg_rasters <- c()
   
  # Loop through each year and calculate the yearly average
for (i in seq_along(min_year:max_year)) {
  # i = 1
  # Extract the layers corresponding to the current year
  start_layer <- (i - 1) * days_per_year + 1
  end_layer <- i * days_per_year
  year_layers <- ocean_hist_wind[[start_layer:end_layer]]
  
  # Calculate the yearly average for that year
  yearly_avg_rasters[[i]] <- mean(year_layers, na.rm = TRUE)
}

# Stack the yearly averages into a single raster
yearly_avg_raster_stack <- rast(yearly_avg_rasters)

# Name the layers of the stacked raster by the year
names(yearly_avg_raster_stack) <- paste0(min_year:max_year)

# Save the raster stack to a file (optional)
writeRaster(yearly_avg_raster_stack, file.path(rdsi_raw_dir, glue("ISIMIP/wind/yearly_wind_averages_{min_year}_{max_year}_0.5.tif")), overwrite=TRUE)

}

## now we need to read these in as a stack, and subtract from the observational data, and take a mean of those rasters to get our bias adjustment

wind_hist_stack_1993_2014 <- rast(list.files(file.path(rdsi_raw_dir, "ISIMIP/wind/"), full.names = TRUE, pattern = "yearly_wind_averages"))

bias_yearly <- wind_obs_stack - wind_hist_stack_1993_2014[[3:24]]

bias_mean <- mean(bias_yearly, na.rm = TRUE)

writeRaster(bias_mean, file.path(rdsi_raw_dir, "ISIMIP/wind/mean_bias_rast.tif"))

bias_mean <- rast(file.path(rdsi_raw_dir, "ISIMIP/wind/mean_bias_rast.tif"))

# then bias adjust daily ocean 0.5 data, aggregate daily bias adjust to 1 degree; calculate yearly averages, resample yearly averages, and extract as data frames to save 

hist_files <- list.files(file.path(rdsi_raw_dir, "ISIMIP/wind/ocean"), pattern = "_ocean", full.names = TRUE)

for(file in hist_files){
 #  file = hist_files[1]
  hist_rast <- rast(file) + bias_mean
  
  ocean_hist_wind_bias_agg <- aggregate(hist_rast, fact = 2, fun = "mean")
  
  filename <- strex::str_before_last(basename(file), "\\.tif")

  writeRaster(ocean_hist_wind_bias_agg, file.path(rdsi_raw_dir, glue("ISIMIP/wind/bias_1/{filename}_bias_1.tif")))
}


## now calculate yearly averages and resample to our raster format save rasters for those years 

hist_files <- list.files(file.path(rdsi_raw_dir, "ISIMIP/wind/bias_1"), pattern = "_ocean_bias_1", full.names = TRUE)
for(file in hist_files){
 # file <- hist_files[1]
  ocean_hist_wind <- rast(file)
 
# Use regular expression to extract the start and end years
years <- sub(".*_([0-9]{4})_([0-9]{4}).*.tif", "\\1-\\2", file)

# Split the years into individual components
year_split <- as.integer(strsplit(years, "-")[[1]])

# Extract min and max years
min_year <- min(year_split)
max_year <- max(year_split)  
days_per_year <- 365
  
yearly_avg_rasters <- c()
yearly_sd_rasters <- c()
   
  # Loop through each year and calculate the yearly average
for (i in seq_along(min_year:max_year)) {
  # i = 1
  # Extract the layers corresponding to the current year
  start_layer <- (i - 1) * days_per_year + 1
  end_layer <- i * days_per_year
  year_layers <- ocean_hist_wind[[start_layer:end_layer]]
  
  # Calculate the yearly average for that year
  yearly_avg_rasters[[i]] <- mean(year_layers, na.rm = TRUE)
  yearly_sd_rasters[[i]] <- app(year_layers, fun = sd, na.rm = TRUE)
}

# Stack the yearly averages into a single raster
yearly_avg_raster_stack <- rast(yearly_avg_rasters)
yearly_sd_raster_stack <- rast(yearly_sd_rasters)

# resample to match our raster
yearly_avg_raster_stack_resamp <- resample(yearly_avg_raster_stack, wind_obs_2014, method = "bilinear")

yearly_sd_raster_stack_resamp <- resample(yearly_sd_raster_stack, wind_obs_2014, method = "bilinear")

# Name the layers of the stacked raster by the year
names(yearly_avg_raster_stack) <- paste0(min_year:max_year)
names(yearly_sd_raster_stack) <- paste0(min_year:max_year)

# Save the raster stack to a file (optional)
writeRaster(yearly_avg_raster_stack, file.path(rdsi_raw_dir, glue("ISIMIP/wind/yearly_wind_averages_{min_year}_{max_year}_bias_corrected_1.tif")), overwrite=TRUE)

writeRaster(yearly_sd_raster_stack, file.path(rdsi_raw_dir, glue("ISIMIP/wind/yearly_wind_sd_{min_year}_{max_year}_bias_corrected_1.tif")), overwrite=TRUE)

}
  

test <- rast(hist_files[7])

## now save as.data.frame() for each year

old_wind <- read.csv(here("data/model_features/deg_1_x_1/remss_wind.csv"))
hist_files_mean <- list.files(file.path(rdsi_raw_dir, "ISIMIP/wind/"), pattern = "bias_corrected_1", full.names = TRUE)[1:8]
hist_files_sd <- list.files(file.path(rdsi_raw_dir, "ISIMIP/wind/"), pattern = "bias_corrected_1", full.names = TRUE)[9:16]

  
  hist_rast_df_mean <- rast(hist_files_mean) %>%
    resample(., wind_obs_2014, method = "bilinear") %>%
    as.data.frame(., xy = TRUE) %>%
    pivot_longer(cols = 3:76,
                 names_to = c("year"),
                 values_to = c("wind_speed_ms_mean")) %>%
    filter(year >= 1950) %>%
    left_join(data_grid %>% st_drop_geometry(), by = c("x" = "lon", "y" = "lat")) %>%
        dplyr::select(pixel_id, year, wind_speed_ms_mean) %>%
    filter(!is.na(pixel_id))

  
  
  hist_rast_df_sd <- rast(hist_files_sd) %>%
    resample(., wind_obs_2014, method = "bilinear") %>%
    as.data.frame(., xy = TRUE) %>%
    pivot_longer(cols = 3:76,
                 names_to = c("year"),
                 values_to = c("wind_speed_ms_sd")) %>%
    filter(year >= 1950) %>%
    left_join(data_grid %>% st_drop_geometry(), by = c("x" = "lon", "y" = "lat")) %>%
    dplyr::select(pixel_id, year, wind_speed_ms_sd) %>%
    filter(!is.na(pixel_id))


  
  hist_rast_all <- hist_rast_df_mean %>%
    left_join(hist_rast_df_sd)
  
  qs::qsave(hist_rast_all, here("data/int/prediction_historical_data/wind_yearly_1950_2014.qs"))
  
```

Aggregate days at sea to fishing hours conversion factors from Rousseau et al. to 1x1 degree: https://data.imas.utas.edu.au/attachments/1241a51d-c8c2-4432-aa68-3d2bae142794/ConversionFishingHours.csv

```{r}

conversion_0.5 <- read.csv("https://data.imas.utas.edu.au/attachments/1241a51d-c8c2-4432-aa68-3d2bae142794/ConversionFishingHours.csv") %>%
  clean_names() %>%
    dplyr::select(lon, lat, mean, length_category)

data_grid_rast <- data_grid %>%
  st_drop_geometry() %>% 
  dplyr::select(lon, lat) %>%
  rast(., type = "xyz")
  

conversion_1_df <- data.frame(x = NA, y = NA, mean = NA, length_category = NA)

for(l in unique(conversion_0.5$length_category)){
  
  # l = "Over 50m"
  
  conversion_l <- conversion_0.5 %>%
    filter(length_category == l) %>%
    dplyr::select(-length_category) %>%
    rast(.) %>%
    aggregate(., fact = 2, fun = "mean") %>%
    resample(., data_grid_rast, method = "bilinear") %>%
    as.data.frame(., xy = TRUE) %>%
    mutate(length_category = l)
  
  conversion_1_df <- rbind(conversion_1_df, conversion_l)
  
}

conversion_1_df <- conversion_1_df %>%
  filter(!is.na(x))

qs::qsave(conversion_1_df, here("data/int/hours_to_days_conversion.qs"))

```

