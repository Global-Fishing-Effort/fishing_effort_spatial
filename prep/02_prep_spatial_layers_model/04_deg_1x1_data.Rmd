---
title: "Aggregate data layers to 1 by 1 degree where necessary"
author: "Gage Clawson (IMAS)"
date: '`r format(Sys.time(), "%m/%d/%Y")`'
output: 
  pdf_document: 
    number_sections: yes
    toc: true
    toc_depth: 4
editor_options: 
  chunk_output_type: console
---

# Summary 

Check 1x1 degree data downloaded from McDonald et al. 2024

```{r}
# Load all necessary packages
library(tidyverse)
library(sf)
library(stars)
library(rerddap)
library(rnaturalearth)
library(glue)
library(scico)
library(lubridate)
library(furrr)
library(here)
library(terra)
library(fasterize)
library(raster)
library(janitor)

source(here::here("prep/02_prep_spatial_layers_model/_functions_data_wrangling_erddap.R"))
source(here("R/dir.R"))

# Set the data directory. This specifies the folder in our drive where the data can be found. 

data_directory <- rdsi_raw_dir

theme_set(theme_minimal() +
            theme(axis.title.y = element_text(angle = 0,vjust=0.5),
                  strip.background = element_blank(),
                  strip.text.y = element_text(angle=0),
                  strip.text.y.right = element_text(angle=0),
                  strip.text.y.left = element_text(angle=0),
                  panel.grid = element_blank(),
                  panel.background = element_blank(),
                  panel.grid.minor = element_blank(),
                  panel.grid.major = element_blank()))

# Set mapping projection
map_projection <- "+proj=eqearth +datum=WGS84 +wktext"
# Create global land sf object for mapping
world_plotting <- ne_countries(scale = "small", returnclass = "sf")  %>%
  dplyr::select(geometry)

# Get high-res ocean data
ocean <- ne_download(scale = 50, type = 'ocean', category = 'physical',returnclass = "sf") %>%
  dplyr::select(geometry)

```

# Defining our global grid

```{r}
pixel_size <- 1

# Create polygon rectangle of globe
# This will serve as basis of grid
global_polygon_sf <-
  tibble(lon = c(-180,-180,180,180,-180),
       lat = c(-90,90,90,-90,-90))%>%
  as.matrix() %>%
  list(.) %>%
  st_polygon() %>%
  st_sfc(crs = "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs") %>%
  st_as_sf()

# Start with global ocean
starting_shape <- ocean
#starting_shape <- global_polygon_sf_small

data_grid <- data.table::fread(here::here("data/model_features/deg_1_x_1/global_grid.csv")) %>%
  st_as_sf(wkt = "geometry_wkt",
           crs = "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs")

```


```{r}

chl_df <- read.csv(here("data/model_features/deg_1_x_1/errdap_chl.csv"))
unique(chl_df$year) # only have 2016-2021

# test <- chl_df %>% filter(year == 2017) 
# 
# missing_pixels <- data_grid %>%
#   st_drop_geometry() %>%
#   filter(!(pixel_id %in% unique(test$pixel_id)))
# 
# ## look at iceland full_data file that we feed into the model
# isl_full_data <- qs::qread(file.path(rdsi_dir, "prep/test_isl_full_data.qs")) %>%
#   left_join(., data_grid)
# 
# test_isl <- isl_full_data %>%
#   filter(pixel_id %in% unique(missing_pixels$pixel_id)) %>%
#   filter(year == 2017) ## so i guess there are just cells that don't have chl data. We took the 2017 chl data straight from McDonald et al. 

sst_df <- read.csv(here("data/model_features/deg_1_x_1/errdap_sst.csv"))
unique(sst_df$year) # only have 2016-2021

test <- sst_df %>% filter(year == 2017) %>%
  filter(!is.na(sst_c_mean))

wind_df <- read.csv(here("data/model_features/deg_1_x_1/remss_wind.csv"))
unique(wind_df$year) # only have 2016-2021

test <- wind_df %>% filter(year == 2017) %>%
  filter(!is.na(wind_speed_ms_mean)) # yep all missing some info

rec_df <- read.csv(here("data/model_features/deg_1_x_1/gfw_reception_quality.csv")) # static layer

spatial_df <- read.csv(here("data/model_features/deg_1_x_1/gfw_static_spatial_measures.csv")) # static layers depth, distance to shore, distance to port. 

grid_df <- read.csv(here("data/model_features/deg_1_x_1/global_grid.csv")) # 1 by 1 grid

oceans_df <- read.csv(here("data/model_features/deg_1_x_1/oceans.csv")) 
unique(oceans_df$ocean) # i guess this is just what ocean it is? Is this the mesopelagic data? No 
#  [1] "Southern Ocean"                           ""                                         "Arctic Ocean"                            
#  [4] "South Pacific Ocean"                      "South Atlantic Ocean"                     "Indian Ocean"                            
#  [7] "South China and Easter Archipelagic Seas" "North Atlantic Ocean"                     "North Pacific Ocean"                     
# [10] "Mediterranean Region"                     "Baltic Sea"    

## some pixels with ocean == " "; but no NAs. So that is good I guess. 

# test <- isl_full_data %>% 
#   filter(year == 2017,
#          is.na(ocean)) # 0 good.

```


Need to grab 2015 data for non-static data (sst, wind, chl). 

```{r}
test_sst <- read.csv(file.path(data_directory, "erddap/ncdcOisst21Agg_LonPM180/ncdcOisst21Agg_LonPM180_2023-12-31.csv")) %>%
  dplyr::select(longitude, latitude, sst) %>%
  rast(., type = "xyz") # ok we downloaded it in 0.25 by 0.25 degrees


test_chl <- read.csv(file.path(data_directory, "erddap/erdMH1chlamday/erdMH1chlamday_2013-06-16.csv")) %>%
  dplyr::select(longitude, latitude, chlorophyll) %>%
  rast(., type = "xyz") # not sure of download resolution, but it is smaller than 1 by 1 

test_wind <- rast(file.path(data_directory, "remss/wind-ccmp/wind_2024-08-31.nc")) # downloaded it in like ~0.25 degrees

```

Prep 2015 SST, CHL, data and append to data in model features folder 

```{r}
## start with prepping 1 by 1 sst data for 2015 only. Then we will appead it onto the existing data downloaded from the paper
# Spatially aggregate SST and SST anomaly data
spatially_aggregate_errdap_data_wrapper(dataset_name = "ncdcOisst21Agg_LonPM180",
                                        spatial_aggregation = data_grid,
                                        years = 2014, # choose years you want to run
                                        run_parallel = TRUE)


# temporally aggregate SST and SST anomaly data

temporally_aggregate_errdap_data_wrapper(dataset_name = "ncdcOisst21Agg_LonPM180",
                                         temporal_aggregation = "year",
                                         run_parallel = TRUE,
                                         years_per_chunk = 5)

old_sst <- read.csv(here("data/model_features/deg_1_x_1/errdap_sst.csv"))

test <- old_sst %>% filter(year == 2016) # ok cool

new_sst <- read.csv("data/model_features/deg_1_x_1/ncdcOisst21Agg_LonPM180/errdap_2015_2015.csv")

rewrite_sst <- rbind(new_sst, old_sst)

write.csv(rewrite_sst, here("data/model_features/deg_1_x_1/errdap_sst.csv"), row.names = FALSE)


# Spatially aggregate chl data for 2015
spatially_aggregate_errdap_data_wrapper(dataset_name = "erdMH1chlamday",
                                        spatial_aggregation = data_grid,
                                        years = 2015, # choose years you want to run
                                        run_parallel = FALSE)

temporally_aggregate_errdap_data_wrapper(dataset_name = "erdMH1chlamday",
                                         temporal_aggregation = "year",
                                         run_parallel = TRUE,
                                         years_per_chunk = 5)

old_chl <- read.csv(here("data/model_features/deg_1_x_1/errdap_chl.csv"))

new_chl <- read.csv("data/model_features/deg_1_x_1/erdMH1chlamday/errdap_2015_2015.csv")

test <- old_chl %>% filter(year == 2016) # ok cool


rewrite_chl <- rbind(new_chl, old_chl)

write.csv(rewrite_chl, here("data/model_features/deg_1_x_1/errdap_chl.csv"), row.names = FALSE)


```

Prep 2015 Wind data and append to data in model features folder 

```{r}
## now do wind for 2015

# Process data; spatially aggregate
# Do it in parallel?
run_parallel <- TRUE


tmp_data_directory <- glue::glue("{data_directory}/remss/clean/spatially_aggregated_{pixel_size}_degree")
 if(!dir.exists(tmp_data_directory)) dir.create(tmp_data_directory)
 if(run_parallel) plan(multisession) else plan(sequential)

years = 2015

 pattern <- paste(years, collapse = "|")
 
list.files(glue("{data_directory}/remss/wind-ccmp"), pattern = pattern) %>%
   future_map_dfr(function(tmp_file_name){
         tryCatch({
     
     # tmp_file_name <- "wind_1993-03-07.nc"
     date_tmp <- tmp_file_name %>%
       # Extract date
       stringr::str_replace(glue::glue("wind_"),"") %>% 
       stringr::str_remove(".nc") %>%
       lubridate::date()
     
     processed_file_name <- glue::glue("{tmp_data_directory}/{date_tmp}.csv")
     
          message(glue::glue("Processing file: {tmp_file_name}"))
     
     # If already processed, don't do it again
     if(file.exists(processed_file_name)) {
       message(glue::glue("File already processed: {processed_file_name}"))
       return()
     }
     
 glue::glue("{data_directory}/remss/wind-ccmp/{tmp_file_name}") %>%
       stars::read_ncdf() %>%
       # Calculate wind speed from u and v components, using pythagorean theorum
       dplyr::mutate(wind_speed_m_s = sqrt(uwnd^2 + vwnd^2)) %>%
       # Get rid of these
       dplyr::select(-uwnd,-vwnd,-nobs) %>% 
       # Data are for every 6 hours, so take mean for entire day
       aggregate(FUN = mean, by = "1 day") %>% 
       # Convert stars to raster, so we can use exactextractr::exact_extract
       as("Raster") %>%
       # Need to rotate from 0-360 to -180-180, since original NC is provided in 0-360
       raster::rotate() %>%
       # Let's spatially aggregate by taking the mean value for each of our pixels
       exactextractr::exact_extract(data_grid,
                                    # From the help: "mean - the mean cell value, weighted by the fraction of each cell that is covered by the polygon"
                                    "mean",
                                    # Include pixel_id column so we can match on it later
                                    append_cols = "pixel_id",
                                    progress = FALSE) %>% 
       # Don't save data that are just NAs
       dplyr::filter(!dplyr::if_all(-c(pixel_id),is.na)) %>%
       dplyr::rename(wind_speed_ms_mean = mean) %>%
       dplyr::mutate(date = date_tmp) %>%
       data.table::fwrite(processed_file_name)
     return(NULL)

    }, error = function(e) {
      # Log the error and skip the file
      message(glue::glue("Error processing file {tmp_file_name}: {e$message}"))
      return(NULL)
    })
   }, .options = furrr_options(globals=c("data_directory","data_grid","tmp_data_directory"),
                               seed = 101),.progress=TRUE)


# temporally aggregate data 

run_parallel <- TRUE

tmp_data_directory <- glue::glue("{data_directory}/remss/clean/spatially_aggregated_{pixel_size}_degree")

if(run_parallel) plan(multisession) else plan(sequential)

# Create sequence of start years for each 5-year chunk
start_years <- seq(2015, 2015, by = 5)

# Process each 5-year chunk
for(start_year in start_years) {
  # start_year = 1993
  end_year <- min(start_year + 4, 2024)  # Ensure we don't go beyond 2024
  years_to_process <- start_year:end_year
  
  # Get files for this 5-year chunk
  chunk_files <- list.files(tmp_data_directory, pattern = paste0(years_to_process, collapse = "|"))
  
chunk_data <- chunk_files %>% 
  future_map(function(file_temp) {
    # Construct the full path
    file_path <- glue::glue("{tmp_data_directory}/{file_temp}")
    
    # Read the file
    data <- data.table::fread(file_path)
    
    # Check if the file has rows
    if (nrow(data) == 0) {
      return(NULL)  # Return NULL for files with no rows
    }
    
    # Return the data if it has rows
    data
  }, .options = furrr_options(globals = c("tmp_data_directory"),
                               seed = 101), 
     .progress = TRUE) %>%
  
  # Remove NULL entries (from files with no rows)
  purrr::compact() %>%

  # Combine the data into one table
  data.table::rbindlist() %>%

  # Transform and process
  collapse::ftransform(year = lubridate::year(date)) %>%
  dplyr::select(-date) %>%
  dplyr::rename_with(~ gsub('_mean', '', .x)) %>%
  collapse::fgroup_by(pixel_id, year) %>% {
    collapse::add_vars(
      collapse::add_stub(collapse::fmean(., keep.group_vars = TRUE), "_mean", pre = FALSE, cols = -c(1, 2)),
      collapse::add_stub(collapse::fsd(., keep.group_vars = FALSE), "_sd", pre = FALSE)
    )
  }
  
  data.table::fwrite(chunk_data, here::here(glue::glue("data/model_features/deg_1_x_1/remss_wind/wind_{start_year}_{end_year}.csv")))

  
  # Clear chunk data to free memory
  rm(chunk_data)
  gc()
}


old_wind <- read.csv(here("data/model_features/deg_1_x_1/remss_wind.csv"))

new_wind <- read.csv("data/model_features/deg_1_x_1/remss_wind/wind_2015_2019.csv") # this is actually only 2015

test <- old_wind %>% filter(year == 2016) # ok cool

rewrite_wind <- rbind(new_wind, old_wind)

write.csv(rewrite_wind, here("data/model_features/deg_1_x_1/remss_wind.csv"), row.names = FALSE)

```


Need to prep 1 by 1 EEZ, FAO, and mesopelagic data 

# Determine EEZ of each cell 

Use Marine Region's [Maritime Boundaries Geodatabase: Maritime Boundaries and Exclusive Economic Zones (200NM), version 12]

```{r}
global_grid <- read.csv(here("data/model_features/deg_1_x_1/global_grid.csv"))
eezs <- st_read(file.path(rdsi_raw_dir, "marine_regions/World_EEZ_v12_20231025"), layer = 'eez_v12') %>%
  filter(POL_TYPE != "Overlapping claim",
         ISO_SOV1 != "ATA")  
  
world_shp <- sf::st_as_sf(maps::map("world", plot = FALSE, fill = TRUE))

grid <- data_grid %>%
  st_drop_geometry() %>%
  dplyr::select(lon, lat, pixel_id) %>% 
  rast(., type = "xyz") 

eez_rast <- rast(fasterize::fasterize(eezs, raster(grid), field = "MRGID_SOV1"))
eez_rast

eez_rast[is.na(eez_rast)] <- 99999 # save a value high seas, not if sure necessary
eez_rast <- mask(eez_rast, vect(world_shp), inverse = TRUE) # mask out any land 
eez_rast[is.na(eez_rast)] <- 999999 # save a value for land, not sure if necessary


writeRaster(eez_rast, here("data/model_features/deg_1_x_1/eez/eez_id_rast_1.tif"), overwrite = TRUE)

## get x, y lookup table so we can join to the raw mmsi data when saving and have the EEZ id already saved in the csv 


eez_lookup <- eez_rast %>%
  as.data.frame(xy = TRUE)

eez_check <- eez_lookup %>%
  mutate(x_int = floor(x), y_int = floor(y)) %>%  # extract integer part of x and y
  group_by(x_int, y_int) %>%
  summarise(x_unique_decimals = n_distinct(x),
            y_unique_decimals = n_distinct(y)) %>%
  ungroup()

eez_lookup_fin <- eez_lookup %>%
  dplyr::select(lon = x, lat = y, eez_id = layer) %>%
  left_join(global_grid) %>% 
  dplyr::select(pixel_id, eez_id)
  

write.csv(eez_lookup_fin, here("data/model_features/deg_1_x_1/eez/eez.csv"), row.names = FALSE)

eez_lookup_ids <- eezs %>% 
  st_drop_geometry() %>%
  dplyr::distinct(ISO_SOV1, MRGID_SOV1) %>%
  add_row(ISO_SOV1 = "Land", MRGID_SOV1 = 999999) %>%
  add_row(ISO_SOV1 = "High seas", MRGID_SOV1 = 99999)

write.csv(eez_lookup_ids, here("data/model_features/deg_1_x_1/eez/eez_lookup.csv"), row.names = FALSE)

```

# World bank development regions

```{r}

eez_lookup_ids <- read.csv(here("data/model_features/deg_1_x_1/eez/eez_lookup.csv"))


eez_region_features <- eez_lookup_ids %>%
    mutate(eez_region_world_bank_7 = countrycode::countrycode(ISO_SOV1,"iso3c","region"))%>%
    distinct(ISO_SOV1, eez_region_world_bank_7, MRGID_SOV1)  %>% 
  mutate(eez_region_world_bank_7 = ifelse(ISO_SOV1 == "ESH", "Middle East & North Africa", eez_region_world_bank_7))

write.csv(eez_region_features, here("data/model_features/world_bank_regions.csv"), row.names = FALSE)

```


# Determine FAO region of each cell 
https://data.apps.fao.org/map/catalog/srv/eng/catalog.search#/metadata/ac02a460-da52-11dc-9d70-0017f293bd28d

```{r}
## see if we can fill the missing fao_id pixels using different method 

fao <- st_read(file.path(rdsi_raw_dir, "fao/FAO_AREAS_CWP")) %>%
  filter(F_LEVEL == "MAJOR") %>%
  mutate(F_AREA = as.numeric(F_AREA))

# Print summary of FAO areas
cat("\nFAO areas summary:\n")
print(paste("Number of FAO areas:", nrow(fao)))
print(paste("CRS:", st_crs(fao)$proj4string))

# Check if CRS is different and transform if needed
if (st_crs(fao) != st_crs(data_grid)) {
  cat("\nTransforming FAO CRS to match data grid...\n")
  fao <- st_transform(fao, st_crs(data_grid))
}

# Save FAO major IDs
fao_major_ids <- fao %>% 
  st_drop_geometry() %>%
  distinct(fao_id = F_AREA, NAME_EN, OCEAN) %>%
  clean_names()

write.csv(fao_major_ids, here("data/model_features/deg_1_x_1/fao/fao_major_ids.csv"), row.names = FALSE)

# Convert grid to raster
grid <- data_grid %>%
  st_drop_geometry() %>%
  dplyr::select(lon, lat, pixel_id) %>% 
  rast(., type = "xyz") 

# Print grid raster info
cat("\nGrid raster info:\n")
print(grid)

# Rasterize FAO areas
cat("\nRasterizing FAO areas...\n")
fao_rast <- rast(fasterize::fasterize(fao, raster(grid), field = "F_AREA"))

# Print FAO raster info
cat("\nFAO raster info:\n")
print(fao_rast)

# Count NA cells in FAO raster
na_count <- global(is.na(fao_rast), "sum")
cat("\nNumber of NA cells in FAO raster:", na_count$sum, "\n") # 20265

176*360 - 20265 # 43095; ok, this value SHOULD BE 46229 (the number of ocean cells). However, it is lower than that. So we are missing ~3000 cells that should have FAO fishing area information. 

writeRaster(fao_rast, here("data/model_features/deg_1_x_1/fao/fao_id_rast.tif"), overwrite = TRUE)

# Create lookup table
fao_lookup <- fao_rast %>%
  as.data.frame(xy = TRUE, na.rm = FALSE)

cat("\nFAO lookup table dimensions:", dim(fao_lookup)[1], "rows,", dim(fao_lookup)[2], "columns\n")

# Join with data_grid to get pixel_id and fao_id pairs
fao_lookup_fin <- fao_lookup %>%
  dplyr::select(lon = x, lat = y, fao_id = layer) %>%
  full_join(data_grid %>% st_drop_geometry(), by = c("lon", "lat")) %>% 
  filter(!is.na(pixel_id)) %>%
  dplyr::select(pixel_id, fao_id)

write.csv(fao_lookup_fin, here("data/model_features/deg_1_x_1/fao/fao.csv"), row.names = FALSE)

# 
# test <- isl_full_data %>% 
#   filter(is.na(fao_id)) %>%
#   filter(gear == "Dredges",
#          length_category == "12-24m",
#          year == 2015) # why are there 3157 pixels without FAO ids? 
# 
# ## ok which EEZs are these? 
# missing_eez <- unique(test$eez_id)
# 
# eez_lookup_ids <- read.csv(here("data/model_features/deg_1_x_1/eez/eez_lookup.csv")) %>%
#   filter(MRGID_SOV1 %in% missing_eez)
# unique(eez_lookup_ids$ISO_SOV1) # [1] "USA"       "IRN"       "TKM"       "AZE"       "KAZ"       "RUS"       "Land"      "High seas"
# # ok so we don't care about Land. But the rest are worrisome.

# Analyze missing FAO IDs
missing_fao <- fao_lookup_fin %>%
  filter(is.na(fao_id))

cat("\nNumber of pixels with missing FAO ID:", nrow(missing_fao), "\n")

# Get the coordinates of missing pixels
missing_pixels <- data_grid %>%
  filter(pixel_id %in% missing_fao$pixel_id)

# Save missing pixels for visualization
st_write(missing_pixels, here("data/model_features/deg_1_x_1/fao/missing_fao_pixels.shp"), delete_layer = TRUE)

# Create a simple plot of missing pixels
missing_coords <- missing_pixels %>%
  st_centroid() %>%
  st_coordinates() %>%
  as.data.frame()

# Save coordinates for plotting
write.csv(missing_coords, here("data/model_features/deg_1_x_1/fao/missing_fao_coords.csv"), row.names = FALSE)

## we need to fill in any NA fao_id's with the nearest non-NA pixel_id's fao_id
library(zoo)
fao_lookup_fin <- fao_lookup_fin %>%
  arrange(pixel_id)

# Fill missing fao_id using nearest non-NA values. we will just use filling up or down for this, since we already know the pixel_ids are in order from where they are located. we assume that if the pixel id is close to it, then it will be in the same FAO id. For now. I will try to run the nearest neighbor approach overnight. 
fao_lookup_fin <- fao_lookup_fin %>%
  mutate(fao_id = na.locf(fao_id, na.rm = FALSE, fromLast = FALSE)) %>%  # Forward fill
  mutate(fao_id = na.locf(fao_id, na.rm = FALSE, fromLast = TRUE))       # Backward fill

# Check if there are still any NAs left
sum(is.na(fao_lookup_fin$fao_id)) 

write.csv(fao_lookup_fin, here("data/model_features/deg_1_x_1/fao/fao_fixed.csv"), row.names = FALSE)

# # Attempt to fix missing FAO IDs using nearest neighbor approach
# cat("\nAttempting to fix missing FAO IDs using nearest neighbor approach...\n")
# 
# # Create a spatial points object for missing pixels
# missing_points <- missing_pixels %>%
#   st_centroid()
# 
# # Find the nearest FAO area for each missing point
# if (nrow(missing_points) > 0) {
#   # Create a function to find nearest FAO area
#   find_nearest_fao <- function(point, fao_areas) {
#     distances <- st_distance(point, fao_areas)
#     nearest_idx <- which.min(distances)
#     return(fao_areas$F_AREA[nearest_idx])
#   }
#     # Apply the function to each missing point
#   cat("Finding nearest FAO area for each missing point...\n")
#   nearest_fao_ids <- sapply(1:nrow(missing_points), function(i) {
#     if (i %% 100 == 0) cat("Processing point", i, "of", nrow(missing_points), "\n")
#     find_nearest_fao(missing_points[i,], fao)
#   })
#   
#     # Create a fixed lookup table
#   fixed_lookup <- fao_lookup_fin %>%
#     mutate(fao_id = ifelse(is.na(fao_id) & pixel_id %in% missing_fao$pixel_id,
#                           nearest_fao_ids[match(pixel_id, missing_fao$pixel_id)],
#                           fao_id))
#   
#   # Save the fixed lookup table
#   write.csv(fixed_lookup, here("data/model_features/deg_1_x_1/fao/fao_fixed.csv"), row.names = FALSE)
#   
#   # Check if all pixels now have an FAO ID
#   still_missing <- fixed_lookup %>%
#     filter(is.na(fao_id))
#   
#   cat("\nAfter fixing, number of pixels still missing FAO ID:", nrow(still_missing), "\n")
# } else {
#   cat("No missing points to fix.\n")
# }

```


# Determine if cell is mesopelagic zone or not 

Use Marine Region's [Mesopelagic ecoregions of the world's oceans](https://www.sciencedirect.com/science/article/pii/S0967063717301437?via%3Dihub#ack0005)


```{r}

meso <- st_read(glue("{data_directory}/marine_regions/mesopelagiczones/")) %>%
  clean_names() %>%
  rename(meso_id = provid)

meso_ids <- meso %>%
  st_drop_geometry() %>%
  dplyr::select(meso_id, meso_region = provname)


 grid <- data_grid %>%
  st_drop_geometry() %>%
  dplyr::select(lon, lat, pixel_id) %>% 
  rast(., type = "xyz") 
 
meso_rast <- rast(fasterize::fasterize(meso, raster(grid), field = "meso_id"))


writeRaster(meso_rast, here("data/model_features/deg_1_x_1/mesopelagiczones/meso_id_rast.tif"), overwrite = TRUE)

# Count NA cells in FAO raster
na_count <- global(is.na(meso_rast), "sum")
cat("\nNumber of NA cells in FAO raster:", na_count$sum, "\n") # 24834; this is too many!! 

# test <- meso_rast %>%
#   mutate(layer = ifelse(is.na(layer), 9999999, layer))

meso_lookup <- meso_rast %>%
  as.data.frame(xy = TRUE)


meso_lookup_id <- meso_lookup %>%
  dplyr::select(lon = x, lat = y, meso_id = layer) %>%
  full_join(data_grid) %>% 
    filter(!is.na(pixel_id)) %>%
  dplyr::select(pixel_id, meso_id)

cat("\nFAO lookup table dimensions:", dim(meso_lookup_id)[1], "rows,", dim(meso_lookup_id)[2], "columns\n") # ok good but many of these are NA... we need to fill these in


test <- meso_lookup_id %>%
  group_by(pixel_id) %>%
  summarise(n_distinct(meso_id)) # ok cool, seems to have worked?! 
  

write.csv(meso_lookup_id, here("data/model_features/deg_1_x_1/mesopelagiczones/mesopelagiczones.csv"), row.names = FALSE)

## we need to fill in any NA meso_id's with the nearest non-NA pixel_id's fao_id

meso_lookup_id <- meso_lookup_id %>%
  arrange(pixel_id)

# Fill missing fao_id using nearest non-NA values. we will just use filling up or down for this, since we already know the pixel_ids are in order from where they are located. we assume that if the pixel id is close to it, then it will be in the same FAO id. For now. I will try to run the nearest neighbor approach overnight. 
library(zoo)
meso_lookup_id <- meso_lookup_id %>%
  mutate(meso_id = na.locf(meso_id, na.rm = FALSE, fromLast = FALSE)) %>%  # Forward fill
  mutate(meso_id = na.locf(meso_id, na.rm = FALSE, fromLast = TRUE))       # Backward fill

# Check if there are still any NAs left
sum(is.na(meso_lookup_id$meso_id)) 

write.csv(meso_lookup_id, here("data/model_features/deg_1_x_1/mesopelagiczones/mesopelagiczones_fixed.csv"), row.names = FALSE)

```

Look at ocean data from Gavin's paper. There are some cells with missing ocean values. We will fill these in like we have with meso regions and fao ids 

```{r}
ocean_df <- read.csv(here("data/model_features/deg_1_x_1/oceans.csv"))

# ocean_df_ids <- ocean_df %>%
#     mutate(ocean = ifelse(ocean == "", "MISSING", ocean)) %>%
#   mutate(ocean_id = as.numeric(as.factor(ocean))) %>%
#   distinct(ocean, ocean_id)
# 
# ocean_rast <- ocean_df %>%
#   mutate(ocean = ifelse(ocean == "", "MISSING", ocean)) %>%
#   mutate(ocean = as.numeric(as.factor(ocean))) %>%
#   left_join(data_grid) %>%
#   dplyr::select(lon, lat, ocean) %>%
#   rast(., type = "xyz")


ocean_df_fin <- ocean_df %>%
  arrange(pixel_id) %>%
  mutate(ocean = ifelse(ocean == "", NA, ocean)) %>%
  mutate(ocean = na.locf(ocean, na.rm = FALSE, fromLast = FALSE)) %>%  # Forward fill
  mutate(ocean = na.locf(ocean, na.rm = FALSE, fromLast = TRUE))       # Backward fill

# Check if there are still any NAs left
sum(is.na(meso_lookup_id$provid)) 

write.csv(ocean_df_fin, here("data/model_features/deg_1_x_1/oceans_fixed.csv"), row.names = FALSE)


# ocean_df_ids <- ocean_df_fin %>%
#     mutate(ocean = ifelse(ocean == "", "MISSING", ocean)) %>%
#   mutate(ocean_id = as.numeric(as.factor(ocean))) %>%
#   distinct(ocean, ocean_id)
# 
# ocean_rast <- ocean_df_fin %>%
#   mutate(ocean = ifelse(ocean == "", "MISSING", ocean)) %>%
#   mutate(ocean = as.numeric(as.factor(ocean))) %>%
#   left_join(data_grid) %>%
#   dplyr::select(lon, lat, ocean) %>%
#   rast(., type = "xyz")
# 
# plot(ocean_rast) # looks good! 

```


