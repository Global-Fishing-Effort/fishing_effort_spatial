---
title: "FAO fishing effort project - Data wrangling - REMSS wind data"
author: "Gage Clawson (IMAS)"
date: '`r format(Sys.time(), "%m/%d/%Y")`'
output: 
  pdf_document: 
    number_sections: yes
    toc: true
    toc_depth: 4
editor_options: 
  chunk_output_type: console
---

# Summary 

Download wind data 

```{r echo = FALSE}
# This chunk sets up default settings for all chunks below
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE,fig.width = 7.5,fig.height = 5,dev = 'png',dpi=300)
```

```{r include=FALSE}
# Load all necessary packages
library(tidyverse)
library(sf)
library(stars)
library(glue)
library(lubridate)
library(collapse)
library(furrr)
library(rnaturalearth)
library(here)
library(data.table)

#source(here::here("r/data_wrangling_functions.R"))
source(here("R/dir.R"))

# Set the data directory. This specifies the folder in our drive where the data can be found. 

data_directory <- rdsi_raw_dir

# Set mapping projection
map_projection <- "+proj=eqearth +datum=WGS84 +wktext"

# Create global land sf object for mapping
world_plotting <- ne_countries(scale = "small", returnclass = "sf")  %>%
  dplyr::select(geometry)

pixel_size <- 0.5

# Read in data_grid, generated in data_wrangling.Rmd
data_grid <- data.table::fread(here("data/model_features/global_grid.csv"))%>%
  st_as_sf(wkt = "geometry_wkt",
           crs = "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs") %>%
  mutate(pixel_area_m2 = st_area(geometry_wkt)%>%
           units::drop_units())
```

# Download data

Data come from the [CCMP Wind Vector Analysis Product](https://www.remss.com/measurements/ccmp/). We use V3.1 for dates from 1993-2024. The raw data provide u- and v- vectors of wind speed. For each location, we combine these to create absolute windspeed using wind_speed_m_s = sqrt(uwnd^2 + vwnd^2).

Data are downloaded from this [http server](https://data.remss.com/ccmp/).

```{r eval = params$download_data}
v31_dates <- tibble(date = seq(date("2024-09-01"),date("2024-12-31"),by="1 day")) %>%
  mutate(file_path = glue("https://data.remss.com/ccmp/v03.1/Y{year(date)}/M{ifelse(nchar(month(date))==1,paste0(0,month(date)),month(date))}/CCMP_Wind_Analysis_{str_remove_all(date,'-')}_V03.1_L4.nc"))


# Download data
# Do it in parallel?
run_parallel <- TRUE

if(run_parallel) plan(multisession) else plan(sequential)

bind_rows(v31_dates) %>%
  mutate(downloaded = future_map2(date,file_path,function(date,file_path){
    # This wraps the download function and tries the download several times
    # This is useful, since sometimes it times out
    # Adapted from Gavin Mcdonald's code at https://zenodo.org/records/11625791 which is adapted from Jen Raynor's code in emLab/Projects/current-projects/arnhold-bwmpa/project-materials/programs/helper-download_erddap.R
  #  date = v31_dates$date[1]
  # file_path = v31_dates$file_path[1]
    
    tmp_file_name <- glue::glue("{data_directory}/remss/wind-ccmp/wind_{date}.nc")
    
    # If it's already downloaded, skip, give indication if download was successful or not
    if(file.exists(tmp_file_name)) return()
    r <- NULL
    
    attempt <- 1
    
    # proceed for 10 attempts
    while(is.null(r) && attempt <= 10){
      
      attempt <- attempt + 1
      
      try(r <- download.file(file_path,
                             tmp_file_name))
      
    } 
    
    # Give indication if download was successful or not
    return()
  }, .options = furrr_options(globals=c("data_directory"),
                              seed = 101),.progress=TRUE))

# test <- stars::read_ncdf("/home/ubuntu/data_storage/raw_data/remss/wind-ccmp/wind_1993-01-02.nc") %>%
#        # Calculate wind speed from u and v components, using pythagorean theorum
#       dplyr::mutate(wind_speed_m_s = sqrt(uwnd^2 + vwnd^2)) %>%
#       # Get rid of these
#       dplyr::select(-uwnd,-vwnd,-nobs) %>% 
#       # Data are for every 6 hours, so take mean for entire day
#       aggregate(FUN = mean, by = "1 day") %>% 
#       # Convert stars to raster, so we can use exactextractr::exact_extract
#       as("Raster") %>%
#       # Need to rotate from 0-360 to -180-180, since original NC is provided in 0-360
#       raster::rotate() %>%
#       # Let's spatially aggregate by taking the mean value for each of our pixels
#       exactextractr::exact_extract(data_grid,
#                                    # From the help: "mean - the mean cell value, weighted by the fraction of each cell that is covered by the polygon"
#                                    "mean",
#                                    # Include pixel_id column so we can match on it later
#                                    append_cols = "pixel_id",
#                                    progress = FALSE) %>% 
#       # Don't save data that are just NAs
#       dplyr::filter(!dplyr::if_all(-c(pixel_id),is.na)) %>%
#       dplyr::rename(wind_speed_ms_mean = mean) 


```

# Spatially aggregate data

```{r eval = params$spatially_aggregate_data}
# Process data
# Do it in parallel?
run_parallel <- TRUE


tmp_data_directory <- glue::glue("{data_directory}/remss/clean/spatially_aggregated_{pixel_size}_degree")
 if(!dir.exists(tmp_data_directory)) dir.create(tmp_data_directory)

 if(run_parallel) plan(multisession) else plan(sequential)
 
list.files(glue("{data_directory}/remss/wind-ccmp"), pattern = "2024") %>%
   future_map_dfr(function(tmp_file_name){
         tryCatch({
     
     # tmp_file_name <- "wind_1993-03-07.nc"
     date_tmp <- tmp_file_name %>%
       # Extract date
       stringr::str_replace(glue::glue("wind_"),"") %>% 
       stringr::str_remove(".nc") %>%
       lubridate::date()
     
     processed_file_name <- glue::glue("{tmp_data_directory}/{date_tmp}.csv")
     
          message(glue::glue("Processing file: {tmp_file_name}"))
     
     # If already processed, don't do it again
     if(file.exists(processed_file_name)) {
       message(glue::glue("File already processed: {processed_file_name}"))
       return()
     }
     
 glue::glue("{data_directory}/remss/wind-ccmp/{tmp_file_name}") %>%
       stars::read_ncdf() %>%
       # Calculate wind speed from u and v components, using pythagorean theorum
       dplyr::mutate(wind_speed_m_s = sqrt(uwnd^2 + vwnd^2)) %>%
       # Get rid of these
       dplyr::select(-uwnd,-vwnd,-nobs) %>% 
       # Data are for every 6 hours, so take mean for entire day
       aggregate(FUN = mean, by = "1 day") %>% 
       # Convert stars to raster, so we can use exactextractr::exact_extract
       as("Raster") %>%
       # Need to rotate from 0-360 to -180-180, since original NC is provided in 0-360
       raster::rotate() %>%
       # Let's spatially aggregate by taking the mean value for each of our pixels
       exactextractr::exact_extract(data_grid,
                                    # From the help: "mean - the mean cell value, weighted by the fraction of each cell that is covered by the polygon"
                                    "mean",
                                    # Include pixel_id column so we can match on it later
                                    append_cols = "pixel_id",
                                    progress = FALSE) %>% 
       # Don't save data that are just NAs
       dplyr::filter(!dplyr::if_all(-c(pixel_id),is.na)) %>%
       dplyr::rename(wind_speed_ms_mean = mean) %>%
       dplyr::mutate(date = date_tmp) %>%
       data.table::fwrite(processed_file_name)
     return(NULL)

    }, error = function(e) {
      # Log the error and skip the file
      message(glue::glue("Error processing file {tmp_file_name}: {e$message}"))
      return(NULL)
    })
   }, .options = furrr_options(globals=c("data_directory","data_grid","tmp_data_directory"),
                               seed = 101),.progress=TRUE)

```

# Temporally aggregate data

We also aggregate mean and standard deviation wind speed by year and location.

```{r}
# Do it in parallel?
run_parallel <- TRUE

tmp_data_directory <- glue::glue("{data_directory}/remss/clean/spatially_aggregated_{pixel_size}_degree")

if(run_parallel) plan(multisession) else plan(sequential)

# Create sequence of start years for each 5-year chunk
start_years <- seq(2023, 2024, by = 5)

# Process each 5-year chunk
for(start_year in start_years) {
  # start_year = 1993
  end_year <- min(start_year + 4, 2024)  # Ensure we don't go beyond 2024
  years_to_process <- start_year:end_year
  
  # Get files for this 5-year chunk
  chunk_files <- list.files(tmp_data_directory, pattern = paste0(years_to_process, collapse = "|"))
  
chunk_data <- chunk_files %>% 
  future_map(function(file_temp) {
    # Construct the full path
    file_path <- glue::glue("{tmp_data_directory}/{file_temp}")
    
    # Read the file
    data <- data.table::fread(file_path)
    
    # Check if the file has rows
    if (nrow(data) == 0) {
      return(NULL)  # Return NULL for files with no rows
    }
    
    # Return the data if it has rows
    data
  }, .options = furrr_options(globals = c("tmp_data_directory"),
                               seed = 101), 
     .progress = TRUE) %>%
  
  # Remove NULL entries (from files with no rows)
  purrr::compact() %>%

  # Combine the data into one table
  data.table::rbindlist() %>%

  # Transform and process
  collapse::ftransform(year = lubridate::year(date)) %>%
  dplyr::select(-date) %>%
  dplyr::rename_with(~ gsub('_mean', '', .x)) %>%
  collapse::fgroup_by(pixel_id, year) %>% {
    collapse::add_vars(
      collapse::add_stub(collapse::fmean(., keep.group_vars = TRUE), "_mean", pre = FALSE, cols = -c(1, 2)),
      collapse::add_stub(collapse::fsd(., keep.group_vars = FALSE), "_sd", pre = FALSE)
    )
  }
  
  data.table::fwrite(chunk_data, here::here(glue::glue("data/model_features/remss_wind/wind_{start_year}_{end_year}.csv")))

  
  # Clear chunk data to free memory
  rm(chunk_data)
  gc()
}

```

