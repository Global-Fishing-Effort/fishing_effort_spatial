---
title: "Fit prediction model for fishing effort"
output: html_document
date: "2024-12-11"
---

# Summary 

We use the data compiled in the previous scripts to fit a logistic regression model (or a random forest) to predict the prescence (1) or absence (0) of fishing effort in every cell. We will do this for each flag country individually. We will start by testing some individual countries. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(betareg)
library(foreach)
library(doParallel)
library(here)
library(mgcv)
library(tictoc)
library(progress)
library(terra)
library(glue)
library(arrow)

source(here("R/dir.R"))


```

```{r}


# Load prepared data
model_data <- readRDS(here("data/model_features/prepared_data.rds")) %>%
  mutate(eez_id = as.factor(eez_id),
         fao_id = as.factor(fao_id),
         year = as.numeric(year)) %>%
  dplyr::select(-fao_region, -eez) %>% # this is data which contains all of the predictors necessary for the model.
  mutate(presence = 1) # i think we need to adjust the prepared data to include cells which have no fishing effort in them. We can do this in the loop below? 


# Prepare model formula
model_formula <- formula(
  presence ~ 
    # Categorical/factor predictors
   # flag_fin + 
   # gear  + length_category +
    # meso_region +  # need to fix this variable in prep scripts 
    factor(eez_id) + factor(fao_id) +  # Spatial categorical variables
    # Total effort predictor (log-transformed)
    log_total_fishing_hours + # honestly not sure if this is necessary. It was a suggestion from Julia to include. Basically a sort of fishing capacity variable. 
    # Continuous predictors
    lon + lat + 
    elevation_m + # depth
    distance_from_port_m + 
    distance_from_shore_m +
    chl_mg_per_m3_mean + 
    chl_mg_per_m3_sd +
    sst_c_mean + 
    sst_c_sd +
    wind_speed_mean + 
    wind_speed_sd 
  #+
    # Year effect
   # s(year, k=2) # smooth term for year; maybe not necessary? Fishing effort isn't necessarily linear so keeping for now. 
)


## read in environmental variables

# Function to prepare prediction data for a given year
prepare_pred_data <- function(yr) {
  # yr = 2015
  message(sprintf("Loading environmental data for year %d...", yr))
  
  # Load data in chunks to manage memory
  chunks <- list()
  
  # Load and process chlorophyll data
  chunks$chl <- read.csv(here("data/model_features/erdMH1chlamday/errdap_2013_2017.csv")) %>% # since we're only training on 2015-2017
    filter(year == yr) %>%
    dplyr::select(pixel_id, chl_mg_per_m3_mean, chl_mg_per_m3_sd)
  
  # Load SST data
  if(yr == 2015){
  chunks$sst <- read.csv(here("data/model_features/ncdcOisst21Agg_LonPM180/errdap_sst_2011_2015.csv")) %>%
    filter(year == yr) %>%
    dplyr::select(pixel_id, sst_c_mean, sst_c_sd)
  
  }else{
      chunks$sst <- read.csv(here("data/model_features/ncdcOisst21Agg_LonPM180/errdap_sst_2016_2020.csv")) %>%
    filter(year == yr) %>%
    dplyr::select(pixel_id, sst_c_mean, sst_c_sd)
  }
  
  # Load and process wind data
  chunks$wind <- read.csv(here("data/model_features/remss_wind/wind_2013_2017.csv")) %>%
    filter(year == yr) %>%
    dplyr::select(pixel_id, wind_speed_mean = wind_speed_ms_mean, wind_speed_sd = wind_speed_ms_sd)
  
  # Load static spatial data
  chunks$spatial <- read.csv(here("data/model_features/gfw_static_spatial_measures.csv")) %>%
  dplyr::select(pixel_id, elevation_m, distance_from_port_m, distance_from_shore_m)
  
  # Load categorical spatial data
  chunks$eez <- read.csv(here("data/model_features/eez/eez.csv")) %>%
    dplyr::select(pixel_id, eez_id)
  chunks$fao <- read.csv(here("data/model_features/fao/fao.csv")) %>%
    dplyr::select(pixel_id, fao_id)
  
  # Load global grid
  global_grid <- read.csv(here("data/model_features/global_grid.csv"))
  
  # Combine all predictor data efficiently
  message("Merging data chunks...")
  pred_data <- global_grid %>%
    left_join(chunks$spatial, by = "pixel_id") %>%
    left_join(chunks$chl, by = "pixel_id") %>%
    left_join(chunks$sst, by = "pixel_id") %>%
    left_join(chunks$wind, by = "pixel_id") %>%
    left_join(chunks$eez, by = "pixel_id") %>%
    left_join(chunks$fao, by = "pixel_id") %>%
    mutate(eez_id = as.factor(eez_id),
           fao_id = as.factor(fao_id)) %>%
    mutate(year = yr)
  
  # # Ensure categorical variables are factors with correct levels
  # pred_data$eez_id <- factor(pred_data$eez_id,
  #                        levels = unique(model$model$eez_id))
  # pred_data$fao_id <- factor(pred_data$fao_id,
  #                               levels = unique(model$model$fao_id))
  
  # Clean up chunks to free memory
  rm(chunks)
  gc()
  
  return(pred_data)
}



# Get total fishing hours for a specific year from IMAS data
get_historical_total_fishing_hours <- function(yr) {
  # This function would load and process the IMAS data for the given year
  # Note: Implement based on your IMAS data structure
  imas_data <- qs::qread(here("data/int/rousseau_gear_fix.qs")) %>%
    group_by(year, flag_fin = country, gear = gear_new, length_category) %>%
    summarize(
      total_fishing_hours = sum(eff_active_hours, na.rm = TRUE),
      .groups = 'drop'
    ) %>%
    filter(year == yr) %>%
    dplyr::select(flag_fin, year, gear, length_category, total_fishing_hours)
  
  
  return(imas_data)
}


```

Run the models 

```{r}


  env_data <- prepare_pred_data(2015) %>%
    rbind(., prepare_pred_data(2016)) %>%
    rbind(., prepare_pred_data(2017))
  
  hist_fish_data <- get_historical_total_fishing_hours(2015) %>%
    rbind(., get_historical_total_fishing_hours(2016)) %>%
    rbind(., get_historical_total_fishing_hours(2017)) %>%
    mutate(log_total_fishing_hours = log1p(total_fishing_hours))
  
  flags <- unique(hist_fish_data$flag_fin)
  gears <- unique(hist_fish_data$gear)
  years <- 2015:2017
  lengths <- unique(hist_fish_data$length_category)
  
  
unique_combinations <- hist_fish_data %>%
  distinct(year, flag_fin, gear, length_category) %>% # we only want to make models for these combinations since these are what is in the IMAS/FAO data 
  mutate(row_n = row_number())
  

# for(flag in flags){
#   for(gear_type in gears){
#     for(yr in years){
#       for(length in lengths){
        
for(row in 1:nrow(unique_combinations)){
  
#  row = 1 
  
  data <- unique_combinations %>%
    filter(row_n == row)
  
  flag = pull(data, flag_fin)
  gear_type = pull(data, gear)
  length = pull(data, length_category)
  yr = pull(data, year)
  
 # flag = "ISL"
 # gear_type = "Trawl_Midwater_or_Unsp"
 # length = "24-50m"
 # yr = 2017
  
  if(file.exists(glue(here("data/output/logistic_regression_outputs/fitted_model_{flag}_{gear_type}_{length}_{yr}.rds")))){
    cat("exists... next")
    next()
  }
        
        

# Fit beta regression model
message("Fitting logistic regression model...")
  

  model_data_flag <- model_data %>%
    dplyr::select(lon, lat, flag_fin, gear, length_category, year, presence) %>%
    filter(flag_fin == flag,
           gear == gear_type, 
           length_category == length, 
           year == yr) 
  
  
# OK we need to make sure that the data which we feel into the model has cells which have NO fishing effort in them. Hence the way we are expanding the grid below. This could cause problems computationally, given the sheer number of rows that will be produced by this, so I'm not sure this is the best way to go about this. I wonder if we should loop through each gear, length, and year combination instead? Trying to run the model with 0 fishing presence cells gives me this error: Error in qr.default(G$X) : too large a matrix for LINPACK


env_grid <- env_data %>% filter (year == yr) %>%
  dplyr::select(lon, lat, year) %>% distinct()

full_grid <- crossing(env_grid, flag_fin = flag, gear = gear_type, length_category = length)

# Join with environmental data (already included in full_grid)
full_data <- full_grid %>%
  left_join(env_data, by = c("lon", "lat", "year"))

# Join with fishing presence data
full_data <- full_data %>%
  left_join(model_data_flag, by = c("lon", "lat", "year", "flag_fin", "gear", "length_category"))


full_data <- full_data %>%
  mutate(presence = ifelse(is.na(presence), 0, presence))

full_data <- full_data %>%
  left_join(hist_fish_data) %>%
  mutate(total_fishing_hours = ifelse(is.na(total_fishing_hours), 0, total_fishing_hours),
         log_total_fishing_hours = ifelse(is.na(log_total_fishing_hours), 0, log_total_fishing_hours)) # there are some combinations that are not in the IMAS data but in the GFW data, so we'll just put total fishing hours as 0 for these cases... This will just predict 0 for those, which is fine, since we technically don't have to predict anything for these anyways. 

# what about if they are in IMAS data but not in GFW? 

ctrl <- list(nthreads = 20) # specify cores; for some reason >=24 cores didn't work, maybe a memory issue

tic()
model <- gam(
  model_formula,
  data = full_data,
  na.action = na.exclude,
  control = ctrl, # I think this does parallelization? 
  family = binomial(link = "logit")
)
toc() # took 2.5 minutes for USA, seine, over 50m, 2017; took 2.5 mins for ISL, midwater trawls, 24-50m, 2017

# Save model
saveRDS(model, glue(here("data/output/logistic_regression_outputs/fitted_model_{flag}_{gear_type}_{length}_{yr}.rds")))

}
      
  #       }
  #     }
  #   }
  # }

# Print model summary
summary(model)



# randomForest() # random forest instead of logistic regression????

```


Test to see if predictions look OK

```{r}

global_grid <- read.csv(here("data/model_features/global_grid.csv"))



all_predictions <- list()
    
# Function to make predictions for a specific combination

  flags <- unique(hist_fish_data$flag_fin)
  gears <- unique(hist_fish_data$gear)
  years <- 2015:2017
  lengths <- unique(hist_fish_data$length_category)
  
  for(yr in years){
    for(gear_type in gear){
      for(length in lengths){
        for(flag in flags){
          
          

# flag = "RUS"
# yr = 2015
# gear_type = "Lines_Longlines"
# length = "Over 50m"
    
    pred_data <- env_data %>%
      filter(year == yr)
    imas_data <- hist_fish_data %>%
      filter(year == yr)
  
      
      #flag = "USA"
    flag_data <- imas_data %>% 
    filter(year == yr, 
           flag_fin == flag) %>%
    filter(gear == gear_type,
           length_category == length) 
  
    
    model <- readRDS(glue(here("data/output/logistic_regression_outputs/fitted_model_{flag}_{gear_type}_{length}_{yr}.rds")))
    
#     eez_levels_in_model <- gsub("^eez_id", "", grep("eez_id", names(model$coefficients), value = TRUE))
# eez_levels_in_model <- str_replace_all(eez_levels_in_model, "factor\\(eez_id\\)", "") # we only want to make predictions in EEZs which are represented in the gfw data? 
# 
#    fao_levels_in_model <- gsub("^fao_id", "", grep("fao_id", names(model$coefficients), value = TRUE))
# fao_levels_in_model <- str_replace_all(fao_levels_in_model, "factor\\(fao_id\\)", "") # we only want to make predictions in faos which are represented in the gfw data? 

    
  # Create prediction dataset
  pred_df <- pred_data %>%
    left_join(flag_data)


  # Make predictions in chunks to manage memory
  chunk_size <- 10000
  n_chunks <- ceiling(nrow(pred_df) / chunk_size)
  predictions <- numeric(nrow(pred_df))
  
  pb <- progress_bar$new(
    format = "Predicting [:bar] :percent eta: :eta",
    total = n_chunks
  )

  for(j in 1:n_chunks) { # not sure if we need to chunk but might as well...
    # j = 1
    chunk_start <- (j-1) * chunk_size + 1
    chunk_end <- min(j * chunk_size, nrow(pred_df))
    chunk_indices <- chunk_start:chunk_end
    
  predicted_props <- mgcv::predict.gam(model, newdata = pred_df[chunk_indices,], type = "response") # should the predicted proportions add to one for each category? 
  
  # Ensure predictions are valid proportions
 # predicted_props <- pmax(0, pmin(1, predicted_props))
  
  predictions[chunk_indices] <- predicted_props  
  
    pb$tick()
  }
  
  # Add predictions to dataframe
  
  predictions[is.na(predictions)] <- 0 # make any NAs into 0
  
  pred_df$predicted_proportion <- predictions
  


      
      # probably wanna save these one at a time instead of in a list in the environment? 
  
  write_parquet(pred_df %>% filter(predicted_proportion > 0), glue(here("data/output/logistic_regression_predictions/preds_{flag}_{gear_type}_{length}_{yr}.parquet")))
  
        }
      }
    }
  }
      
    
# lets take a look at our predictions
all_predictions_df <- read_parquet(glue(here("data/output/logistic_regression_predictions/preds_{flag}_{gear_type}_{length}_{yr}.parquet"))) %>%
  left_join(global_grid)

test <- all_predictions_df %>%
  filter(predicted_proportion > 0) %>%
#  mutate(predicted_proportion = 1) %>%
  dplyr::select(lon, lat, predicted_proportion) %>%
  rast(., type = "xyz")
    
plot(test)

    
    ## lets look at raw GFW data 
test_gfw <- qs::qread(file.path(rdsi_dir, "prep/gfw_props/deg_half/all_effort_gear_length_props_2017.qs")) %>%
      filter(flag_fin == "ISL", gear == "Trawl_Midwater_or_Unsp", length_category == "24-50m") %>% 
      group_by(x, y) %>%
      summarise(fishing_hours = sum(fishing_hours, na.rm = TRUE)) %>%
      dplyr::select(x, y, fishing_hours) %>%
      rast(., type = "xyz")

plot(test_gfw)

```


