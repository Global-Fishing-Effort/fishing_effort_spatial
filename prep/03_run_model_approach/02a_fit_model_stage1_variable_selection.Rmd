---
title: "Fit prediction model for fishing effort: stage 1"
output: html_document
date: "2024-03-20"
editor_options: 
  chunk_output_type: console
---

# Summary 

We use the data compiled in the previous scripts to fit a random forest regression to predict the presence (1) or absence (0) of fishing effort in every cell. We will do this for each flag country individually.  

We apply a variable selection methodology of...

## Environment

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

rm(list = ls())

library(tidyverse)
library("qs")
library(betareg)
library(foreach)
library(doParallel)
library(here)
library(mgcv)
library(tictoc)
library(progress)
library(terra)
library(glue)
library(arrow)
library(strex)
library(broom)
library(randomForest)
library(PRROC)

source(here("R/dir.R"))


```

## Functions to prepare enviro and effort data  

```{r}

elnino <- read.csv(here("data/model_features/enso_index.csv"))

pdo <- read.csv(here("data/model_features/pdo_index.csv"))

world_bank <- read.csv(here("data/model_features/world_bank_regions.csv")) %>%
  filter(ISO_SOV1 != "GIB") # filter out gibralter bc it is duplicating the UK for some reason

gfi_df <- read.csv(here("data/model_features/global_fishing_index_governance.csv"))

  
# Load prepared data
model_data <- readRDS(here("data/model_features/prepared_data_1deg.rds")) %>%
  mutate(year = as.numeric(year)) %>%
  mutate(presence = 1) # i think we need to adjust the prepared data to include cells which have no fishing effort in them. We can do this in the loop below? 

## read in environmental variables

  global_grid <- read.csv(here("data/model_features/deg_1_x_1/global_grid.csv"))
  
  ocean_data <- read.csv(here("data/model_features/deg_1_x_1/errdap_chl.csv")) %>%
  left_join(read.csv(here("data/model_features/deg_1_x_1/errdap_sst.csv")), by = c("pixel_id", "year")) %>% 
  left_join(read.csv(here("data/model_features/deg_1_x_1/remss_wind.csv")), by = c("pixel_id", "year")) %>%
  filter(year %in% c(2015:2017)) 


  spatial_data <- global_grid %>%
    left_join(read.csv(here("data/model_features/deg_1_x_1/gfw_static_spatial_measures.csv")) %>% dplyr::select(-lat, -lon), by = "pixel_id") %>%
   left_join(read.csv(here("data/model_features/deg_1_x_1/mesopelagiczones/mesopelagiczones_fixed.csv")), by = "pixel_id") %>%
   left_join(read.csv(here("data/model_features/deg_1_x_1/eez/eez.csv")), by = "pixel_id") %>%
   left_join(read.csv(here("data/model_features/deg_1_x_1/fao/fao_fixed.csv")), by = "pixel_id") %>%
   left_join(read.csv(here("data/model_features/deg_1_x_1/oceans_fixed.csv")), by = "pixel_id") %>%
    left_join(read.csv(here("data/model_features/deg_1_x_1/seamounts.csv")), by = "pixel_id") %>% 
   crossing(., year = c(2015:2017)) 
  
  
  env_data <- spatial_data %>%
    left_join(ocean_data) %>% # cool.
    dplyr::select(-geometry_wkt) %>%
    left_join(elnino) %>%
    left_join(pdo) %>%
    left_join(world_bank, by = c("eez_id" = "MRGID_SOV1")) %>%
    mutate(eez_region_world_bank_7 = ifelse(ISO_SOV1 %in% c("High seas", "Land"), "High seas", eez_region_world_bank_7)) %>% 
    left_join(gfi_df, by = c("ISO_SOV1" = "flag_fin")) %>% # add in global fishing index data here
    mutate(gov_score = ifelse(eez_id >= 99999 & is.na(gov_score), "high_seas", gov_score)) %>%
        mutate(gov_score = ifelse(eez_id < 99999 & is.na(gov_score), "no_data", gov_score)) %>%
    dplyr::select(-ISO_SOV1, -nearest_seamount_id) %>% 
    distinct() 


# Get total fishing hours for a specific year from IMAS data
get_historical_total_fishing_hours <- function(yr) {
  # This function would load and process the IMAS data for the given year
  # Note: Implement based on your IMAS data structure
  imas_data <- qs::qread(here("data/int/rousseau_gear_fix.qs")) %>%
    
    ## remove artisanal
    filter(sector == "I") %>%
    
    group_by(year, flag_fin = country, gear = gear_new, length_category) %>%
    summarize(
      total_fishing_hours = sum(eff_active_hours, na.rm = TRUE),
      .groups = 'drop'
    ) %>%
    filter(year == yr) %>%
    dplyr::select(flag_fin, year, gear, length_category, total_fishing_hours)
  
  
  return(imas_data)
}


```

## Specify model formula and load data 

```{r}

# Prepare model formula
# apply model on train 
model_formula_rf <- formula(
  presence ~ 
    # Categorical/factor predictors
    gear  + 
    length_category +
    meso_id +
    eez_id +
    fao_id +
    ocean +  # Spatial categorical variables
    eez_region_world_bank_7 + # world bank regions
    gov_score + # global fishing index; make sure this is categorical and not a numeric variable
    # Continuous predictors
    lon + lat + 
    elevation_m + # depth
    distance_from_port_m + 
    distance_from_shore_m +
    chl_mg_per_m3_mean + 
    chl_mg_per_m3_sd +
    sst_c_mean + 
    sst_c_sd +
    wind_speed_ms_mean + 
    wind_speed_ms_sd +
    enso_index_mean + # el nino data 
    enso_index_sd + 
    pdo_index_mean +  # pacific decadal oscillation
    pdo_index_sd + 
    nearest_seamount_distance_m + 
    year
)
  

hist_fish_data <- get_historical_total_fishing_hours(2015) %>%
    rbind(., get_historical_total_fishing_hours(2016)) %>%
    rbind(., get_historical_total_fishing_hours(2017)) %>%
    mutate(log_total_fishing_hours = log1p(total_fishing_hours)) %>%
    filter(total_fishing_hours > 0)
  
flags <- unique(hist_fish_data$flag_fin) # get the flags we need to run models for # 167 of them

unique_combinations <- hist_fish_data %>%
  distinct(year, flag_fin, gear, length_category) %>% # we only want to make models for these combinations since these are what is in the IMAS/FAO data 
  mutate(row_n = row_number())

# test_aus <- unique_combinations %>% filter(flag_fin == "AUS") # ok, AUS for example has 128 different combinations that 1 model will be able to predict. 128*46229 = ~6 million rows being fed into the model. Will this work?? Let's try! 
#   
# 
# setdiff(unique(hist_fish_data$flag_fin), unique(model_data$flag_fin))

env_grid <- env_data %>% 
  dplyr::select(lon, lat) %>% distinct()

```

## Run models and variable selection for every individual country

 - calculate the full model using all variables available per flag country
 - calculate variable importance metrics and RMSE or AUC
 - Set a threshold of the 10th quantile of the variable importance (GINI index for classification) and remove any variables with variable importance less than that
 - Rerun model with new variables and calculate model importance metrics and AUC-PR again
 - If the AUC doesn't increase at all, we stop the model pruning, if it does, we continue, hoping that the AUC will improve even more in the next iteration. 
 - If the threshold doesn't remove any variables, we increase the threshold by 1% until a variable is removed, and rerun the process
 - We loop through this until the AUC does not improve at all. 
 
 NOTE: lets use Area Under the Precision-Recall Curve (AUC-PR) instead of RMSE because RMSE isn't really relevant to classification regression; Higher AUC-PR value is better. 



Run full models first 

```{r}

flags <- unique(hist_fish_data$flag_fin)

missing_flags <- setdiff(unique(hist_fish_data$flag_fin), unique(model_data$flag_fin))
length(missing_flags) # 28 these are flags that are in the rousseau data but not in the gfw data, meaning we can't fit a model or make predictions on them.

flags <- setdiff(flags, missing_flags)

flags <- setdiff(flags, c("BGR", "BEL", "CMR"))

# Set up parallel backend
num_cores <- 3 
cl <- makeCluster(num_cores)
registerDoParallel(cl)

# Run the loop in parallel
foreach(flag = flags, .packages = c("dplyr", "tidyr", "randomForest", "qs", "glue", "tictoc")) %dopar% {
  
    # flag = "BGR"
  model_data_flag <- model_data %>%
    dplyr::select(lon, lat, flag_fin, gear, length_category, year, presence) %>%
    filter(flag_fin == flag)
  
  
  distinct_cats <- model_data_flag %>%
    distinct(year, flag_fin, gear, length_category)
  
  full_grid <- tidyr::crossing(env_grid, distinct_cats)
  
  full_data <- full_grid %>%
    left_join(env_data, by = c("lon", "lat", "year")) %>%
    left_join(model_data_flag, by = c("lon", "lat", "year", "flag_fin", "gear", "length_category")) %>%
    mutate(presence = ifelse(is.na(presence), 0, presence)) %>%
    dplyr::select(-pixel_id)
  
  if (nrow(full_data) == 0) {
    cat(flag, "not in data... skipping\n")
    return(NULL)
  }
  
  data_random_forest <- full_data %>% 
    na.omit() %>%
    mutate(presence = as.factor(presence)) %>%
    distinct()
  
  if (nrow(full_data) == 1) {
    cat(flag, "not enough rows\n")
    return(NULL)
  }
  
  # Split data into training and testing sets
  set.seed(123)
  samp <- sample(nrow(data_random_forest), 0.6 * nrow(data_random_forest)) 
  train <- data_random_forest[samp, ]
  test <- data_random_forest[-samp, ]
  
  tic()
  model <- randomForest(model_formula_rf, data = train, type = "classification", proximity = FALSE, ntree = 100, importance = TRUE)
  toc()
  
 # model <- qs::qread(glue(file.path(rdsi_dir, "prep/random_forest/stage_1_models/pruning/stage_1_rf_train_{flag}_25.qs")))

  
  var_imp_i <- importance(model)
  
  n_vars <- nrow(var_imp_i)
  qs::qsave(model, glue(file.path(rdsi_dir, "prep/random_forest/stage_1_models/pruning/stage_1_rf_train_{flag}_{n_vars}.qs")))

  
}

stopCluster(cl)

```
 
Now apply variable selection methods 

```{r}
# Threshold-Based Selection: Remove variables with importance scores below a certain threshold (e.g., the median or a predefined percentage of the highest importance value).
# Recursive Feature Elimination (RFE): Iteratively remove the least important variable and re-run the model until performance stabilizes. - NOTE: this would probably be the best option, but would take the longest 
#### How to check when model performance stabilizes?? Look at AUC-PR vs number of variables in each model. When the AUC-PR stops getting better is where you make the variable delineation.  


## lets try the threshold based selection, where the threshold is 10% of max importance score (i.e. we keep any variable that is above that 10% of max importance score) and rerun the model and test to see if RMSE improves or not 

flags <- unique(hist_fish_data$flag_fin)

missing_flags <- setdiff(unique(hist_fish_data$flag_fin), unique(model_data$flag_fin))
length(missing_flags) # 28 these are flags that are in the rousseau data but not in the gfw data, meaning we can't fit a model or make predictions on them.

flags <- setdiff(flags, missing_flags)


# Set up parallel backend
#### NOTE ABOUT PARALLEL: THIS ISN'T WORKING; for some reason it gets hung up when saving the new rf model and saves it as realllllly large and only saves one per flag, which isn't correct. I think it might have to do with the while loops I've included. In theory it should work though... Without it takes ~12 hours to run. 
# num_cores <- 3 # Use one less than the total available cores
# cl <- makeCluster(num_cores)
# registerDoParallel(cl)

# Run the loop in parallel
# foreach(flag = flags, .packages = c("dplyr", "tidyr", "randomForest", "qs", "glue", "tictoc")) %dopar% {

  auc_values <- c()
  num_vars <- c()
  
#  tracking_df <- data.frame(flag = NA, auc = NA, n_vars = NA)

for(flag in flags){
  
    # flag = "GNQ"
  model_data_flag <- model_data %>%
    dplyr::select(lon, lat, flag_fin, gear, length_category, year, presence) %>%
    filter(flag_fin == flag)
  
  if(nrow(model_data_flag) == 1){
    cat(flag, "not enough categories to model... skipping\n") # should i keep track of the flags we are missing? 
    # return(NULL)
    next()
  }
  
  distinct_cats <- model_data_flag %>%
    distinct(year, flag_fin, gear, length_category)
  
  full_grid <- tidyr::crossing(env_grid, distinct_cats)
  
  full_data <- full_grid %>%
    left_join(env_data, by = c("lon", "lat", "year")) %>%
    left_join(model_data_flag, by = c("lon", "lat", "year", "flag_fin", "gear", "length_category")) %>%
    mutate(presence = ifelse(is.na(presence), 0, presence)) %>%
    dplyr::select(-pixel_id)
  
  if (nrow(full_data) == 0) {
    cat(flag, "not in data... skipping\n")
    # return(NULL)
    next()
  }
  
  data_random_forest <- full_data %>% 
    na.omit() %>%
    mutate(presence = as.factor(presence)) %>%
    distinct()
  
  if (nrow(full_data) == 1) {
    cat(flag, "not enough rows\n")
    # return(NULL)
    next()
  }
  
  # Split data into training and testing sets
  set.seed(123)
  samp <- sample(nrow(data_random_forest), 0.6 * nrow(data_random_forest)) 
  train <- data_random_forest[samp, ]
  test <- data_random_forest[-samp, ]
  
  ## if the file doesn't exist for some reason, run this. They should all be there from the previous chunk though.
  if(!file.exists((glue(file.path(rdsi_dir, "prep/random_forest/stage_1_models/pruning/stage_1_rf_train_{flag}_25.qs"))))){
    
  tic()
  model <- randomForest(model_formula_rf, data = train, type = "classification", proximity = FALSE, ntree = 100, importance = TRUE)
  toc()
  
  }else{ 
    model <- qs::qread(glue(file.path(rdsi_dir, "prep/random_forest/stage_1_models/pruning/stage_1_rf_train_{flag}_25.qs")))
}
  
  var_imp_i <- importance(model)
  num_vars <- c(nrow(var_imp_i))
  threshold_i <- quantile(var_imp_i[, 4], 0.10)  # Use the 10th percentile instead of a fixed multiplier
  
    # threshold_i <- max(var_imp_i[, 4])*0.1  # Use the 10th percentile instead of a fixed multiplier

  
  if (threshold_i == 0) {
    threshold_i <- 0.0000001
  }
  
  
  pred_probs <- predict(model, newdata = test, type = "prob")[, 2]
  
  true_labels <- as.numeric(as.character(test$presence))  # Replace with the actual labels in your test data

  pr_curve <- pr.curve(scores.class0 = pred_probs, weights.class0 = true_labels, curve = TRUE)
  auc_pr <- pr_curve$auc.integral # higher auc_pr is BETTER. So if it gets lower, then we don't want the next model 

  auc_values <- c(auc_values, auc_pr)
  # rmse <- sqrt(mean((as.numeric(as.character(test$presence)) - pred_probs)^2))
  # rmse_values <- c(rmse_values, rmse)
  
  iteration <- 1
 while (TRUE) {
    
   selected_vars <- names(var_imp_i[, 4][var_imp_i[, 4] > threshold_i])
    
    # Rank variables by importance (column 4 of var_imp_i)
   #  sorted_vars <- names(sort(var_imp_i[, 4], decreasing = TRUE))
    
    
    
     if (length(selected_vars) < 6) break  # Ensure at least 5 variables remain
    
        # selected_vars <- sorted_vars[-length(sorted_vars)]
        
        # Increase the threshold iteratively until at least one variable is removed
    while (length(selected_vars) == num_vars[length(num_vars)]) {
        threshold_i <- threshold_i * 1.01  # Increase threshold by 1%
        selected_vars <- names(var_imp_i[, 4][var_imp_i[, 4] > threshold_i])
    }
    
    model_i <- randomForest(presence ~ ., data = train[, c("presence", selected_vars)], 
                            type = "classification", proximity = FALSE, 
                            ntree = 100, importance = TRUE)
    
    var_imp_i <- importance(model_i)
    
    # Compute a new threshold dynamically based on the 10th percentile
    threshold_i <- quantile(var_imp_i[, 4], 0.10)  # Use the 10th percentile instead of a fixed multiplier
    # threshold_i <- max(var_imp_i[, 4])*0.1  # Use 10% of max importance

    pred_probs_i <- predict(model_i, newdata = test, type = "prob")[, 2]
    # rmse_i <- sqrt(mean((as.numeric(as.character(test$presence)) - pred_probs_i)^2))
    
   true_labels_i <- as.numeric(as.character(test$presence))  # Replace with the actual labels in your test data

  pr_curve <- pr.curve(scores.class0 = pred_probs_i, weights.class0 = true_labels_i, curve = TRUE)
  auc_pr_i <- pr_curve$auc.integral # higher auc_pr is BETTER. So if it gets lower, then we don't want the next model 

  auc_values <- c(auc_values, auc_pr_i)
    
    # rmse_values <- c(rmse_values, rmse_i)
    num_vars <- c(num_vars, length(selected_vars))
    
    n_vars <- length(selected_vars)
    file_path <- glue(file.path(rdsi_dir, "prep/random_forest/stage_1_models/pruning/stage_1_rf_train_{flag}_{n_vars}.qs"))

    if(!file.exists(file_path)){

          qs::qsave(model_i, file_path)
    }
    
    if (length(auc_values) > 1) {
      
      model_improvement <- auc_values[length(auc_values)] - auc_values[length(auc_values) - 1]
        
        if (model_improvement <= 0)  break  # Stop if improvement is <=0
        
    }
    
    iteration <- iteration + 1
 }

  
  # tracking_df_i <- data.frame(flag = flag, auc = auc_values, n_vars = num_vars)
  # 
  # tracking_df <- rbind(tracking_df, tracking_df_i)
  
}

# Stop parallel cluster
#stopCluster(cl)




```


Now write code to select the "best" model per the variable selection from above. We will rerun the model with just those variables on the FULL dataset to leverage all of the data from GFW and save

```{r}
# grab the flags that were run in the folder
stage_1_path <- file.path(rdsi_dir, "prep/random_forest/stage_1_models/pruning")
stage_1_files <- list.files(stage_1_path, full.names = TRUE)
stage_1_flags <- unique(sub(".*stage_1_rf_train_([A-Z]{3})_.*", "\\1", stage_1_files)) # 118 flags; weird there are more here than in stage 2?!


stage_1_path_done <- file.path(rdsi_dir, "prep/random_forest/stage_1_models/")
stage_1_files_done <- list.files(stage_1_path_done, full.names = TRUE)[-1]
stage_1_flags_done <- unique(sub(".*stage_1_rf_model_full_data_([A-Z]{3})_.*", "\\1", stage_1_files_done)) # 118 flags; weird there are more here than in stage 2?!

stage_1_flags <- setdiff(stage_1_flags, stage_1_flags_done)

# stage_1_flags <- c("AGO", "ALB", "AUS")

# num_cores <- 3 # Use one less than the total available cores
# cl <- makeCluster(num_cores)
# registerDoParallel(cl)
# foreach(flag = stage_1_flags, .packages = c("dplyr", "tidyr", "randomForest", "qs", "glue", "tictoc", "strex", "stats")) %dopar% {

for(flag in stage_1_flags) {

#  flag = "ALB"
  flag_files <- list.files(stage_1_path, pattern = glue("_{flag}_"))
  # select the model with the next to lowest number of variables. E.g., if the final model run for ZAF is 17, then we want to grab the model with the next to lowest number of variables, which is 19. 
  n_variables <- as.numeric(str_before_first(str_after_last(flag_files, "_"), "\\."))
  
  if(length(n_variables) > 1){
  best_model_n <- as.character(sort(n_variables[2]))
  }else{
    best_model_n <- as.character(n_variables[1])
  }
  
  best_train_model <- qs::qread(file.path(stage_1_path, glue("stage_1_rf_train_{flag}_{best_model_n}.qs")))
  
  rf_formula <- as.formula(deparse(formula(best_train_model)) |> paste(collapse = " "))
  
  model_data_flag <- model_data %>%
    dplyr::select(lon, lat, flag_fin, gear, length_category, year, presence) %>%
    filter(flag_fin == flag)
  
  
  distinct_cats <- model_data_flag %>%
    distinct(year, flag_fin, gear, length_category)
  
  full_grid <- tidyr::crossing(env_grid, distinct_cats)
  
  full_data <- full_grid %>%
    left_join(env_data, by = c("lon", "lat", "year")) %>%
    left_join(model_data_flag, by = c("lon", "lat", "year", "flag_fin", "gear", "length_category")) %>%
    mutate(presence = ifelse(is.na(presence), 0, presence)) %>%
    dplyr::select(-pixel_id)
  
  # if (nrow(full_data) == 0) { # shouldn't ever happen but just in case
  #   cat(flag, "not in data... skipping\n")
  #   # return(NULL)
  #   next()
  # }
  
  data_random_forest <- full_data %>% 
    na.omit() %>%
    mutate(presence = as.factor(presence)) %>%
    distinct()
  
  # if (nrow(full_data) == 1) { # shouldn't ever happen but just in case
  #   cat(flag, "not enough rows\n")
  #   #return(NULL)
  #   next()
  # }
  
  set.seed(123)
  
  tic()
  model <- randomForest(rf_formula, data = data_random_forest, type = "classification", proximity = FALSE, ntree = 100)
  toc()

  qs::qsave(model, glue(file.path(rdsi_dir, "prep/random_forest/stage_1_models/stage_1_rf_model_full_data_{flag}_{best_model_n}.qs")))


}
 # stopCluster(cl)
 
  

```

Check to see if it worked properly; testing AGO

```{r}

flag = "AGO"

## read in models 
files <- list.files(file.path(rdsi_dir, "prep/random_forest/stage_1_models/pruning/"), pattern = "AGO", full.names = TRUE)

all_models <- lapply(files, qs::qread)

## get training and testing datasets
  model_data_flag <- model_data %>%
    dplyr::select(lon, lat, flag_fin, gear, length_category, year, presence) %>%
    filter(flag_fin == flag)
  
  distinct_cats <- model_data_flag %>%
    distinct(year, flag_fin, gear, length_category)
  
  full_grid <- tidyr::crossing(env_grid, distinct_cats)
  
  full_data <- full_grid %>%
    left_join(env_data, by = c("lon", "lat", "year")) %>%
    left_join(model_data_flag, by = c("lon", "lat", "year", "flag_fin", "gear", "length_category")) %>%
    mutate(presence = ifelse(is.na(presence), 0, presence)) %>%
    dplyr::select(-pixel_id)
  
  data_random_forest <- full_data %>% 
    na.omit() %>%
    mutate(presence = as.factor(presence)) %>%
    distinct()
  
  
  # Split data into training and testing sets
  set.seed(123)
  samp <- sample(nrow(data_random_forest), 0.6 * nrow(data_random_forest)) 
  train <- data_random_forest[samp, ]
  test <- data_random_forest[-samp, ]

  # test the AUC values
  

# Function to calculate AUC-PR
calculate_auc_pr <- function(model, test_data, test_labels) {
 # model <- all_models[[i]]
  
  # Predict probabilities for the positive class (assumes binary classification)
    probs <- predict(model, newdata = test_data, type = "prob")[, 2]
  
  # Calculate AUC-PR using PRROC package
  pr <- pr.curve(scores.class0 = probs, weights.class0 = test_labels, curve = TRUE)$auc.integral
  
  return(pr)  # Return the AUC-PR value
}

# Initialize a vector to store AUC-PR scores for each model
auc_pr_scores <- numeric(length(all_models))
 
 test_labels <- as.numeric(as.character(test$presence))  # Adjust based on your data



# Loop through models and calculate AUC-PR
for (i in 1:length(all_models)) {
  # i = 2
  
  auc_pr_scores[i] <- calculate_auc_pr(model = all_models[[i]], test_data = test, test_labels = test_labels)
}
 
 auc_pr_scores # ok this makes sense. The next to last model run (in this case, the second number in the list) is the highest. 
  
```



**How the model from McDonald et al works:**

**Overview of the Model**
The researchers developed a two-stage hurdle random forest model to predict fishing effort:

Stage 1 (Classification/Extensive Margin): Predicts whether any fishing occurs in a pixel (binary outcome: 0 or 1)
Stage 2 (Regression/Intensive Margin): Predicts the intensity of fishing effort if fishing occurred (continuous outcome: hours/m²)

**Data Used in Both Stages**
Both stages use the same set of input features, which include:

1. MPA implementation features (11 features): Distance to nearest MPA, years since MPA designation, fraction of MPA coverage, etc.
2. Environmental features (12 features): Sea surface temperature, chlorophyll-A, wind speed, etc.
3. Geographic features (7 features): Latitude, longitude, distance to shore, distance to seamount, bathymetry depth, etc.
4. Governance features (6 features): EEZ presence, governance capacity scores, etc.
5. Economic features (3 features): Distance to port, fuel prices, etc.
6. Technological features (2 features): AIS reception quality metrics
7. Residual effects features: Lagged fishing effort from previous years, year variable

**Relationship Between Stage 1 and Stage 2**
Training Process: The stages are trained independently but with different subsets of data:

 - Stage 1 uses all observations from the training dataset
 - Stage 2 only uses observations where fishing effort is non-zero (conditional on fishing occurring)
 
Prediction Process: When making predictions, the outputs from both stages are combined:

 - The paper explicitly states: "We combine each Stage 1 and Stage 2 out-of-sample predictions into full hurdle model predictions. For each observation, we simply multiply the stage 1 classification prediction (0 or 1) by the stage 2 prediction (h/m² of fishing effort)."
  
Data Flow: The Stage 1 model does not directly feed data into Stage 2 during training. Instead:

 - Stage 1 determines IF fishing occurs (probability of fishing)
 - Stage 2 determines HOW MUCH fishing occurs (intensity of fishing)
 - The final prediction multiplies these two outputs together
 
This is a classic hurdle model approach where the first stage models the "hurdle" of whether an event occurs at all, and the second stage models the intensity given that the hurdle is crossed.

**Performance Metrics**
 - Stage 1 (Classification): Evaluated using ROC area-under-curve (~0.97), F1 score (~0.91), precision, and recall
 - Stage 2 (Regression): Evaluated using R² (~0.8), RMSE, and normalized RMSE
 
**Summary**
The two stages work together but are trained separately. Stage 1 doesn't directly feed data into Stage 2 during training, but their outputs are multiplied together during prediction. This approach allows the model to separately handle the binary question of "does fishing occur?" and the continuous question of "how much fishing occurs?" which is particularly useful when dealing with data that has many zeros (areas with no fishing).

