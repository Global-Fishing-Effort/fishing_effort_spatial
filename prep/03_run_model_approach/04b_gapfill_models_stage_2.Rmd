---
title: "Regional models for gapfilling"
output: html_document
date: "2024-12-11"
---

# Summary

Here we fit stage 1 larger regional models which will be used for gapfilling missing flag countries in the GFW data. We fit regional models for these 12 regions: 

 - sub continental reported in Rousseau data (SAUPtoCountry.csv)? E.g., [1] "SubAfrica"      "Europe"         "LatinAmerica"   "AustraliaNZ"    "IndianP"        "Caribbean"      "SouthEastAsia" 
 [8] "NorthAmerica"   "NorthEastAsia"  "IslandsNations" "NWAfrica"       "MidEast"     
 
```{r}
knitr::opts_chunk$set(echo = TRUE)

rm(list = ls())

library(tidyverse)
library("qs")
library(foreach)
library(doParallel)
library(here)
library(mgcv)
library(tictoc)
library(progress)
library(terra)
library(glue)
library(arrow)
library(strex)
library(broom)
library(randomForest)
library(PRROC)
library(janitor)
library(ranger)

source(here("R/dir.R"))

```
 
## Functions to prepare enviro and effort data  
```{r}

elnino <- read.csv(here("data/model_features/enso_index.csv"))

pdo <- read.csv(here("data/model_features/pdo_index.csv"))

world_bank <- read.csv(here("data/model_features/world_bank_regions.csv")) 

gfi_df <- read.csv(here("data/model_features/global_fishing_index_governance.csv"))

  
# Load prepared data
model_data <- readRDS(here("data/model_features/prepared_regional_data_1deg.rds")) %>%
  mutate(year = as.numeric(year))


## read in environmental variables

  global_grid <- read.csv(here("data/model_features/deg_1_x_1/global_grid.csv"))
  
  ocean_data <- read.csv(here("data/model_features/deg_1_x_1/errdap_chl.csv")) %>%
  left_join(read.csv(here("data/model_features/deg_1_x_1/errdap_sst.csv")), by = c("pixel_id", "year")) %>% 
  left_join(read.csv(here("data/model_features/deg_1_x_1/remss_wind.csv")), by = c("pixel_id", "year")) %>%
  filter(year %in% c(2015:2024)) 


  spatial_data <- global_grid %>%
    left_join(read.csv(here("data/model_features/deg_1_x_1/gfw_static_spatial_measures.csv")) %>% dplyr::select(-lat, -lon), by = "pixel_id") %>%
   left_join(read.csv(here("data/model_features/deg_1_x_1/mesopelagiczones/mesopelagiczones_fixed.csv")), by = "pixel_id") %>%
   left_join(read.csv(here("data/model_features/deg_1_x_1/eez/eez_fix.csv")), by = "pixel_id") %>%
   left_join(read.csv(here("data/model_features/deg_1_x_1/fao/fao_fixed.csv")), by = "pixel_id") %>%
   left_join(read.csv(here("data/model_features/deg_1_x_1/oceans_fixed.csv")), by = "pixel_id") %>%
    left_join(read.csv(here("data/model_features/deg_1_x_1/seamounts.csv")), by = "pixel_id") %>% 
   crossing(., year = c(2015:2024)) 
  
      
  mpa_data <- qs::qread(here("data/model_features/deg_1_x_1/mpa/mpa_data_all.qs")) %>%
    filter(year >= 2015)
  
  env_data <- spatial_data %>%
    left_join(ocean_data) %>% # cool.
    left_join(mpa_data) %>%
    dplyr::select(-geometry_wkt) %>%
    left_join(elnino) %>%
    left_join(pdo) %>%
    left_join(world_bank, by = c("eez_id")) %>%
    mutate(eez_region_world_bank_7 = ifelse(eez_sovereign %in% c("High seas", "Land"), "High seas", eez_region_world_bank_7)) %>% 
    left_join(gfi_df, by = c("eez_sovereign" = "flag_fin")) %>% # add in global fishing index data here
    mutate(gov_score = ifelse(eez_id >= 99999 & is.na(gov_score), "high_seas", gov_score)) %>%
        mutate(gov_score = ifelse(eez_id < 99999 & is.na(gov_score), "no_data", gov_score)) %>%
    dplyr::select(-eez_sovereign, -nearest_seamount_id) %>% 
    distinct() 

  gfw_reception_data <- read.csv(here("data/model_features/deg_1_x_1/gfw_reception_quality.csv"))

``` 
 
## Specify model formula and regions to run 

```{r}

# Prepare model formula
# apply model on train 
model_formula_rf <- formula(
  prop_fishing_hours_cell ~ 
    # Categorical/factor predictors
    gear  + 
    length_category +
    meso_id +
    eez_id +
    fao_id +
    ocean +  # Spatial categorical variables
    gov_score + # global fishing index; make sure this is categorical and not a numeric variable
    # Continuous predictors
    lon + lat + 
    elevation_m + # depth
    distance_from_port_m + 
    distance_from_shore_m +
    chl_mg_per_m3_mean + 
    chl_mg_per_m3_sd +
    sst_c_mean + 
    sst_c_sd +
    wind_speed_ms_mean + 
    wind_speed_ms_sd +
    enso_index_mean + # el nino data 
    enso_index_sd + # pacific decadal oscillation
    pdo_index_mean +  
    pdo_index_sd + 
    nearest_seamount_distance_m + 
    year +
    ais_reception_positions_per_day_class_A +
    ais_reception_positions_per_day_class_B + 
    nearest_mpa_distance_m + 
    nearest_mpa_years_since_designation
)
  
env_grid <- env_data %>% 
  dplyr::select(lon, lat) %>% distinct()

## Use the regions as described in Rousseau et al, and assigning any landlocked countries to their continent:

rousseau_regions <- read.csv("https://data.imas.utas.edu.au/attachments/1241a51d-c8c2-4432-aa68-3d2bae142794/SAUPtoCountry.csv") %>%
  dplyr::select(-X) %>%
  rename(region = Region) 

fix_landlocked <- rousseau_regions %>%
  filter(region == "Landlocked") %>%
  mutate(region = case_when(
    Country == "AFG" ~ "MidEast",
    Country == "AND" ~ "Europe",
    Country == "AZE" ~ "MidEast",
    Country == "AUT" ~ "Europe",
    Country == "ARM" ~ "MidEast",
    Country == "BTN" ~ "SouthEastAsia",
    Country == "BOL" ~ "LatinAmerica",
    Country == "BWA" ~ "SubAfrica",
    Country == "BDI" ~ "SubAfrica",
    Country == "BLR" ~ "Europe",
    Country == "CAF" ~ "SubAfrica",
    Country == "TCD" ~ "NWAfrica",
    Country == "CSK" ~ "Europe",   # Historical, treat as Europe if included
    Country == "CZE" ~ "Europe",
    Country == "ETH" ~ "SubAfrica",
    Country == "HUN" ~ "Europe",
    Country == "KAZ" ~ "MidEast",
    Country == "KGZ" ~ "MidEast",
    Country == "LAO" ~ "SouthEastAsia",
    Country == "LSO" ~ "SubAfrica",
    Country == "LIE" ~ "Europe",
    Country == "LUX" ~ "Europe",
    Country == "MWI" ~ "SubAfrica",
    Country == "MLI" ~ "NWAfrica",
    Country == "MNG" ~ "NorthEastAsia",
    Country == "MDA" ~ "Europe",
    Country == "NPL" ~ "SouthEastAsia",
    Country == "NER" ~ "NWAfrica",
    Country == "PRY" ~ "LatinAmerica",
    Country == "RWA" ~ "SubAfrica",
    Country == "SMR" ~ "Europe",
    Country == "SVK" ~ "Europe",
    Country == "SWZ" ~ "SubAfrica",
    Country == "CHE" ~ "Europe",
    Country == "TJK" ~ "MidEast",
    Country == "TKM" ~ "MidEast",
    Country == "UGA" ~ "SubAfrica",
    Country == "MKD" ~ "Europe",
    Country == "BFA" ~ "NWAfrica",
    Country == "UZB" ~ "MidEast",
    Country == "SRB" ~ "Europe",
    Country == "ZMB" ~ "SubAfrica",
    Country == "FLK" ~ "LatinAmerica", # CHECK THIS
    TRUE ~ region
  ))

rousseau_fix <- rousseau_regions %>%
  filter(region != "Landlocked") %>%
  rbind(fix_landlocked) %>%
  dplyr::select(-SAUP) %>%
  filter(region  != "Unknown") %>%
    add_row(Country = "ATF", region = "IslandNations") %>%
    add_row(Country = "VAT", region = "Europe") %>%
    add_row(Country = "BES", region = "Caribbean") %>%
    add_row(Country = "CCK", region = "IslandNations")

## ok we could use the world bank regions
hist_fish_data <- qs::qread(here("data/int/rousseau_gear_fix.qs")) %>%
    ## remove artisanal
    filter(sector == "I") %>%
    group_by(flag_fin = country, gear = gear_new, length_category) %>%
    summarize(
      total_fishing_hours = sum(eff_active_hours, na.rm = TRUE),
      .groups = 'drop'
    ) %>%
    dplyr::select(flag_fin, gear, length_category, total_fishing_hours) %>%
    filter(total_fishing_hours > 0) %>%
  left_join(rousseau_fix, by = c("flag_fin" = "Country"))

missing <- hist_fish_data %>%
  filter(is.na(region)) %>%
  distinct(flag_fin, region) # 0 good

```

## Run models and variable selection for every region

 - calculate the "full" model using all variables available per region (24 variables)
 - calculate variable importance metrics and root mean squared error (RMSE)
 - Set a threshold of the 10th quantile of the variable importance (RMSE for regression) and remove any variables with variable importance less than that
 - Rerun model with new variables and calculate model importance metrics and RMSE again
 - If the RMSE doesn't DECREASE at all, we stop the model pruning, if it does, we continue, hoping that the RMSE will improve even more in the next iteration. 
 - If the threshold doesn't remove any variables, we increase the threshold by 1% until a variable is removed, and rerun the process
 - We loop through this until the RMSE does not improve at all (improvement being a decrease in RMSE)
 
 NOTE: Lower RMSE is better

Save full models first 


```{r}
regions_to_run <- unique(hist_fish_data$region) # get the regions we want to run

model_data_loop <- model_data %>% 
  filter(!is.na(region)) # filter out the unknown flags 

# Set up parallel backend
num_cores <- 12 # Use one less than the total available cores
cl <- makeCluster(num_cores)
registerDoParallel(cl)

# Run the loop in parallel
foreach(region_loop = regions_to_run, .packages = c("dplyr", "tidyr", "randomForest", "qs", "glue", "tictoc", "ranger")) %dopar% {
 
  # region_loop = "IndianP"
  
  model_data_region <- model_data_loop %>%
  dplyr::select(lon, lat, region, gear, length_category, year, prop_fishing_hours_cell) %>%
  filter(region == region_loop) %>% # only 2419 rows for new zealand - could be problematic! 
  left_join(env_data) %>%
  left_join(gfw_reception_data) %>%
  na.omit() # this removes the categories which are not in the Rousseau data.

if(nrow(model_data_region) == 1){
  next()
}
  
set.seed(123)
samp <- sample(nrow(model_data_region), 0.6 * nrow(model_data_region))  # do 60/40 split since this data is mostly small
train <- model_data_region[samp, ]
test <- model_data_region[-samp, ]

tic()
model <- ranger(
  formula = model_formula_rf,
  data = train,
  num.trees = 100,
  importance = "impurity",  # Or "permutation" for permutation-based importance
  write.forest = TRUE,
  classification = FALSE
)
toc() 

# Initialize tracking
var_imp_i <- data.frame(ranger::importance(model))
n_vars <- nrow(var_imp_i)

  qs::qsave(model, glue(file.path(rdsi_dir, "prep/random_forest/stage_2_models_regional/pruning/stage_2_rf_train_{region_loop}_{n_vars}.qs")))
   
}
stopCluster(cl)

```

Now apply variable selection methods 


```{r}
num_cores <- 12 # Use one less than the total available cores
cl <- makeCluster(num_cores)
registerDoParallel(cl)

# Run the loop in parallel
foreach(region_loop = regions_to_run, .packages = c("dplyr", "tidyr", "randomForest", "qs", "glue", "tictoc", "ranger", "PRROC")) %dopar% {

    # region_loop = "IndianP"
rmse_values <- c()

  model_data_region <- model_data_loop %>%
    dplyr::select(lon, lat, region, gear, length_category, year, prop_fishing_hours_cell) %>%
    filter(region == region_loop) %>%
    left_join(env_data) %>%
    left_join(gfw_reception_data) %>%
    na.omit()

if(nrow(model_data_region) == 1){
  next()
}
  
  
set.seed(123)
samp <- sample(nrow(model_data_region), 0.6 * nrow(model_data_region))  # do 60/40 split since this data is mostly small

train <- model_data_region[samp, ]

test <- model_data_region[-samp, ]
  
if(!file.exists(glue(file.path(rdsi_dir, "prep/random_forest/stage_2_models_regional/pruning/stage_2_rf_train_{region_loop}_28.qs")))){
tic()
  
model <- ranger(
  formula = model_formula_rf,
  data = train,
  num.trees = 100,
  importance = "impurity",  # Or "permutation" for permutation-based importance
  write.forest = TRUE,
  classification = FALSE
)
toc() 

}else{
  
  model <- qs::qread(glue(file.path(rdsi_dir, "prep/random_forest/stage_2_models_regional/pruning/stage_2_rf_train_{region_loop}_28.qs")))
  
}

# Initialize tracking
var_imp_i <- data.frame(ranger::importance(model)) %>%
  rename("importance" = 1)
num_vars <- c(nrow(var_imp_i))
threshold_i <- quantile(var_imp_i[, "importance"], 0.10)  # Use the 10th percentile instead of a fixed multiplier
n_vars <- nrow(var_imp_i)

  if (threshold_i == 0) {
    threshold_i <- 0.0000001
  }

pred_props <- predict(model, data = test)$predictions
rmse <- sqrt(mean((test$prop_fishing_hours_cell - pred_props)^2)) # use RMSE for probabilities instead?

rmse_values <- c(rmse_values, rmse)

# Iteratively remove low-importance variables
iteration <- 1
  max_iterations <- 50  # Safety cap to prevent infinite loops
while (TRUE) {
  
  # Select variables above threshold
      selected_vars <- var_imp_i %>%
      filter(importance > threshold_i) %>%
      row.names()
    
    selected_vars <- unique(c("year", "ais_reception_positions_per_day_class_A", "ais_reception_positions_per_day_class_B", selected_vars)) # always retain year and the ais reception data (for ais data biases)
  
  # Stop if too few variables remain
  if (length(selected_vars) < 6) break  # Avoid over-pruning. We can change this to any number of variables. Maybe 10 would be better computationally? 
  
          # Increase the threshold iteratively until at least one variable is removed
    while (length(selected_vars) == num_vars[length(num_vars)]) {
        threshold_i <- threshold_i * 1.01  # Increase threshold by 1%
    selected_vars <- var_imp_i %>%
      filter(importance > threshold_i) %>%
      row.names()
    
    selected_vars <- unique(c("year", "ais_reception_positions_per_day_class_A", "ais_reception_positions_per_day_class_B", selected_vars)) # always retain year and the ais reception data (for ais data biases)
    }

  # Refit model with selected variables

      n_vars <- length(selected_vars)
  file_path <-   glue(file.path(rdsi_dir, "prep/random_forest/stage_2_models_regional/pruning/stage_2_rf_train_{region_loop}_{n_vars}.qs"))
  
  if(!file.exists(file_path)){
model_i <- ranger(
  formula = prop_fishing_hours_cell ~.,
  data = train[, c("prop_fishing_hours_cell", selected_vars)],
  num.trees = 100,
  importance = "impurity",  # Or "permutation" for permutation-based importance
  write.forest = TRUE,
  classification = FALSE)
  }else{
      model_i <- qs::qread(file_path)

}
  
  
  # Get new variable importance
  var_imp_i <- data.frame(ranger::importance(model_i)) %>%
    rename("importance" = 1)

  threshold_i <- quantile(var_imp_i[, 1], 0.1) # calculate new threshold since variable importance metric will change
  
  # Compute new RMSE
    pred_props_i <- predict(model, data = test)$predictions
  rmse_i <- sqrt(mean(test$prop_fishing_hours_cell - pred_props_i)^2)
  
  # Store metrics
  rmse_values <- c(rmse_values, rmse_i)
  num_vars <- c(num_vars, length(selected_vars))
  

  if(!file.exists(file_path)){
    # save model here? Put number of variables (length(selected_vars)) in model save name so we know which one to pick for best predictions? 
  qs::qsave(model_i, file_path)
  }

  # Check RMSE stability: Stop if no improvement
  if (length(rmse_values) > 1) {
    
    rmse_improvement <- (rmse_values[length(rmse_values) - 1] - rmse_values[length(rmse_values)]) / rmse_values[length(rmse_values) - 1]
    
    if (rmse_improvement <= 0) break  # Stop if RMSE stabilizes
    
  }


  iteration <- iteration + 1
  
      if (iteration > max_iterations) {
      cat(region_loop, " - Reached max iterations. Stopping.\n")
      break
      }
}

}

stopCluster(cl)

```

Now write code to select the "best" model per the variable selection from above. We will rerun the model with just those variables on the FULL dataset to leverage all of the data from GFW and save

```{r}
# grab the flags that were run in the folder
stage_2_path <- file.path(rdsi_dir, "prep/random_forest/stage_2_models_regional/pruning")
stage_2_files <- list.files(stage_2_path, full.names = TRUE)
stage_2_regions <- unique(sub(".*stage_2_rf_train_([^_]+).*", "\\1", stage_2_files))


num_cores <- 12 # Use one less than the total available cores
cl <- makeCluster(num_cores)
registerDoParallel(cl)

# Run the loop in parallel
foreach(region_loop = stage_2_regions, .packages = c("dplyr", "tidyr", "randomForest", "qs", "glue", "tictoc", "ranger", "PRROC", "strex")) %dopar% {

#  region_loop = "IndianP"
  region_files <- list.files(stage_2_path, pattern = glue("_{region_loop}_"))
  # select the model with the next to lowest number of variables. E.g., if the final model run for ZAF is 17, then we want to grab the model with the next to lowest number of variables, which is 19. 
  n_variables <- as.numeric(str_before_first(str_after_last(region_files, "_"), "\\."))
  
  if(length(n_variables) > 1){
  best_model_n <- as.character(sort(n_variables[2]))
  }else{
    best_model_n <- as.character(n_variables[1])
  }
  
  best_train_model <- qs::qread(file.path(stage_2_path, glue("stage_2_rf_train_{region_loop}_{best_model_n}.qs")))
  
  expanded_formula <- as.formula(
  paste("prop_fishing_hours_cell ~", paste(best_train_model$forest$independent.variable.names, collapse = " + "))
)

  rf_formula <- as.formula(deparse(formula(expanded_formula)) |> paste(collapse = " "))
  
model_data_flag <- model_data %>%
  dplyr::select(lon, lat, region, gear, length_category, year, prop_fishing_hours_cell) %>%
  filter(region == region_loop) %>% 
  left_join(env_data) %>%
  left_join(gfw_reception_data) %>%
  na.omit() # this removes the categories which are not in the Rousseau data. For example, Rousseau does not have data for NZL, Lines_Longlines, 24-50m, 2015, but GFW does. 

if(nrow(model_data_flag) == 1){
  next()
} # skip if not enough data, I don't think this is a worry here but just in case.


tic()
model <- ranger(
  formula = rf_formula,
  data = model_data_flag,
  num.trees = 500,
  importance = "impurity",  # Or "permutation" for permutation-based importance
  write.forest = TRUE,
  classification = FALSE
)
toc() 


qs::qsave(model, glue(file.path(rdsi_dir, "prep/random_forest/stage_2_models_regional/stage_2_rf_model_full_data_{region_loop}_{best_model_n}.qs")))
}

stopCluster(cl)


```

